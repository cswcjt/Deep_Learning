{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cswcjt/Deep_Learning/blob/main/LSTM101_Stock_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Z2xW_xLE30"
      },
      "source": [
        "# LSTM을 활용한 주가 예측 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR3vKtdrLE31"
      },
      "source": [
        "이번 튜토리얼 에서는 다음과 같은 **프로세스 파이프라인**으로 주가 예측을 진행합니다.\n",
        "\n",
        "- FinanceDataReader를 활용하여 주가 데이터 받아오기\n",
        "- TensorFlow Dataset 클래스를 활용하여 주가 데이터 구축\n",
        "- LSTM 을 활용한 주가 예측 모델 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcwoA1HLE31"
      },
      "source": [
        "## 필요한 모듈 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2__qOsCLE32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50cwwi8GcmyY",
        "outputId": "c265b92d-2bf3-488e-cf39-861c6c82e9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKZDKR4yLE32"
      },
      "source": [
        "## 데이터 (FinanceDataReader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2kOCvFsLE32"
      },
      "source": [
        "**FinanceDataReader**는 주가 데이터를 편리하게 가져올 수 있는 파이썬 패키지입니다.\n",
        "\n",
        "- [GitHub Repo](https://github.com/FinanceData/FinanceDataReader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxDocUKtLE33"
      },
      "source": [
        "**FinanceDataReader**가 아직 설치 되지 않으신 분들은 아래의 주석을 해제한 후 명령어로 설치해 주시기 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRO7eLyKLE33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2507c63b-8ef9-4af6-c9aa-5da315d52ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: finance-datareader in /usr/local/lib/python3.7/dist-packages (0.9.50)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (4.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (2.23.0)\n",
            "Requirement already satisfied: requests-file in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (1.5.1)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->finance-datareader) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->finance-datareader) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->finance-datareader) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19.2->finance-datareader) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install finance-datareader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU1-4LUwLE33"
      },
      "outputs": [],
      "source": [
        "import FinanceDataReader as fdr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPPrmbHBLE33"
      },
      "outputs": [],
      "source": [
        "# 삼성전자(005930) 전체 (1998-08-21 ~ 현재)\n",
        "samsung = fdr.DataReader('005930')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__FL2e1ULE33"
      },
      "source": [
        "매우 편리하게 삼성전자 주가 데이터를 `DataFrame`형식으로 받아옵니다.\n",
        "\n",
        "기본 **오름차순 정렬**이 된 데이터임을 알 수 있습니다.\n",
        "\n",
        "### 컬럼 설명\n",
        "\n",
        "- `Open`:   시가\n",
        "- `High`:   고가\n",
        "- `Low`:    저가\n",
        "- `Close`:  종가\n",
        "- `Volume`: 거래량\n",
        "- `Change`: 대비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ENwL2kmLE33",
        "outputId": "dd9d71cc-028a-4aba-ffdb-145deb889940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Open   High    Low  Close    Volume    Change\n",
              "Date                                                      \n",
              "1998-08-22    856    856    804    816    252940       NaN\n",
              "1998-08-24    816    823    794    801    427220 -0.018382\n",
              "1998-08-25    808    871    808    847    495580  0.057428\n",
              "1998-08-26    862    917    852    898   1102590  0.060213\n",
              "1998-08-27    889    892    864    881    473890 -0.018931\n",
              "...           ...    ...    ...    ...       ...       ...\n",
              "2022-11-11  63100  63200  62300  62900  20037163  0.041391\n",
              "2022-11-14  62900  62900  61700  61900  15973416 -0.015898\n",
              "2022-11-15  62200  62500  61600  62400  12310986  0.008078\n",
              "2022-11-16  62400  62700  61700  62700  12909260  0.004808\n",
              "2022-11-17  62000  62000  61300  61600   6840958 -0.017544\n",
              "\n",
              "[6000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c2662f0-ae5a-4fcc-99b5-8a26fd6ae1ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Change</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-08-22</th>\n",
              "      <td>856</td>\n",
              "      <td>856</td>\n",
              "      <td>804</td>\n",
              "      <td>816</td>\n",
              "      <td>252940</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-24</th>\n",
              "      <td>816</td>\n",
              "      <td>823</td>\n",
              "      <td>794</td>\n",
              "      <td>801</td>\n",
              "      <td>427220</td>\n",
              "      <td>-0.018382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-25</th>\n",
              "      <td>808</td>\n",
              "      <td>871</td>\n",
              "      <td>808</td>\n",
              "      <td>847</td>\n",
              "      <td>495580</td>\n",
              "      <td>0.057428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-26</th>\n",
              "      <td>862</td>\n",
              "      <td>917</td>\n",
              "      <td>852</td>\n",
              "      <td>898</td>\n",
              "      <td>1102590</td>\n",
              "      <td>0.060213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-27</th>\n",
              "      <td>889</td>\n",
              "      <td>892</td>\n",
              "      <td>864</td>\n",
              "      <td>881</td>\n",
              "      <td>473890</td>\n",
              "      <td>-0.018931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-11</th>\n",
              "      <td>63100</td>\n",
              "      <td>63200</td>\n",
              "      <td>62300</td>\n",
              "      <td>62900</td>\n",
              "      <td>20037163</td>\n",
              "      <td>0.041391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-14</th>\n",
              "      <td>62900</td>\n",
              "      <td>62900</td>\n",
              "      <td>61700</td>\n",
              "      <td>61900</td>\n",
              "      <td>15973416</td>\n",
              "      <td>-0.015898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-15</th>\n",
              "      <td>62200</td>\n",
              "      <td>62500</td>\n",
              "      <td>61600</td>\n",
              "      <td>62400</td>\n",
              "      <td>12310986</td>\n",
              "      <td>0.008078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-16</th>\n",
              "      <td>62400</td>\n",
              "      <td>62700</td>\n",
              "      <td>61700</td>\n",
              "      <td>62700</td>\n",
              "      <td>12909260</td>\n",
              "      <td>0.004808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-17</th>\n",
              "      <td>62000</td>\n",
              "      <td>62000</td>\n",
              "      <td>61300</td>\n",
              "      <td>61600</td>\n",
              "      <td>6840958</td>\n",
              "      <td>-0.017544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c2662f0-ae5a-4fcc-99b5-8a26fd6ae1ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c2662f0-ae5a-4fcc-99b5-8a26fd6ae1ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c2662f0-ae5a-4fcc-99b5-8a26fd6ae1ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "samsung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FBTlube0l4d"
      },
      "source": [
        "## 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "MWcaWEa80l4o",
        "outputId": "cdfdfbb5-177e-4cfc-c410-5129bb4756b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.lineplot(data=samsung, x='Date', y=\"Close\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJBwHrRULE37",
        "outputId": "c7a7b09c-4737-453b-f7a8-eee3a038ade5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x648 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "time_steps = [['1990', '2000'], \n",
        "              ['2000', '2010'], \n",
        "              ['2010', '2020'], \n",
        "              ['2020', '2022']] # COVID-19\n",
        "\n",
        "fig, axes = plt.subplots(2, 2)\n",
        "fig.set_size_inches(16, 9)\n",
        "for i in range(4):\n",
        "    ax = axes[i//2, i%2]\n",
        "    df = samsung.loc[(samsung.index > time_steps[i][0]) & (samsung.index < time_steps[i][1])]\n",
        "    sns.lineplot(y=df['Close'], x=df.index, ax=ax)\n",
        "    ax.set_title(f'{time_steps[i][0]}~{time_steps[i][1]}')\n",
        "    ax.set_xlabel('time')\n",
        "    ax.set_ylabel('price')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWBrHadCLE37"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhHJ-SbbLE37"
      },
      "source": [
        "주가 데이터에 대하여 딥러닝 모델이 더 잘 학습할 수 있도록 **정규화(Normalization)**를 해주도록 하겠습니다.\n",
        "\n",
        "**표준화 (Standardization)**와 **정규화(Normalization)**에 대한 내용은 아래 링크에서 더 자세히 다루니, 참고해 보시기 바랍니다.\n",
        "\n",
        "- [데이터 전처리에 관하여](https://teddylee777.github.io/scikit-learn/scikit-learn-preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-4PgRMZ0l5Q",
        "outputId": "d63fc699-58bf-4b15-b4c7-6031bd7538b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00947951, 0.00884298, 0.00898324, 0.00280092, 0.00196985],\n",
              "       [0.00903654, 0.00850207, 0.00887151, 0.00473079, 0.00180386],\n",
              "       [0.00894795, 0.00899793, 0.00902793, 0.00548778, 0.00231292],\n",
              "       ...,\n",
              "       [0.68881506, 0.64566116, 0.68826816, 0.13632496, 0.68349527],\n",
              "       [0.6910299 , 0.64772727, 0.68938547, 0.14294991, 0.68681525],\n",
              "       [0.68660022, 0.64049587, 0.6849162 , 0.07575294, 0.674642  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# 스케일을 적용할 column을 정의합니다.\n",
        "scale_cols = ['Open', 'High', 'Low', 'Volume', 'Close']\n",
        "# 스케일 후 columns\n",
        "scaled = scaler.fit_transform(samsung[scale_cols])\n",
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJutxx1oLE37"
      },
      "source": [
        "스케일이 완료된 column으로 새로운 데이터프레임을 생성합니다.\n",
        "\n",
        "**시간 순으로 정렬**되어 있으며, datetime index는 제외했습니다.\n",
        "\n",
        "*6,000개의 row, 5개 column*으로 이루어진 데이터셋이 DataFrame으로 정리되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIJjE21B0l5m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "4270712d-d3ae-49fd-e682-e1cf3e2815e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Open      High       Low    Volume     Close\n",
              "Date                                                        \n",
              "1998-08-22  0.009480  0.008843  0.008983  0.002801  0.001970\n",
              "1998-08-24  0.009037  0.008502  0.008872  0.004731  0.001804\n",
              "1998-08-25  0.008948  0.008998  0.009028  0.005488  0.002313\n",
              "1998-08-26  0.009546  0.009473  0.009520  0.012209  0.002877\n",
              "1998-08-27  0.009845  0.009215  0.009654  0.005248  0.002689\n",
              "...              ...       ...       ...       ...       ...\n",
              "2022-11-11  0.698782  0.652893  0.696089  0.221880  0.689029\n",
              "2022-11-14  0.696567  0.649793  0.689385  0.176881  0.677962\n",
              "2022-11-15  0.688815  0.645661  0.688268  0.136325  0.683495\n",
              "2022-11-16  0.691030  0.647727  0.689385  0.142950  0.686815\n",
              "2022-11-17  0.686600  0.640496  0.684916  0.075753  0.674642\n",
              "\n",
              "[6000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03b279f3-938f-41c9-bed7-0deec40efd00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1998-08-22</th>\n",
              "      <td>0.009480</td>\n",
              "      <td>0.008843</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>0.002801</td>\n",
              "      <td>0.001970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-24</th>\n",
              "      <td>0.009037</td>\n",
              "      <td>0.008502</td>\n",
              "      <td>0.008872</td>\n",
              "      <td>0.004731</td>\n",
              "      <td>0.001804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-25</th>\n",
              "      <td>0.008948</td>\n",
              "      <td>0.008998</td>\n",
              "      <td>0.009028</td>\n",
              "      <td>0.005488</td>\n",
              "      <td>0.002313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-26</th>\n",
              "      <td>0.009546</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>0.009520</td>\n",
              "      <td>0.012209</td>\n",
              "      <td>0.002877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998-08-27</th>\n",
              "      <td>0.009845</td>\n",
              "      <td>0.009215</td>\n",
              "      <td>0.009654</td>\n",
              "      <td>0.005248</td>\n",
              "      <td>0.002689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-11</th>\n",
              "      <td>0.698782</td>\n",
              "      <td>0.652893</td>\n",
              "      <td>0.696089</td>\n",
              "      <td>0.221880</td>\n",
              "      <td>0.689029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-14</th>\n",
              "      <td>0.696567</td>\n",
              "      <td>0.649793</td>\n",
              "      <td>0.689385</td>\n",
              "      <td>0.176881</td>\n",
              "      <td>0.677962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-15</th>\n",
              "      <td>0.688815</td>\n",
              "      <td>0.645661</td>\n",
              "      <td>0.688268</td>\n",
              "      <td>0.136325</td>\n",
              "      <td>0.683495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-16</th>\n",
              "      <td>0.691030</td>\n",
              "      <td>0.647727</td>\n",
              "      <td>0.689385</td>\n",
              "      <td>0.142950</td>\n",
              "      <td>0.686815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-11-17</th>\n",
              "      <td>0.686600</td>\n",
              "      <td>0.640496</td>\n",
              "      <td>0.684916</td>\n",
              "      <td>0.075753</td>\n",
              "      <td>0.674642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03b279f3-938f-41c9-bed7-0deec40efd00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03b279f3-938f-41c9-bed7-0deec40efd00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03b279f3-938f-41c9-bed7-0deec40efd00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df = pd.DataFrame(scaled, columns=scale_cols, index=samsung.index)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgxgLtG8LE37"
      },
      "source": [
        "## train / test 분할\n",
        "\n",
        "- 2021년 이전까지의 모든 데이터로, 2022년을 예측해봅니다.\n",
        "\n",
        "\n",
        "- 시계열 예측에서는 \"예측 당시\"에는 미래의 데이터를 모릅니다. 그래서 train 데이터는 무조건 test 데이터보다 시기적으로 **과거**여야합니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cdo6VJXLE37"
      },
      "outputs": [],
      "source": [
        "train = df.loc[(df.index > '2020-01-01') & (df.index < '2022-01-01')]\n",
        "X_train, y_train = train.drop(\"Close\", axis=1), train.Close\n",
        "test = df.loc[df.index >= '2022-01-01']\n",
        "X_test, y_test = test.drop(\"Close\", axis=1), test.Close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZAzbDU7LE38",
        "outputId": "b7f54002-c650-4bb6-9540-ef462fd7b2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Open      High       Low    Volume\n",
              "Date                                              \n",
              "2020-01-02  0.614618  0.578512  0.614525  0.143880\n",
              "2020-01-03  0.620155  0.584711  0.613408  0.170777\n",
              "2020-01-06  0.607973  0.574380  0.610056  0.113823\n",
              "2020-01-07  0.616833  0.582645  0.621229  0.110843\n",
              "2020-01-08  0.622370  0.592975  0.624581  0.260239\n",
              "...              ...       ...       ...       ...\n",
              "2021-12-24  0.888151  0.834711  0.896089  0.133838\n",
              "2021-12-27  0.892580  0.832645  0.891620  0.119409\n",
              "2021-12-28  0.888151  0.830579  0.890503  0.201828\n",
              "2021-12-29  0.888151  0.828512  0.877095  0.219196\n",
              "2021-12-30  0.873754  0.821281  0.872626  0.157649\n",
              "\n",
              "[496 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d49088cf-1382-4789-9a7a-5fdfce71ec31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-02</th>\n",
              "      <td>0.614618</td>\n",
              "      <td>0.578512</td>\n",
              "      <td>0.614525</td>\n",
              "      <td>0.143880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-03</th>\n",
              "      <td>0.620155</td>\n",
              "      <td>0.584711</td>\n",
              "      <td>0.613408</td>\n",
              "      <td>0.170777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-06</th>\n",
              "      <td>0.607973</td>\n",
              "      <td>0.574380</td>\n",
              "      <td>0.610056</td>\n",
              "      <td>0.113823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-07</th>\n",
              "      <td>0.616833</td>\n",
              "      <td>0.582645</td>\n",
              "      <td>0.621229</td>\n",
              "      <td>0.110843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-08</th>\n",
              "      <td>0.622370</td>\n",
              "      <td>0.592975</td>\n",
              "      <td>0.624581</td>\n",
              "      <td>0.260239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-24</th>\n",
              "      <td>0.888151</td>\n",
              "      <td>0.834711</td>\n",
              "      <td>0.896089</td>\n",
              "      <td>0.133838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-27</th>\n",
              "      <td>0.892580</td>\n",
              "      <td>0.832645</td>\n",
              "      <td>0.891620</td>\n",
              "      <td>0.119409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-28</th>\n",
              "      <td>0.888151</td>\n",
              "      <td>0.830579</td>\n",
              "      <td>0.890503</td>\n",
              "      <td>0.201828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-29</th>\n",
              "      <td>0.888151</td>\n",
              "      <td>0.828512</td>\n",
              "      <td>0.877095</td>\n",
              "      <td>0.219196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-30</th>\n",
              "      <td>0.873754</td>\n",
              "      <td>0.821281</td>\n",
              "      <td>0.872626</td>\n",
              "      <td>0.157649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>496 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d49088cf-1382-4789-9a7a-5fdfce71ec31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d49088cf-1382-4789-9a7a-5fdfce71ec31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d49088cf-1382-4789-9a7a-5fdfce71ec31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcmWpxbDhvlc",
        "outputId": "f107ef94-b061-4eca-922c-de275598938d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2022-01-03    0.862774\n",
              "2022-01-04    0.863881\n",
              "2022-01-05    0.849494\n",
              "2022-01-06    0.843961\n",
              "2022-01-07    0.859454\n",
              "                ...   \n",
              "2022-11-11    0.689029\n",
              "2022-11-14    0.677962\n",
              "2022-11-15    0.683495\n",
              "2022-11-16    0.686815\n",
              "2022-11-17    0.674642\n",
              "Name: Close, Length: 216, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwjthJu1LE38"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_7eEgmCLE38"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIylbqfgLE38"
      },
      "outputs": [],
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1000)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3aD8hS9LE38"
      },
      "source": [
        "Hyperparameter를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt6ET8zHLE38"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE=10 # 20일 단위씩 학습\n",
        "BATCH_SIZE=8  # 한번에 32개씩 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6sNPrV_LE38"
      },
      "outputs": [],
      "source": [
        "# trian_data는 학습용 데이터셋, test_data는 검증용 데이터셋 입니다.\n",
        "train_data = windowed_dataset(y_train, WINDOW_SIZE, BATCH_SIZE, False)\n",
        "test_data = windowed_dataset(y_test, WINDOW_SIZE, BATCH_SIZE, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vio_TJJ6LE38",
        "outputId": "7f61c674-1342-41c9-e1f8-d9d5bd921431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터셋(X) 구성(batch_size, window_size, feature갯수): (8, 10, 1)\n",
            "데이터셋(Y) 구성(batch_size, window_size, feature갯수): (8, 1)\n"
          ]
        }
      ],
      "source": [
        "# 아래의 코드로 데이터셋의 구성을 확인해 볼 수 있습니다.\n",
        "# X: (batch_size, window_size, feature)\n",
        "# Y: (batch_size, feature)\n",
        "for data in train_data.take(1):\n",
        "    print(f'데이터셋(X) 구성(batch_size, window_size, feature갯수): {data[0].shape}')\n",
        "    print(f'데이터셋(Y) 구성(batch_size, window_size, feature갯수): {data[1].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GyH6elLE39"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAkmGu1j0l8M"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential([\n",
        "    # 1차원 feature map 생성\n",
        "    Conv1D(filters=32, kernel_size=5,\n",
        "           padding=\"causal\",\n",
        "           activation=\"relu\",\n",
        "           input_shape=[WINDOW_SIZE, 1]),\n",
        "    # LSTM\n",
        "    LSTM(16, activation='tanh', input_shape=(WINDOW_SIZE, 1)),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xShll_EX0l8T"
      },
      "outputs": [],
      "source": [
        "# Sequence 학습에 비교적 좋은 퍼포먼스를 내는 MSE()를 사용합니다.\n",
        "loss_fn = 'mean_squared_error'\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30DhBFhZLE39"
      },
      "outputs": [],
      "source": [
        "# val_loss 기준 체크포인터도 생성합니다.\n",
        "filename = \"best_model.ckpt\"\n",
        "checkpoint = ModelCheckpoint(filename, \n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6522bd84-ae6f-4f2d-b98d-ad53989ca293",
        "id": "QpG9AdUXLE39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "     61/Unknown - 9s 10ms/step - loss: 0.2856 - mse: 0.2856\n",
            "Epoch 1: val_loss improved from inf to 0.16611, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 13s 61ms/step - loss: 0.2856 - mse: 0.2856 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "Epoch 2/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.1310 - mse: 0.1310\n",
            "Epoch 2: val_loss improved from 0.16611 to 0.05342, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 18ms/step - loss: 0.1310 - mse: 0.1310 - val_loss: 0.0534 - val_mse: 0.0534\n",
            "Epoch 3/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 3: val_loss improved from 0.05342 to 0.00220, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 18ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0022 - val_mse: 0.0022\n",
            "Epoch 4/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
            "Epoch 4: val_loss improved from 0.00220 to 0.00180, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 5/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
            "Epoch 5: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 18ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 6/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.0026\n",
            "Epoch 6: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 7/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
            "Epoch 7: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 8/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0025 - mse: 0.0025\n",
            "Epoch 8: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 9/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0024 - mse: 0.0024\n",
            "Epoch 9: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 10/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
            "Epoch 10: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 11/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0023 - mse: 0.0023\n",
            "Epoch 11: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0021 - val_mse: 0.0021\n",
            "Epoch 12/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0022 - mse: 0.0022\n",
            "Epoch 12: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0020 - val_mse: 0.0020\n",
            "Epoch 13/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0021 - mse: 0.0021\n",
            "Epoch 13: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 14/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0021 - mse: 0.0021\n",
            "Epoch 14: val_loss did not improve from 0.00180\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0019 - val_mse: 0.0019\n",
            "Epoch 15/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0019 - mse: 0.0019\n",
            "Epoch 15: val_loss improved from 0.00180 to 0.00178, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0018 - val_mse: 0.0018\n",
            "Epoch 16/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0019 - mse: 0.0019\n",
            "Epoch 16: val_loss improved from 0.00178 to 0.00170, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0017 - val_mse: 0.0017\n",
            "Epoch 17/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0017 - mse: 0.0017\n",
            "Epoch 17: val_loss improved from 0.00170 to 0.00162, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0016 - val_mse: 0.0016\n",
            "Epoch 18/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0018 - mse: 0.0018\n",
            "Epoch 18: val_loss improved from 0.00162 to 0.00154, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0015 - val_mse: 0.0015\n",
            "Epoch 19/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.0016\n",
            "Epoch 19: val_loss improved from 0.00154 to 0.00145, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0015 - val_mse: 0.0015\n",
            "Epoch 20/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0015 - mse: 0.0015\n",
            "Epoch 20: val_loss improved from 0.00145 to 0.00137, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0014 - val_mse: 0.0014\n",
            "Epoch 21/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0016 - mse: 0.0016\n",
            "Epoch 21: val_loss improved from 0.00137 to 0.00129, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0013 - val_mse: 0.0013\n",
            "Epoch 22/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0015 - mse: 0.0015\n",
            "Epoch 22: val_loss improved from 0.00129 to 0.00121, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0012 - val_mse: 0.0012\n",
            "Epoch 23/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
            "Epoch 23: val_loss improved from 0.00121 to 0.00114, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0011 - val_mse: 0.0011\n",
            "Epoch 24/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
            "Epoch 24: val_loss improved from 0.00114 to 0.00107, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0011 - val_mse: 0.0011\n",
            "Epoch 25/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0014 - mse: 0.0014\n",
            "Epoch 25: val_loss improved from 0.00107 to 0.00100, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 9.9954e-04 - val_mse: 9.9954e-04\n",
            "Epoch 26/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
            "Epoch 26: val_loss improved from 0.00100 to 0.00094, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 9.3772e-04 - val_mse: 9.3772e-04\n",
            "Epoch 27/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
            "Epoch 27: val_loss improved from 0.00094 to 0.00088, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 8.8071e-04 - val_mse: 8.8071e-04\n",
            "Epoch 28/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 28: val_loss improved from 0.00088 to 0.00083, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 8.2850e-04 - val_mse: 8.2850e-04\n",
            "Epoch 29/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0013 - mse: 0.0013\n",
            "Epoch 29: val_loss improved from 0.00083 to 0.00078, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 7.8101e-04 - val_mse: 7.8101e-04\n",
            "Epoch 30/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 30: val_loss improved from 0.00078 to 0.00074, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 7.3812e-04 - val_mse: 7.3812e-04\n",
            "Epoch 31/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 31: val_loss improved from 0.00074 to 0.00070, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.9956e-04 - val_mse: 6.9956e-04\n",
            "Epoch 32/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 32: val_loss improved from 0.00070 to 0.00067, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.6506e-04 - val_mse: 6.6506e-04\n",
            "Epoch 33/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 33: val_loss improved from 0.00067 to 0.00063, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.3427e-04 - val_mse: 6.3427e-04\n",
            "Epoch 34/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 34: val_loss improved from 0.00063 to 0.00061, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 6.0687e-04 - val_mse: 6.0687e-04\n",
            "Epoch 35/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 35: val_loss improved from 0.00061 to 0.00058, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.8260e-04 - val_mse: 5.8260e-04\n",
            "Epoch 36/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 36: val_loss improved from 0.00058 to 0.00056, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.6110e-04 - val_mse: 5.6110e-04\n",
            "Epoch 37/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 37: val_loss improved from 0.00056 to 0.00054, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.4209e-04 - val_mse: 5.4209e-04\n",
            "Epoch 38/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 38: val_loss improved from 0.00054 to 0.00053, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.2526e-04 - val_mse: 5.2526e-04\n",
            "Epoch 39/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 39: val_loss improved from 0.00053 to 0.00051, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.1039e-04 - val_mse: 5.1039e-04\n",
            "Epoch 40/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 40: val_loss improved from 0.00051 to 0.00050, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9731e-04 - val_mse: 4.9731e-04\n",
            "Epoch 41/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 41: val_loss improved from 0.00050 to 0.00049, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8583e-04 - val_mse: 4.8583e-04\n",
            "Epoch 42/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 42: val_loss improved from 0.00049 to 0.00048, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.7581e-04 - val_mse: 4.7581e-04\n",
            "Epoch 43/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 43: val_loss improved from 0.00048 to 0.00047, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9762e-04 - mse: 9.9762e-04 - val_loss: 4.6711e-04 - val_mse: 4.6711e-04\n",
            "Epoch 44/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 44: val_loss improved from 0.00047 to 0.00046, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9551e-04 - mse: 9.9551e-04 - val_loss: 4.5951e-04 - val_mse: 4.5951e-04\n",
            "Epoch 45/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 45: val_loss improved from 0.00046 to 0.00045, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.9403e-04 - mse: 9.9403e-04 - val_loss: 4.5290e-04 - val_mse: 4.5290e-04\n",
            "Epoch 46/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 46: val_loss improved from 0.00045 to 0.00045, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9318e-04 - mse: 9.9318e-04 - val_loss: 4.4715e-04 - val_mse: 4.4715e-04\n",
            "Epoch 47/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.9578e-04 - mse: 9.9578e-04\n",
            "Epoch 47: val_loss improved from 0.00045 to 0.00044, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.9290e-04 - mse: 9.9290e-04 - val_loss: 4.4219e-04 - val_mse: 4.4219e-04\n",
            "Epoch 48/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 48: val_loss improved from 0.00044 to 0.00044, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9317e-04 - mse: 9.9317e-04 - val_loss: 4.3792e-04 - val_mse: 4.3792e-04\n",
            "Epoch 49/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 49: val_loss improved from 0.00044 to 0.00043, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.9392e-04 - mse: 9.9392e-04 - val_loss: 4.3434e-04 - val_mse: 4.3434e-04\n",
            "Epoch 50/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 50: val_loss improved from 0.00043 to 0.00043, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9516e-04 - mse: 9.9516e-04 - val_loss: 4.3130e-04 - val_mse: 4.3130e-04\n",
            "Epoch 51/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 51: val_loss improved from 0.00043 to 0.00043, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.9681e-04 - mse: 9.9681e-04 - val_loss: 4.2877e-04 - val_mse: 4.2877e-04\n",
            "Epoch 52/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 52: val_loss improved from 0.00043 to 0.00043, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9882e-04 - mse: 9.9882e-04 - val_loss: 4.2673e-04 - val_mse: 4.2673e-04\n",
            "Epoch 53/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 53: val_loss improved from 0.00043 to 0.00043, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2513e-04 - val_mse: 4.2513e-04\n",
            "Epoch 54/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 54: val_loss improved from 0.00043 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2392e-04 - val_mse: 4.2392e-04\n",
            "Epoch 55/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 55: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2313e-04 - val_mse: 4.2313e-04\n",
            "Epoch 56/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 56: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2270e-04 - val_mse: 4.2270e-04\n",
            "Epoch 57/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 57: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2261e-04 - val_mse: 4.2261e-04\n",
            "Epoch 58/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 58: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2285e-04 - val_mse: 4.2285e-04\n",
            "Epoch 59/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 59: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2341e-04 - val_mse: 4.2341e-04\n",
            "Epoch 60/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 60: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2424e-04 - val_mse: 4.2424e-04\n",
            "Epoch 61/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 61: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2532e-04 - val_mse: 4.2532e-04\n",
            "Epoch 62/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 62: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2664e-04 - val_mse: 4.2664e-04\n",
            "Epoch 63/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 63: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.2821e-04 - val_mse: 4.2821e-04\n",
            "Epoch 64/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 64: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.3000e-04 - val_mse: 4.3000e-04\n",
            "Epoch 65/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 65: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.3204e-04 - val_mse: 4.3204e-04\n",
            "Epoch 66/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 66: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.3427e-04 - val_mse: 4.3427e-04\n",
            "Epoch 67/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 67: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.3670e-04 - val_mse: 4.3670e-04\n",
            "Epoch 68/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 68: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.3881e-04 - val_mse: 4.3881e-04\n",
            "Epoch 69/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 69: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.4132e-04 - val_mse: 4.4132e-04\n",
            "Epoch 70/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 70: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.4367e-04 - val_mse: 4.4367e-04\n",
            "Epoch 71/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 71: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.4616e-04 - val_mse: 4.4616e-04\n",
            "Epoch 72/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 72: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.4894e-04 - val_mse: 4.4894e-04\n",
            "Epoch 73/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 73: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.5180e-04 - val_mse: 4.5180e-04\n",
            "Epoch 74/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 74: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.5487e-04 - val_mse: 4.5487e-04\n",
            "Epoch 75/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 75: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.5782e-04 - val_mse: 4.5782e-04\n",
            "Epoch 76/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 76: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.6125e-04 - val_mse: 4.6125e-04\n",
            "Epoch 77/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 77: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.6437e-04 - val_mse: 4.6437e-04\n",
            "Epoch 78/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 78: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.6770e-04 - val_mse: 4.6770e-04\n",
            "Epoch 79/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 79: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.7120e-04 - val_mse: 4.7120e-04\n",
            "Epoch 80/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 80: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.7470e-04 - val_mse: 4.7470e-04\n",
            "Epoch 81/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 81: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.7827e-04 - val_mse: 4.7827e-04\n",
            "Epoch 82/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 82: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.8184e-04 - val_mse: 4.8184e-04\n",
            "Epoch 83/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 83: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.8542e-04 - val_mse: 4.8542e-04\n",
            "Epoch 84/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 84: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.8905e-04 - val_mse: 4.8905e-04\n",
            "Epoch 85/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 85: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.9283e-04 - val_mse: 4.9283e-04\n",
            "Epoch 86/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 86: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.9576e-04 - val_mse: 4.9576e-04\n",
            "Epoch 87/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 87: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 4.9947e-04 - val_mse: 4.9947e-04\n",
            "Epoch 88/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 88: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0215e-04 - val_mse: 5.0215e-04\n",
            "Epoch 89/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 89: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0561e-04 - val_mse: 5.0561e-04\n",
            "Epoch 90/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 90: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0880e-04 - val_mse: 5.0880e-04\n",
            "Epoch 91/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 91: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1172e-04 - val_mse: 5.1172e-04\n",
            "Epoch 92/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 92: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1448e-04 - val_mse: 5.1448e-04\n",
            "Epoch 93/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 93: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1716e-04 - val_mse: 5.1716e-04\n",
            "Epoch 94/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 94: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1960e-04 - val_mse: 5.1960e-04\n",
            "Epoch 95/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 95: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2196e-04 - val_mse: 5.2196e-04\n",
            "Epoch 96/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 96: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.2418e-04 - val_mse: 5.2418e-04\n",
            "Epoch 97/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 97: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.2618e-04 - val_mse: 5.2618e-04\n",
            "Epoch 98/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 98: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.2836e-04 - val_mse: 5.2836e-04\n",
            "Epoch 99/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 99: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.3010e-04 - val_mse: 5.3010e-04\n",
            "Epoch 100/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 100: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.3161e-04 - val_mse: 5.3161e-04\n",
            "Epoch 101/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 101: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.3328e-04 - val_mse: 5.3328e-04\n",
            "Epoch 102/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 102: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 5.3459e-04 - val_mse: 5.3459e-04\n",
            "Epoch 103/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 103: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3563e-04 - val_mse: 5.3563e-04\n",
            "Epoch 104/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 104: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3639e-04 - val_mse: 5.3639e-04\n",
            "Epoch 105/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 105: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3731e-04 - val_mse: 5.3731e-04\n",
            "Epoch 106/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 106: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3794e-04 - val_mse: 5.3794e-04\n",
            "Epoch 107/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 107: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3823e-04 - val_mse: 5.3823e-04\n",
            "Epoch 108/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 108: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3891e-04 - val_mse: 5.3891e-04\n",
            "Epoch 109/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 109: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3914e-04 - val_mse: 5.3914e-04\n",
            "Epoch 110/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 110: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3923e-04 - val_mse: 5.3923e-04\n",
            "Epoch 111/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 111: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3958e-04 - val_mse: 5.3958e-04\n",
            "Epoch 112/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 112: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3967e-04 - val_mse: 5.3967e-04\n",
            "Epoch 113/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 113: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3988e-04 - val_mse: 5.3988e-04\n",
            "Epoch 114/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 114: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3971e-04 - val_mse: 5.3971e-04\n",
            "Epoch 115/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 115: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3988e-04 - val_mse: 5.3988e-04\n",
            "Epoch 116/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 116: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3967e-04 - val_mse: 5.3967e-04\n",
            "Epoch 117/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 117: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3927e-04 - val_mse: 5.3927e-04\n",
            "Epoch 118/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 118: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3903e-04 - val_mse: 5.3903e-04\n",
            "Epoch 119/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 119: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3839e-04 - val_mse: 5.3839e-04\n",
            "Epoch 120/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 120: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3770e-04 - val_mse: 5.3770e-04\n",
            "Epoch 121/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 121: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3676e-04 - val_mse: 5.3676e-04\n",
            "Epoch 122/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 122: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3621e-04 - val_mse: 5.3621e-04\n",
            "Epoch 123/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 123: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3522e-04 - val_mse: 5.3522e-04\n",
            "Epoch 124/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 124: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3434e-04 - val_mse: 5.3434e-04\n",
            "Epoch 125/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 125: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3365e-04 - val_mse: 5.3365e-04\n",
            "Epoch 126/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 126: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3233e-04 - val_mse: 5.3233e-04\n",
            "Epoch 127/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 127: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3165e-04 - val_mse: 5.3165e-04\n",
            "Epoch 128/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 128: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.3047e-04 - val_mse: 5.3047e-04\n",
            "Epoch 129/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 129: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2970e-04 - val_mse: 5.2970e-04\n",
            "Epoch 130/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 130: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2881e-04 - val_mse: 5.2881e-04\n",
            "Epoch 131/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 131: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2836e-04 - val_mse: 5.2836e-04\n",
            "Epoch 132/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 132: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2811e-04 - val_mse: 5.2811e-04\n",
            "Epoch 133/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 133: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2841e-04 - val_mse: 5.2841e-04\n",
            "Epoch 134/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 134: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2721e-04 - val_mse: 5.2721e-04\n",
            "Epoch 135/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.0012\n",
            "Epoch 135: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2619e-04 - val_mse: 5.2619e-04\n",
            "Epoch 136/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 136: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2458e-04 - val_mse: 5.2458e-04\n",
            "Epoch 137/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 137: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2317e-04 - val_mse: 5.2317e-04\n",
            "Epoch 138/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 138: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2176e-04 - val_mse: 5.2176e-04\n",
            "Epoch 139/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 139: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.2042e-04 - val_mse: 5.2042e-04\n",
            "Epoch 140/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 140: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1924e-04 - val_mse: 5.1924e-04\n",
            "Epoch 141/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 141: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1778e-04 - val_mse: 5.1778e-04\n",
            "Epoch 142/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 142: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1605e-04 - val_mse: 5.1605e-04\n",
            "Epoch 143/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 143: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1444e-04 - val_mse: 5.1444e-04\n",
            "Epoch 144/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 144: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1293e-04 - val_mse: 5.1293e-04\n",
            "Epoch 145/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 145: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1138e-04 - val_mse: 5.1138e-04\n",
            "Epoch 146/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 146: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.1002e-04 - val_mse: 5.1002e-04\n",
            "Epoch 147/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 147: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0846e-04 - val_mse: 5.0846e-04\n",
            "Epoch 148/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 148: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0722e-04 - val_mse: 5.0722e-04\n",
            "Epoch 149/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 149: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0577e-04 - val_mse: 5.0577e-04\n",
            "Epoch 150/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 150: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 5.0463e-04 - val_mse: 5.0463e-04\n",
            "Epoch 151/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 151: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.0347e-04 - val_mse: 5.0347e-04\n",
            "Epoch 152/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 152: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.0249e-04 - val_mse: 5.0249e-04\n",
            "Epoch 153/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 153: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.0126e-04 - val_mse: 5.0126e-04\n",
            "Epoch 154/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 154: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 5.0028e-04 - val_mse: 5.0028e-04\n",
            "Epoch 155/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 155: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9912e-04 - val_mse: 4.9912e-04\n",
            "Epoch 156/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 156: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9818e-04 - val_mse: 4.9818e-04\n",
            "Epoch 157/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 157: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9711e-04 - val_mse: 4.9711e-04\n",
            "Epoch 158/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 158: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9631e-04 - val_mse: 4.9631e-04\n",
            "Epoch 159/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 159: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9533e-04 - val_mse: 4.9533e-04\n",
            "Epoch 160/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 160: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9469e-04 - val_mse: 4.9469e-04\n",
            "Epoch 161/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 161: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9347e-04 - val_mse: 4.9347e-04\n",
            "Epoch 162/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 162: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9269e-04 - val_mse: 4.9269e-04\n",
            "Epoch 163/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 163: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9187e-04 - val_mse: 4.9187e-04\n",
            "Epoch 164/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 164: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9108e-04 - val_mse: 4.9108e-04\n",
            "Epoch 165/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 165: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.9036e-04 - val_mse: 4.9036e-04\n",
            "Epoch 166/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 166: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8982e-04 - val_mse: 4.8982e-04\n",
            "Epoch 167/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 167: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8906e-04 - val_mse: 4.8906e-04\n",
            "Epoch 168/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 168: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8854e-04 - val_mse: 4.8854e-04\n",
            "Epoch 169/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 169: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8756e-04 - val_mse: 4.8756e-04\n",
            "Epoch 170/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 170: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8696e-04 - val_mse: 4.8696e-04\n",
            "Epoch 171/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 171: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8629e-04 - val_mse: 4.8629e-04\n",
            "Epoch 172/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 172: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8568e-04 - val_mse: 4.8568e-04\n",
            "Epoch 173/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 173: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8530e-04 - val_mse: 4.8530e-04\n",
            "Epoch 174/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 174: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8440e-04 - val_mse: 4.8440e-04\n",
            "Epoch 175/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 175: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8386e-04 - val_mse: 4.8386e-04\n",
            "Epoch 176/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 176: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8324e-04 - val_mse: 4.8324e-04\n",
            "Epoch 177/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 177: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8286e-04 - val_mse: 4.8286e-04\n",
            "Epoch 178/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 178: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8200e-04 - val_mse: 4.8200e-04\n",
            "Epoch 179/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 179: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8135e-04 - val_mse: 4.8135e-04\n",
            "Epoch 180/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 180: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8077e-04 - val_mse: 4.8077e-04\n",
            "Epoch 181/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 181: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.8043e-04 - val_mse: 4.8043e-04\n",
            "Epoch 182/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 182: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 4.7964e-04 - val_mse: 4.7964e-04\n",
            "Epoch 183/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 183: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.9990e-04 - mse: 9.9990e-04 - val_loss: 4.7929e-04 - val_mse: 4.7929e-04\n",
            "Epoch 184/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.9872e-04 - mse: 9.9872e-04\n",
            "Epoch 184: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 9.9872e-04 - mse: 9.9872e-04 - val_loss: 4.7843e-04 - val_mse: 4.7843e-04\n",
            "Epoch 185/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 185: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 9.9703e-04 - mse: 9.9703e-04 - val_loss: 4.7784e-04 - val_mse: 4.7784e-04\n",
            "Epoch 186/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 186: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9572e-04 - mse: 9.9572e-04 - val_loss: 4.7752e-04 - val_mse: 4.7752e-04\n",
            "Epoch 187/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 187: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.9453e-04 - mse: 9.9453e-04 - val_loss: 4.7667e-04 - val_mse: 4.7667e-04\n",
            "Epoch 188/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 188: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.9284e-04 - mse: 9.9284e-04 - val_loss: 4.7620e-04 - val_mse: 4.7620e-04\n",
            "Epoch 189/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 189: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 9.9163e-04 - mse: 9.9163e-04 - val_loss: 4.7616e-04 - val_mse: 4.7616e-04\n",
            "Epoch 190/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 190: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 9.9060e-04 - mse: 9.9060e-04 - val_loss: 4.7439e-04 - val_mse: 4.7439e-04\n",
            "Epoch 191/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 191: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.8806e-04 - mse: 9.8806e-04 - val_loss: 4.7398e-04 - val_mse: 4.7398e-04\n",
            "Epoch 192/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.9119e-04 - mse: 9.9119e-04\n",
            "Epoch 192: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.8722e-04 - mse: 9.8722e-04 - val_loss: 4.7408e-04 - val_mse: 4.7408e-04\n",
            "Epoch 193/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 193: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.8627e-04 - mse: 9.8627e-04 - val_loss: 4.7303e-04 - val_mse: 4.7303e-04\n",
            "Epoch 194/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 194: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.8431e-04 - mse: 9.8431e-04 - val_loss: 4.7301e-04 - val_mse: 4.7301e-04\n",
            "Epoch 195/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010        \n",
            "Epoch 195: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.8349e-04 - mse: 9.8349e-04 - val_loss: 4.7142e-04 - val_mse: 4.7142e-04\n",
            "Epoch 196/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.8479e-04 - mse: 9.8479e-04\n",
            "Epoch 196: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.8086e-04 - mse: 9.8086e-04 - val_loss: 4.7063e-04 - val_mse: 4.7063e-04\n",
            "Epoch 197/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.7979e-04 - mse: 9.7979e-04\n",
            "Epoch 197: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.7979e-04 - mse: 9.7979e-04 - val_loss: 4.7093e-04 - val_mse: 4.7093e-04\n",
            "Epoch 198/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.8300e-04 - mse: 9.8300e-04\n",
            "Epoch 198: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.7906e-04 - mse: 9.7906e-04 - val_loss: 4.7004e-04 - val_mse: 4.7004e-04\n",
            "Epoch 199/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 199: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.7718e-04 - mse: 9.7718e-04 - val_loss: 4.6967e-04 - val_mse: 4.6967e-04\n",
            "Epoch 200/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.9320e-04 - mse: 9.9320e-04\n",
            "Epoch 200: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.7608e-04 - mse: 9.7608e-04 - val_loss: 4.6865e-04 - val_mse: 4.6865e-04\n",
            "Epoch 201/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 201: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.7416e-04 - mse: 9.7416e-04 - val_loss: 4.6830e-04 - val_mse: 4.6830e-04\n",
            "Epoch 202/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.9492e-04 - mse: 9.9492e-04\n",
            "Epoch 202: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.7308e-04 - mse: 9.7308e-04 - val_loss: 4.6728e-04 - val_mse: 4.6728e-04\n",
            "Epoch 203/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.7117e-04 - mse: 9.7117e-04\n",
            "Epoch 203: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.7117e-04 - mse: 9.7117e-04 - val_loss: 4.6689e-04 - val_mse: 4.6689e-04\n",
            "Epoch 204/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.8705e-04 - mse: 9.8705e-04\n",
            "Epoch 204: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.7003e-04 - mse: 9.7003e-04 - val_loss: 4.6584e-04 - val_mse: 4.6584e-04\n",
            "Epoch 205/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.8508e-04 - mse: 9.8508e-04\n",
            "Epoch 205: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.6811e-04 - mse: 9.6811e-04 - val_loss: 4.6546e-04 - val_mse: 4.6546e-04\n",
            "Epoch 206/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 0.0010 - mse: 0.0010\n",
            "Epoch 206: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.6700e-04 - mse: 9.6700e-04 - val_loss: 4.6452e-04 - val_mse: 4.6452e-04\n",
            "Epoch 207/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.6907e-04 - mse: 9.6907e-04\n",
            "Epoch 207: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.6520e-04 - mse: 9.6520e-04 - val_loss: 4.6444e-04 - val_mse: 4.6444e-04\n",
            "Epoch 208/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.6817e-04 - mse: 9.6817e-04\n",
            "Epoch 208: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.6430e-04 - mse: 9.6430e-04 - val_loss: 4.6272e-04 - val_mse: 4.6272e-04\n",
            "Epoch 209/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.8316e-04 - mse: 9.8316e-04\n",
            "Epoch 209: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.6170e-04 - mse: 9.6170e-04 - val_loss: 4.6169e-04 - val_mse: 4.6169e-04\n",
            "Epoch 210/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.6424e-04 - mse: 9.6424e-04\n",
            "Epoch 210: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.6040e-04 - mse: 9.6040e-04 - val_loss: 4.6181e-04 - val_mse: 4.6181e-04\n",
            "Epoch 211/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.8095e-04 - mse: 9.8095e-04\n",
            "Epoch 211: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.5949e-04 - mse: 9.5949e-04 - val_loss: 4.6082e-04 - val_mse: 4.6082e-04\n",
            "Epoch 212/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.7901e-04 - mse: 9.7901e-04\n",
            "Epoch 212: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.5763e-04 - mse: 9.5763e-04 - val_loss: 4.6058e-04 - val_mse: 4.6058e-04\n",
            "Epoch 213/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.6038e-04 - mse: 9.6038e-04\n",
            "Epoch 213: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.5656e-04 - mse: 9.5656e-04 - val_loss: 4.5880e-04 - val_mse: 4.5880e-04\n",
            "Epoch 214/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.8160e-04 - mse: 9.8160e-04\n",
            "Epoch 214: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.5393e-04 - mse: 9.5393e-04 - val_loss: 4.5778e-04 - val_mse: 4.5778e-04\n",
            "Epoch 215/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.7381e-04 - mse: 9.7381e-04\n",
            "Epoch 215: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.5260e-04 - mse: 9.5260e-04 - val_loss: 4.5829e-04 - val_mse: 4.5829e-04\n",
            "Epoch 216/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.0010        \n",
            "Epoch 216: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.5204e-04 - mse: 9.5204e-04 - val_loss: 4.5653e-04 - val_mse: 4.5653e-04\n",
            "Epoch 217/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.7682e-04 - mse: 9.7682e-04\n",
            "Epoch 217: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.4937e-04 - mse: 9.4937e-04 - val_loss: 4.5555e-04 - val_mse: 4.5555e-04\n",
            "Epoch 218/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 9.9026e-04 - mse: 9.9026e-04\n",
            "Epoch 218: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.4805e-04 - mse: 9.4805e-04 - val_loss: 4.5623e-04 - val_mse: 4.5623e-04\n",
            "Epoch 219/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.5129e-04 - mse: 9.5129e-04\n",
            "Epoch 219: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.4753e-04 - mse: 9.4753e-04 - val_loss: 4.5389e-04 - val_mse: 4.5389e-04\n",
            "Epoch 220/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.6524e-04 - mse: 9.6524e-04\n",
            "Epoch 220: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.4436e-04 - mse: 9.4436e-04 - val_loss: 4.5260e-04 - val_mse: 4.5260e-04\n",
            "Epoch 221/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.4658e-04 - mse: 9.4658e-04\n",
            "Epoch 221: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.4286e-04 - mse: 9.4286e-04 - val_loss: 4.5284e-04 - val_mse: 4.5284e-04\n",
            "Epoch 222/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.6909e-04 - mse: 9.6909e-04\n",
            "Epoch 222: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.4200e-04 - mse: 9.4200e-04 - val_loss: 4.5134e-04 - val_mse: 4.5134e-04\n",
            "Epoch 223/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.5605e-04 - mse: 9.5605e-04\n",
            "Epoch 223: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.3971e-04 - mse: 9.3971e-04 - val_loss: 4.5044e-04 - val_mse: 4.5044e-04\n",
            "Epoch 224/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.5908e-04 - mse: 9.5908e-04\n",
            "Epoch 224: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.3836e-04 - mse: 9.3836e-04 - val_loss: 4.5044e-04 - val_mse: 4.5044e-04\n",
            "Epoch 225/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.6411e-04 - mse: 9.6411e-04\n",
            "Epoch 225: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.3727e-04 - mse: 9.3727e-04 - val_loss: 4.4874e-04 - val_mse: 4.4874e-04\n",
            "Epoch 226/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.3489e-04 - mse: 9.3489e-04\n",
            "Epoch 226: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.3489e-04 - mse: 9.3489e-04 - val_loss: 4.4782e-04 - val_mse: 4.4782e-04\n",
            "Epoch 227/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.3352e-04 - mse: 9.3352e-04\n",
            "Epoch 227: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.3352e-04 - mse: 9.3352e-04 - val_loss: 4.4819e-04 - val_mse: 4.4819e-04\n",
            "Epoch 228/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.5933e-04 - mse: 9.5933e-04\n",
            "Epoch 228: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.3273e-04 - mse: 9.3273e-04 - val_loss: 4.4611e-04 - val_mse: 4.4611e-04\n",
            "Epoch 229/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 9.8429e-04 - mse: 9.8429e-04\n",
            "Epoch 229: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.3001e-04 - mse: 9.3001e-04 - val_loss: 4.4504e-04 - val_mse: 4.4504e-04\n",
            "Epoch 230/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.4465e-04 - mse: 9.4465e-04\n",
            "Epoch 230: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.2854e-04 - mse: 9.2854e-04 - val_loss: 4.4517e-04 - val_mse: 4.4517e-04\n",
            "Epoch 231/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.4364e-04 - mse: 9.4364e-04\n",
            "Epoch 231: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.2757e-04 - mse: 9.2757e-04 - val_loss: 4.4327e-04 - val_mse: 4.4327e-04\n",
            "Epoch 232/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.2503e-04 - mse: 9.2503e-04\n",
            "Epoch 232: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.2503e-04 - mse: 9.2503e-04 - val_loss: 4.4213e-04 - val_mse: 4.4213e-04\n",
            "Epoch 233/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.2361e-04 - mse: 9.2361e-04\n",
            "Epoch 233: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.2361e-04 - mse: 9.2361e-04 - val_loss: 4.4393e-04 - val_mse: 4.4393e-04\n",
            "Epoch 234/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.4360e-04 - mse: 9.4360e-04\n",
            "Epoch 234: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.2343e-04 - mse: 9.2343e-04 - val_loss: 4.4048e-04 - val_mse: 4.4048e-04\n",
            "Epoch 235/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.3994e-04 - mse: 9.3994e-04\n",
            "Epoch 235: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.1998e-04 - mse: 9.1998e-04 - val_loss: 4.3896e-04 - val_mse: 4.3896e-04\n",
            "Epoch 236/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.1828e-04 - mse: 9.1828e-04\n",
            "Epoch 236: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.1828e-04 - mse: 9.1828e-04 - val_loss: 4.3891e-04 - val_mse: 4.3891e-04\n",
            "Epoch 237/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.1743e-04 - mse: 9.1743e-04\n",
            "Epoch 237: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.1743e-04 - mse: 9.1743e-04 - val_loss: 4.4033e-04 - val_mse: 4.4033e-04\n",
            "Epoch 238/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.4283e-04 - mse: 9.4283e-04\n",
            "Epoch 238: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.1708e-04 - mse: 9.1708e-04 - val_loss: 4.3753e-04 - val_mse: 4.3753e-04\n",
            "Epoch 239/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.3986e-04 - mse: 9.3986e-04\n",
            "Epoch 239: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.1430e-04 - mse: 9.1430e-04 - val_loss: 4.3649e-04 - val_mse: 4.3649e-04\n",
            "Epoch 240/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.2859e-04 - mse: 9.2859e-04\n",
            "Epoch 240: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.1285e-04 - mse: 9.1285e-04 - val_loss: 4.3780e-04 - val_mse: 4.3780e-04\n",
            "Epoch 241/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.2822e-04 - mse: 9.2822e-04\n",
            "Epoch 241: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.1249e-04 - mse: 9.1249e-04 - val_loss: 4.3466e-04 - val_mse: 4.3466e-04\n",
            "Epoch 242/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.2877e-04 - mse: 9.2877e-04\n",
            "Epoch 242: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.0918e-04 - mse: 9.0918e-04 - val_loss: 4.3304e-04 - val_mse: 4.3304e-04\n",
            "Epoch 243/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.2691e-04 - mse: 9.2691e-04\n",
            "Epoch 243: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.0735e-04 - mse: 9.0735e-04 - val_loss: 4.3292e-04 - val_mse: 4.3292e-04\n",
            "Epoch 244/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.3163e-04 - mse: 9.3163e-04\n",
            "Epoch 244: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.0645e-04 - mse: 9.0645e-04 - val_loss: 4.3423e-04 - val_mse: 4.3423e-04\n",
            "Epoch 245/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.2180e-04 - mse: 9.2180e-04\n",
            "Epoch 245: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.0622e-04 - mse: 9.0622e-04 - val_loss: 4.3114e-04 - val_mse: 4.3114e-04\n",
            "Epoch 246/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.0293e-04 - mse: 9.0293e-04\n",
            "Epoch 246: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 9.0293e-04 - mse: 9.0293e-04 - val_loss: 4.2951e-04 - val_mse: 4.2951e-04\n",
            "Epoch 247/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 9.0451e-04 - mse: 9.0451e-04\n",
            "Epoch 247: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.0110e-04 - mse: 9.0110e-04 - val_loss: 4.2935e-04 - val_mse: 4.2935e-04\n",
            "Epoch 248/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.1947e-04 - mse: 9.1947e-04\n",
            "Epoch 248: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 9.0020e-04 - mse: 9.0020e-04 - val_loss: 4.3122e-04 - val_mse: 4.3122e-04\n",
            "Epoch 249/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.1941e-04 - mse: 9.1941e-04\n",
            "Epoch 249: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 9.0008e-04 - mse: 9.0008e-04 - val_loss: 4.2735e-04 - val_mse: 4.2735e-04\n",
            "Epoch 250/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.9642e-04 - mse: 8.9642e-04\n",
            "Epoch 250: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 8.9642e-04 - mse: 8.9642e-04 - val_loss: 4.2552e-04 - val_mse: 4.2552e-04\n",
            "Epoch 251/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 9.3116e-04 - mse: 9.3116e-04\n",
            "Epoch 251: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.9448e-04 - mse: 8.9448e-04 - val_loss: 4.2532e-04 - val_mse: 4.2532e-04\n",
            "Epoch 252/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.9353e-04 - mse: 8.9353e-04\n",
            "Epoch 252: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.9353e-04 - mse: 8.9353e-04 - val_loss: 4.2672e-04 - val_mse: 4.2672e-04\n",
            "Epoch 253/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.1236e-04 - mse: 9.1236e-04\n",
            "Epoch 253: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.9329e-04 - mse: 8.9329e-04 - val_loss: 4.2392e-04 - val_mse: 4.2392e-04\n",
            "Epoch 254/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.0937e-04 - mse: 9.0937e-04\n",
            "Epoch 254: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.9047e-04 - mse: 8.9047e-04 - val_loss: 4.2252e-04 - val_mse: 4.2252e-04\n",
            "Epoch 255/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.8877e-04 - mse: 8.8877e-04\n",
            "Epoch 255: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.8877e-04 - mse: 8.8877e-04 - val_loss: 4.2280e-04 - val_mse: 4.2280e-04\n",
            "Epoch 256/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.0312e-04 - mse: 9.0312e-04\n",
            "Epoch 256: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.8798e-04 - mse: 8.8798e-04 - val_loss: 4.2121e-04 - val_mse: 4.2121e-04\n",
            "Epoch 257/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 9.0089e-04 - mse: 9.0089e-04\n",
            "Epoch 257: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.8583e-04 - mse: 8.8583e-04 - val_loss: 4.2002e-04 - val_mse: 4.2002e-04\n",
            "Epoch 258/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 9.0298e-04 - mse: 9.0298e-04\n",
            "Epoch 258: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 8.8427e-04 - mse: 8.8427e-04 - val_loss: 4.2126e-04 - val_mse: 4.2126e-04\n",
            "Epoch 259/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 9.2032e-04 - mse: 9.2032e-04\n",
            "Epoch 259: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.8395e-04 - mse: 8.8395e-04 - val_loss: 4.1826e-04 - val_mse: 4.1826e-04\n",
            "Epoch 260/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 9.0447e-04 - mse: 9.0447e-04\n",
            "Epoch 260: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.8085e-04 - mse: 8.8085e-04 - val_loss: 4.1660e-04 - val_mse: 4.1660e-04\n",
            "Epoch 261/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 9.4174e-04 - mse: 9.4174e-04\n",
            "Epoch 261: val_loss improved from 0.00042 to 0.00042, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.7896e-04 - mse: 8.7896e-04 - val_loss: 4.1656e-04 - val_mse: 4.1656e-04\n",
            "Epoch 262/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.9658e-04 - mse: 8.9658e-04\n",
            "Epoch 262: val_loss did not improve from 0.00042\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.7807e-04 - mse: 8.7807e-04 - val_loss: 4.1699e-04 - val_mse: 4.1699e-04\n",
            "Epoch 263/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.7733e-04 - mse: 8.7733e-04\n",
            "Epoch 263: val_loss improved from 0.00042 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.7733e-04 - mse: 8.7733e-04 - val_loss: 4.1457e-04 - val_mse: 4.1457e-04\n",
            "Epoch 264/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.9290e-04 - mse: 8.9290e-04\n",
            "Epoch 264: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.7462e-04 - mse: 8.7462e-04 - val_loss: 4.1304e-04 - val_mse: 4.1304e-04\n",
            "Epoch 265/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.8759e-04 - mse: 8.8759e-04\n",
            "Epoch 265: val_loss did not improve from 0.00041\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 8.7286e-04 - mse: 8.7286e-04 - val_loss: 4.1491e-04 - val_mse: 4.1491e-04\n",
            "Epoch 266/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 9.3489e-04 - mse: 9.3489e-04\n",
            "Epoch 266: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.7266e-04 - mse: 8.7266e-04 - val_loss: 4.1129e-04 - val_mse: 4.1129e-04\n",
            "Epoch 267/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.8736e-04 - mse: 8.8736e-04\n",
            "Epoch 267: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 8.6935e-04 - mse: 8.6935e-04 - val_loss: 4.0935e-04 - val_mse: 4.0935e-04\n",
            "Epoch 268/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 9.1624e-04 - mse: 9.1624e-04\n",
            "Epoch 268: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 8.6730e-04 - mse: 8.6730e-04 - val_loss: 4.0915e-04 - val_mse: 4.0915e-04\n",
            "Epoch 269/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.8427e-04 - mse: 8.8427e-04\n",
            "Epoch 269: val_loss did not improve from 0.00041\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 8.6635e-04 - mse: 8.6635e-04 - val_loss: 4.1111e-04 - val_mse: 4.1111e-04\n",
            "Epoch 270/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.8071e-04 - mse: 8.8071e-04\n",
            "Epoch 270: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.6613e-04 - mse: 8.6613e-04 - val_loss: 4.0757e-04 - val_mse: 4.0757e-04\n",
            "Epoch 271/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.8114e-04 - mse: 8.8114e-04\n",
            "Epoch 271: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.6335e-04 - mse: 8.6335e-04 - val_loss: 4.0579e-04 - val_mse: 4.0579e-04\n",
            "Epoch 272/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 9.2242e-04 - mse: 9.2242e-04\n",
            "Epoch 272: val_loss improved from 0.00041 to 0.00041, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.6134e-04 - mse: 8.6134e-04 - val_loss: 4.0551e-04 - val_mse: 4.0551e-04\n",
            "Epoch 273/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 9.2121e-04 - mse: 9.2121e-04\n",
            "Epoch 273: val_loss did not improve from 0.00041\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.6036e-04 - mse: 8.6036e-04 - val_loss: 4.0848e-04 - val_mse: 4.0848e-04\n",
            "Epoch 274/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 9.0947e-04 - mse: 9.0947e-04\n",
            "Epoch 274: val_loss improved from 0.00041 to 0.00040, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.6099e-04 - mse: 8.6099e-04 - val_loss: 4.0367e-04 - val_mse: 4.0367e-04\n",
            "Epoch 275/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 8.9181e-04 - mse: 8.9181e-04\n",
            "Epoch 275: val_loss improved from 0.00040 to 0.00040, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.5674e-04 - mse: 8.5674e-04 - val_loss: 4.0154e-04 - val_mse: 4.0154e-04\n",
            "Epoch 276/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.7662e-04 - mse: 8.7662e-04\n",
            "Epoch 276: val_loss improved from 0.00040 to 0.00040, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.5457e-04 - mse: 8.5457e-04 - val_loss: 4.0125e-04 - val_mse: 4.0125e-04\n",
            "Epoch 277/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.6775e-04 - mse: 8.6775e-04\n",
            "Epoch 277: val_loss did not improve from 0.00040\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.5351e-04 - mse: 8.5351e-04 - val_loss: 4.0302e-04 - val_mse: 4.0302e-04\n",
            "Epoch 278/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.5321e-04 - mse: 8.5321e-04\n",
            "Epoch 278: val_loss improved from 0.00040 to 0.00040, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.5321e-04 - mse: 8.5321e-04 - val_loss: 3.9914e-04 - val_mse: 3.9914e-04\n",
            "Epoch 279/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.8488e-04 - mse: 8.8488e-04\n",
            "Epoch 279: val_loss improved from 0.00040 to 0.00040, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.4986e-04 - mse: 8.4986e-04 - val_loss: 3.9716e-04 - val_mse: 3.9716e-04\n",
            "Epoch 280/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.8273e-04 - mse: 8.8273e-04\n",
            "Epoch 280: val_loss improved from 0.00040 to 0.00040, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.4782e-04 - mse: 8.4782e-04 - val_loss: 3.9691e-04 - val_mse: 3.9691e-04\n",
            "Epoch 281/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 9.0640e-04 - mse: 9.0640e-04\n",
            "Epoch 281: val_loss did not improve from 0.00040\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.4699e-04 - mse: 8.4699e-04 - val_loss: 4.0153e-04 - val_mse: 4.0153e-04\n",
            "Epoch 282/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.6690e-04 - mse: 8.6690e-04\n",
            "Epoch 282: val_loss improved from 0.00040 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.4962e-04 - mse: 8.4962e-04 - val_loss: 3.9496e-04 - val_mse: 3.9496e-04\n",
            "Epoch 283/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.5989e-04 - mse: 8.5989e-04\n",
            "Epoch 283: val_loss improved from 0.00039 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.4304e-04 - mse: 8.4304e-04 - val_loss: 3.9234e-04 - val_mse: 3.9234e-04\n",
            "Epoch 284/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.7496e-04 - mse: 8.7496e-04\n",
            "Epoch 284: val_loss improved from 0.00039 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.4065e-04 - mse: 8.4065e-04 - val_loss: 3.9202e-04 - val_mse: 3.9202e-04\n",
            "Epoch 285/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.5636e-04 - mse: 8.5636e-04\n",
            "Epoch 285: val_loss did not improve from 0.00039\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.3957e-04 - mse: 8.3957e-04 - val_loss: 3.9355e-04 - val_mse: 3.9355e-04\n",
            "Epoch 286/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.5607e-04 - mse: 8.5607e-04\n",
            "Epoch 286: val_loss improved from 0.00039 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.3929e-04 - mse: 8.3929e-04 - val_loss: 3.9055e-04 - val_mse: 3.9055e-04\n",
            "Epoch 287/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.3650e-04 - mse: 8.3650e-04\n",
            "Epoch 287: val_loss improved from 0.00039 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.3650e-04 - mse: 8.3650e-04 - val_loss: 3.8874e-04 - val_mse: 3.8874e-04\n",
            "Epoch 288/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 8.6871e-04 - mse: 8.6871e-04\n",
            "Epoch 288: val_loss did not improve from 0.00039\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 8.3466e-04 - mse: 8.3466e-04 - val_loss: 3.8877e-04 - val_mse: 3.8877e-04\n",
            "Epoch 289/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.5027e-04 - mse: 8.5027e-04\n",
            "Epoch 289: val_loss improved from 0.00039 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.3371e-04 - mse: 8.3371e-04 - val_loss: 3.8812e-04 - val_mse: 3.8812e-04\n",
            "Epoch 290/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 8.3526e-04 - mse: 8.3526e-04\n",
            "Epoch 290: val_loss did not improve from 0.00039\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.3241e-04 - mse: 8.3241e-04 - val_loss: 3.8888e-04 - val_mse: 3.8888e-04\n",
            "Epoch 291/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.6526e-04 - mse: 8.6526e-04\n",
            "Epoch 291: val_loss improved from 0.00039 to 0.00039, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.3159e-04 - mse: 8.3159e-04 - val_loss: 3.8547e-04 - val_mse: 3.8547e-04\n",
            "Epoch 292/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.2877e-04 - mse: 8.2877e-04\n",
            "Epoch 292: val_loss improved from 0.00039 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.2877e-04 - mse: 8.2877e-04 - val_loss: 3.8404e-04 - val_mse: 3.8404e-04\n",
            "Epoch 293/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.4059e-04 - mse: 8.4059e-04\n",
            "Epoch 293: val_loss did not improve from 0.00038\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.2730e-04 - mse: 8.2730e-04 - val_loss: 3.8963e-04 - val_mse: 3.8963e-04\n",
            "Epoch 294/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 8.3230e-04 - mse: 8.3230e-04\n",
            "Epoch 294: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.3230e-04 - mse: 8.3230e-04 - val_loss: 3.8158e-04 - val_mse: 3.8158e-04\n",
            "Epoch 295/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.6751e-04 - mse: 8.6751e-04\n",
            "Epoch 295: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.2267e-04 - mse: 8.2267e-04 - val_loss: 3.7827e-04 - val_mse: 3.7827e-04\n",
            "Epoch 296/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.3975e-04 - mse: 8.3975e-04\n",
            "Epoch 296: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.2007e-04 - mse: 8.2007e-04 - val_loss: 3.7820e-04 - val_mse: 3.7820e-04\n",
            "Epoch 297/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.3869e-04 - mse: 8.3869e-04\n",
            "Epoch 297: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.1901e-04 - mse: 8.1901e-04 - val_loss: 3.7798e-04 - val_mse: 3.7798e-04\n",
            "Epoch 298/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.5068e-04 - mse: 8.5068e-04\n",
            "Epoch 298: val_loss did not improve from 0.00038\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.1810e-04 - mse: 8.1810e-04 - val_loss: 3.7799e-04 - val_mse: 3.7799e-04\n",
            "Epoch 299/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.3702e-04 - mse: 8.3702e-04\n",
            "Epoch 299: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.1731e-04 - mse: 8.1731e-04 - val_loss: 3.7767e-04 - val_mse: 3.7767e-04\n",
            "Epoch 300/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.4874e-04 - mse: 8.4874e-04\n",
            "Epoch 300: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.1624e-04 - mse: 8.1624e-04 - val_loss: 3.7677e-04 - val_mse: 3.7677e-04\n",
            "Epoch 301/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.3450e-04 - mse: 8.3450e-04\n",
            "Epoch 301: val_loss improved from 0.00038 to 0.00038, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.1488e-04 - mse: 8.1488e-04 - val_loss: 3.7660e-04 - val_mse: 3.7660e-04\n",
            "Epoch 302/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.2688e-04 - mse: 8.2688e-04\n",
            "Epoch 302: val_loss improved from 0.00038 to 0.00037, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.1376e-04 - mse: 8.1376e-04 - val_loss: 3.7397e-04 - val_mse: 3.7397e-04\n",
            "Epoch 303/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.5522e-04 - mse: 8.5522e-04\n",
            "Epoch 303: val_loss improved from 0.00037 to 0.00037, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 8.1129e-04 - mse: 8.1129e-04 - val_loss: 3.7258e-04 - val_mse: 3.7258e-04\n",
            "Epoch 304/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.2250e-04 - mse: 8.2250e-04\n",
            "Epoch 304: val_loss did not improve from 0.00037\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 8.0980e-04 - mse: 8.0980e-04 - val_loss: 3.7883e-04 - val_mse: 3.7883e-04\n",
            "Epoch 305/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.3334e-04 - mse: 8.3334e-04\n",
            "Epoch 305: val_loss improved from 0.00037 to 0.00037, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.2007e-04 - mse: 8.2007e-04 - val_loss: 3.6863e-04 - val_mse: 3.6863e-04\n",
            "Epoch 306/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.1705e-04 - mse: 8.1705e-04\n",
            "Epoch 306: val_loss improved from 0.00037 to 0.00037, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.0435e-04 - mse: 8.0435e-04 - val_loss: 3.6549e-04 - val_mse: 3.6549e-04\n",
            "Epoch 307/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.1677e-04 - mse: 8.1677e-04\n",
            "Epoch 307: val_loss did not improve from 0.00037\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.0192e-04 - mse: 8.0192e-04 - val_loss: 3.6589e-04 - val_mse: 3.6589e-04\n",
            "Epoch 308/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.1947e-04 - mse: 8.1947e-04\n",
            "Epoch 308: val_loss did not improve from 0.00037\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.0105e-04 - mse: 8.0105e-04 - val_loss: 3.6580e-04 - val_mse: 3.6580e-04\n",
            "Epoch 309/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 8.1510e-04 - mse: 8.1510e-04\n",
            "Epoch 309: val_loss did not improve from 0.00037\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 8.0023e-04 - mse: 8.0023e-04 - val_loss: 3.6554e-04 - val_mse: 3.6554e-04\n",
            "Epoch 310/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.4223e-04 - mse: 8.4223e-04\n",
            "Epoch 310: val_loss improved from 0.00037 to 0.00037, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.9940e-04 - mse: 7.9940e-04 - val_loss: 3.6531e-04 - val_mse: 3.6531e-04\n",
            "Epoch 311/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.1134e-04 - mse: 8.1134e-04\n",
            "Epoch 311: val_loss did not improve from 0.00037\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.9864e-04 - mse: 7.9864e-04 - val_loss: 3.6546e-04 - val_mse: 3.6546e-04\n",
            "Epoch 312/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 8.5285e-04 - mse: 8.5285e-04\n",
            "Epoch 312: val_loss improved from 0.00037 to 0.00037, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.9796e-04 - mse: 7.9796e-04 - val_loss: 3.6516e-04 - val_mse: 3.6516e-04\n",
            "Epoch 313/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.9703e-04 - mse: 7.9703e-04\n",
            "Epoch 313: val_loss did not improve from 0.00037\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 7.9703e-04 - mse: 7.9703e-04 - val_loss: 3.6526e-04 - val_mse: 3.6526e-04\n",
            "Epoch 314/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.9868e-04 - mse: 7.9868e-04\n",
            "Epoch 314: val_loss improved from 0.00037 to 0.00036, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.9616e-04 - mse: 7.9616e-04 - val_loss: 3.6274e-04 - val_mse: 3.6274e-04\n",
            "Epoch 315/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.2449e-04 - mse: 8.2449e-04\n",
            "Epoch 315: val_loss improved from 0.00036 to 0.00036, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.9381e-04 - mse: 7.9381e-04 - val_loss: 3.6145e-04 - val_mse: 3.6145e-04\n",
            "Epoch 316/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.9451e-04 - mse: 7.9451e-04\n",
            "Epoch 316: val_loss did not improve from 0.00036\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.9222e-04 - mse: 7.9222e-04 - val_loss: 3.6755e-04 - val_mse: 3.6755e-04\n",
            "Epoch 317/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 8.1795e-04 - mse: 8.1795e-04\n",
            "Epoch 317: val_loss improved from 0.00036 to 0.00036, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 8.0512e-04 - mse: 8.0512e-04 - val_loss: 3.5657e-04 - val_mse: 3.5657e-04\n",
            "Epoch 318/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.0417e-04 - mse: 8.0417e-04\n",
            "Epoch 318: val_loss improved from 0.00036 to 0.00035, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.8679e-04 - mse: 7.8679e-04 - val_loss: 3.5497e-04 - val_mse: 3.5497e-04\n",
            "Epoch 319/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 8.0236e-04 - mse: 8.0236e-04\n",
            "Epoch 319: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.8497e-04 - mse: 7.8497e-04 - val_loss: 3.5531e-04 - val_mse: 3.5531e-04\n",
            "Epoch 320/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.9831e-04 - mse: 7.9831e-04\n",
            "Epoch 320: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.8415e-04 - mse: 7.8415e-04 - val_loss: 3.5522e-04 - val_mse: 3.5522e-04\n",
            "Epoch 321/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.9559e-04 - mse: 7.9559e-04\n",
            "Epoch 321: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.8334e-04 - mse: 7.8334e-04 - val_loss: 3.5504e-04 - val_mse: 3.5504e-04\n",
            "Epoch 322/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.9677e-04 - mse: 7.9677e-04\n",
            "Epoch 322: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.8257e-04 - mse: 7.8257e-04 - val_loss: 3.5499e-04 - val_mse: 3.5499e-04\n",
            "Epoch 323/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.2279e-04 - mse: 8.2279e-04\n",
            "Epoch 323: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.8193e-04 - mse: 7.8193e-04 - val_loss: 3.6137e-04 - val_mse: 3.6137e-04\n",
            "Epoch 324/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.9642e-04 - mse: 7.9642e-04\n",
            "Epoch 324: val_loss improved from 0.00035 to 0.00035, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.9389e-04 - mse: 7.9389e-04 - val_loss: 3.5123e-04 - val_mse: 3.5123e-04\n",
            "Epoch 325/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.9078e-04 - mse: 7.9078e-04\n",
            "Epoch 325: val_loss improved from 0.00035 to 0.00035, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.7709e-04 - mse: 7.7709e-04 - val_loss: 3.4876e-04 - val_mse: 3.4876e-04\n",
            "Epoch 326/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.9192e-04 - mse: 7.9192e-04\n",
            "Epoch 326: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.7516e-04 - mse: 7.7516e-04 - val_loss: 3.4907e-04 - val_mse: 3.4907e-04\n",
            "Epoch 327/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.1498e-04 - mse: 8.1498e-04\n",
            "Epoch 327: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.7440e-04 - mse: 7.7440e-04 - val_loss: 3.4904e-04 - val_mse: 3.4904e-04\n",
            "Epoch 328/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.8738e-04 - mse: 7.8738e-04\n",
            "Epoch 328: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.7363e-04 - mse: 7.7363e-04 - val_loss: 3.4890e-04 - val_mse: 3.4890e-04\n",
            "Epoch 329/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 8.0455e-04 - mse: 8.0455e-04\n",
            "Epoch 329: val_loss improved from 0.00035 to 0.00035, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.7288e-04 - mse: 7.7288e-04 - val_loss: 3.4872e-04 - val_mse: 3.4872e-04\n",
            "Epoch 330/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.1267e-04 - mse: 8.1267e-04\n",
            "Epoch 330: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.7215e-04 - mse: 7.7215e-04 - val_loss: 3.4889e-04 - val_mse: 3.4889e-04\n",
            "Epoch 331/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 8.0002e-04 - mse: 8.0002e-04\n",
            "Epoch 331: val_loss did not improve from 0.00035\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.7147e-04 - mse: 7.7147e-04 - val_loss: 3.5562e-04 - val_mse: 3.5562e-04\n",
            "Epoch 332/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.9813e-04 - mse: 7.9813e-04\n",
            "Epoch 332: val_loss improved from 0.00035 to 0.00034, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.8388e-04 - mse: 7.8388e-04 - val_loss: 3.4414e-04 - val_mse: 3.4414e-04\n",
            "Epoch 333/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.6774e-04 - mse: 7.6774e-04\n",
            "Epoch 333: val_loss improved from 0.00034 to 0.00034, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.6556e-04 - mse: 7.6556e-04 - val_loss: 3.4206e-04 - val_mse: 3.4206e-04\n",
            "Epoch 334/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.7549e-04 - mse: 7.7549e-04\n",
            "Epoch 334: val_loss did not improve from 0.00034\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 7.6383e-04 - mse: 7.6383e-04 - val_loss: 3.4226e-04 - val_mse: 3.4226e-04\n",
            "Epoch 335/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 8.0263e-04 - mse: 8.0263e-04\n",
            "Epoch 335: val_loss did not improve from 0.00034\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.6306e-04 - mse: 7.6306e-04 - val_loss: 3.4215e-04 - val_mse: 3.4215e-04\n",
            "Epoch 336/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.7551e-04 - mse: 7.7551e-04\n",
            "Epoch 336: val_loss improved from 0.00034 to 0.00034, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.6226e-04 - mse: 7.6226e-04 - val_loss: 3.4193e-04 - val_mse: 3.4193e-04\n",
            "Epoch 337/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.6367e-04 - mse: 7.6367e-04\n",
            "Epoch 337: val_loss improved from 0.00034 to 0.00034, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.6146e-04 - mse: 7.6146e-04 - val_loss: 3.4166e-04 - val_mse: 3.4166e-04\n",
            "Epoch 338/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.7239e-04 - mse: 7.7239e-04\n",
            "Epoch 338: val_loss did not improve from 0.00034\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.6069e-04 - mse: 7.6069e-04 - val_loss: 3.4218e-04 - val_mse: 3.4218e-04\n",
            "Epoch 339/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.8830e-04 - mse: 7.8830e-04\n",
            "Epoch 339: val_loss did not improve from 0.00034\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.6049e-04 - mse: 7.6049e-04 - val_loss: 3.4891e-04 - val_mse: 3.4891e-04\n",
            "Epoch 340/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.8703e-04 - mse: 7.8703e-04\n",
            "Epoch 340: val_loss improved from 0.00034 to 0.00034, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.7040e-04 - mse: 7.7040e-04 - val_loss: 3.3721e-04 - val_mse: 3.3721e-04\n",
            "Epoch 341/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.6926e-04 - mse: 7.6926e-04\n",
            "Epoch 341: val_loss improved from 0.00034 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.5394e-04 - mse: 7.5394e-04 - val_loss: 3.3467e-04 - val_mse: 3.3467e-04\n",
            "Epoch 342/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.6733e-04 - mse: 7.6733e-04\n",
            "Epoch 342: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.5203e-04 - mse: 7.5203e-04 - val_loss: 3.3490e-04 - val_mse: 3.3490e-04\n",
            "Epoch 343/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.6266e-04 - mse: 7.6266e-04\n",
            "Epoch 343: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.5131e-04 - mse: 7.5131e-04 - val_loss: 3.3482e-04 - val_mse: 3.3482e-04\n",
            "Epoch 344/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.6191e-04 - mse: 7.6191e-04\n",
            "Epoch 344: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.5056e-04 - mse: 7.5056e-04 - val_loss: 3.3466e-04 - val_mse: 3.3466e-04\n",
            "Epoch 345/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.6514e-04 - mse: 7.6514e-04\n",
            "Epoch 345: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.4981e-04 - mse: 7.4981e-04 - val_loss: 3.3443e-04 - val_mse: 3.3443e-04\n",
            "Epoch 346/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.8749e-04 - mse: 7.8749e-04\n",
            "Epoch 346: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.4906e-04 - mse: 7.4906e-04 - val_loss: 3.3420e-04 - val_mse: 3.3420e-04\n",
            "Epoch 347/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.5979e-04 - mse: 7.5979e-04\n",
            "Epoch 347: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.4839e-04 - mse: 7.4839e-04 - val_loss: 3.3580e-04 - val_mse: 3.3580e-04\n",
            "Epoch 348/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.8691e-04 - mse: 7.8691e-04\n",
            "Epoch 348: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.4843e-04 - mse: 7.4843e-04 - val_loss: 3.3372e-04 - val_mse: 3.3372e-04\n",
            "Epoch 349/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.5920e-04 - mse: 7.5920e-04\n",
            "Epoch 349: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.4645e-04 - mse: 7.4645e-04 - val_loss: 3.3405e-04 - val_mse: 3.3405e-04\n",
            "Epoch 350/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.6106e-04 - mse: 7.6106e-04\n",
            "Epoch 350: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.4582e-04 - mse: 7.4582e-04 - val_loss: 3.3201e-04 - val_mse: 3.3201e-04\n",
            "Epoch 351/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.7081e-04 - mse: 7.7081e-04\n",
            "Epoch 351: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.4381e-04 - mse: 7.4381e-04 - val_loss: 3.3214e-04 - val_mse: 3.3214e-04\n",
            "Epoch 352/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.8117e-04 - mse: 7.8117e-04\n",
            "Epoch 352: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.4317e-04 - mse: 7.4317e-04 - val_loss: 3.3045e-04 - val_mse: 3.3045e-04\n",
            "Epoch 353/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.5255e-04 - mse: 7.5255e-04\n",
            "Epoch 353: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.4133e-04 - mse: 7.4133e-04 - val_loss: 3.3096e-04 - val_mse: 3.3096e-04\n",
            "Epoch 354/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.5575e-04 - mse: 7.5575e-04\n",
            "Epoch 354: val_loss improved from 0.00033 to 0.00033, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 7.4082e-04 - mse: 7.4082e-04 - val_loss: 3.2897e-04 - val_mse: 3.2897e-04\n",
            "Epoch 355/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.7628e-04 - mse: 7.7628e-04\n",
            "Epoch 355: val_loss did not improve from 0.00033\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 7.3908e-04 - mse: 7.3908e-04 - val_loss: 3.3460e-04 - val_mse: 3.3460e-04\n",
            "Epoch 356/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.7486e-04 - mse: 7.7486e-04\n",
            "Epoch 356: val_loss improved from 0.00033 to 0.00032, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 7.4765e-04 - mse: 7.4765e-04 - val_loss: 3.2483e-04 - val_mse: 3.2483e-04\n",
            "Epoch 357/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.3312e-04 - mse: 7.3312e-04\n",
            "Epoch 357: val_loss improved from 0.00032 to 0.00032, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 7.3312e-04 - mse: 7.3312e-04 - val_loss: 3.2134e-04 - val_mse: 3.2134e-04\n",
            "Epoch 358/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.7909e-04 - mse: 7.7909e-04\n",
            "Epoch 358: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.3100e-04 - mse: 7.3100e-04 - val_loss: 3.2172e-04 - val_mse: 3.2172e-04\n",
            "Epoch 359/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.4216e-04 - mse: 7.4216e-04\n",
            "Epoch 359: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.3043e-04 - mse: 7.3043e-04 - val_loss: 3.2172e-04 - val_mse: 3.2172e-04\n",
            "Epoch 360/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.5547e-04 - mse: 7.5547e-04\n",
            "Epoch 360: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.2985e-04 - mse: 7.2985e-04 - val_loss: 3.2161e-04 - val_mse: 3.2161e-04\n",
            "Epoch 361/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.5490e-04 - mse: 7.5490e-04\n",
            "Epoch 361: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.2928e-04 - mse: 7.2928e-04 - val_loss: 3.2145e-04 - val_mse: 3.2145e-04\n",
            "Epoch 362/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.7670e-04 - mse: 7.7670e-04\n",
            "Epoch 362: val_loss improved from 0.00032 to 0.00032, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.2870e-04 - mse: 7.2870e-04 - val_loss: 3.2125e-04 - val_mse: 3.2125e-04\n",
            "Epoch 363/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.3889e-04 - mse: 7.3889e-04\n",
            "Epoch 363: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.2812e-04 - mse: 7.2812e-04 - val_loss: 3.2133e-04 - val_mse: 3.2133e-04\n",
            "Epoch 364/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.4087e-04 - mse: 7.4087e-04\n",
            "Epoch 364: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.2737e-04 - mse: 7.2737e-04 - val_loss: 3.2739e-04 - val_mse: 3.2739e-04\n",
            "Epoch 365/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.6931e-04 - mse: 7.6931e-04\n",
            "Epoch 365: val_loss did not improve from 0.00032\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.4258e-04 - mse: 7.4258e-04 - val_loss: 3.2671e-04 - val_mse: 3.2671e-04\n",
            "Epoch 366/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.7027e-04 - mse: 7.7027e-04\n",
            "Epoch 366: val_loss improved from 0.00032 to 0.00032, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.3308e-04 - mse: 7.3308e-04 - val_loss: 3.1795e-04 - val_mse: 3.1795e-04\n",
            "Epoch 367/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.5697e-04 - mse: 7.5697e-04\n",
            "Epoch 367: val_loss improved from 0.00032 to 0.00031, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.2130e-04 - mse: 7.2130e-04 - val_loss: 3.1409e-04 - val_mse: 3.1409e-04\n",
            "Epoch 368/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 7.4877e-04 - mse: 7.4877e-04\n",
            "Epoch 368: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.1910e-04 - mse: 7.1910e-04 - val_loss: 3.1449e-04 - val_mse: 3.1449e-04\n",
            "Epoch 369/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.2982e-04 - mse: 7.2982e-04\n",
            "Epoch 369: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.1860e-04 - mse: 7.1860e-04 - val_loss: 3.1452e-04 - val_mse: 3.1452e-04\n",
            "Epoch 370/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.5364e-04 - mse: 7.5364e-04\n",
            "Epoch 370: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.1809e-04 - mse: 7.1809e-04 - val_loss: 3.1445e-04 - val_mse: 3.1445e-04\n",
            "Epoch 371/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1759e-04 - mse: 7.1759e-04\n",
            "Epoch 371: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.1759e-04 - mse: 7.1759e-04 - val_loss: 3.1438e-04 - val_mse: 3.1438e-04\n",
            "Epoch 372/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.6415e-04 - mse: 7.6415e-04\n",
            "Epoch 372: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.1720e-04 - mse: 7.1720e-04 - val_loss: 3.1536e-04 - val_mse: 3.1536e-04\n",
            "Epoch 373/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.4117e-04 - mse: 7.4117e-04\n",
            "Epoch 373: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.1674e-04 - mse: 7.1674e-04 - val_loss: 3.2145e-04 - val_mse: 3.2145e-04\n",
            "Epoch 374/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.4706e-04 - mse: 7.4706e-04\n",
            "Epoch 374: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.3269e-04 - mse: 7.3269e-04 - val_loss: 3.2165e-04 - val_mse: 3.2165e-04\n",
            "Epoch 375/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.5767e-04 - mse: 7.5767e-04\n",
            "Epoch 375: val_loss did not improve from 0.00031\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.3139e-04 - mse: 7.3139e-04 - val_loss: 3.2198e-04 - val_mse: 3.2198e-04\n",
            "Epoch 376/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 7.5337e-04 - mse: 7.5337e-04\n",
            "Epoch 376: val_loss improved from 0.00031 to 0.00031, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.2217e-04 - mse: 7.2217e-04 - val_loss: 3.0830e-04 - val_mse: 3.0830e-04\n",
            "Epoch 377/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.1855e-04 - mse: 7.1855e-04\n",
            "Epoch 377: val_loss improved from 0.00031 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.0645e-04 - mse: 7.0645e-04 - val_loss: 3.0490e-04 - val_mse: 3.0490e-04\n",
            "Epoch 378/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.0648e-04 - mse: 7.0648e-04\n",
            "Epoch 378: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.0481e-04 - mse: 7.0481e-04 - val_loss: 3.0562e-04 - val_mse: 3.0562e-04\n",
            "Epoch 379/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.0644e-04 - mse: 7.0644e-04\n",
            "Epoch 379: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.0476e-04 - mse: 7.0476e-04 - val_loss: 3.0589e-04 - val_mse: 3.0589e-04\n",
            "Epoch 380/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.5011e-04 - mse: 7.5011e-04\n",
            "Epoch 380: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.0461e-04 - mse: 7.0461e-04 - val_loss: 3.0602e-04 - val_mse: 3.0602e-04\n",
            "Epoch 381/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 7.0616e-04 - mse: 7.0616e-04\n",
            "Epoch 381: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.0444e-04 - mse: 7.0444e-04 - val_loss: 3.0613e-04 - val_mse: 3.0613e-04\n",
            "Epoch 382/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 7.1511e-04 - mse: 7.1511e-04\n",
            "Epoch 382: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.0436e-04 - mse: 7.0436e-04 - val_loss: 3.0716e-04 - val_mse: 3.0716e-04\n",
            "Epoch 383/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.4873e-04 - mse: 7.4873e-04\n",
            "Epoch 383: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.0341e-04 - mse: 7.0341e-04 - val_loss: 3.1477e-04 - val_mse: 3.1477e-04\n",
            "Epoch 384/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.5734e-04 - mse: 7.5734e-04\n",
            "Epoch 384: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.2108e-04 - mse: 7.2108e-04 - val_loss: 3.1537e-04 - val_mse: 3.1537e-04\n",
            "Epoch 385/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.6797e-04 - mse: 7.6797e-04\n",
            "Epoch 385: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.2024e-04 - mse: 7.2024e-04 - val_loss: 3.1538e-04 - val_mse: 3.1538e-04\n",
            "Epoch 386/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.3229e-04 - mse: 7.3229e-04\n",
            "Epoch 386: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.1831e-04 - mse: 7.1831e-04 - val_loss: 3.1475e-04 - val_mse: 3.1475e-04\n",
            "Epoch 387/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.2934e-04 - mse: 7.2934e-04\n",
            "Epoch 387: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.1551e-04 - mse: 7.1551e-04 - val_loss: 3.1350e-04 - val_mse: 3.1350e-04\n",
            "Epoch 388/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.3594e-04 - mse: 7.3594e-04\n",
            "Epoch 388: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.1097e-04 - mse: 7.1097e-04 - val_loss: 3.1087e-04 - val_mse: 3.1087e-04\n",
            "Epoch 389/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 7.1215e-04 - mse: 7.1215e-04\n",
            "Epoch 389: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.9938e-04 - mse: 6.9938e-04 - val_loss: 3.0715e-04 - val_mse: 3.0715e-04\n",
            "Epoch 390/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 7.0277e-04 - mse: 7.0277e-04\n",
            "Epoch 390: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.9283e-04 - mse: 6.9283e-04 - val_loss: 3.0990e-04 - val_mse: 3.0990e-04\n",
            "Epoch 391/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.3094e-04 - mse: 7.3094e-04\n",
            "Epoch 391: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 7.0636e-04 - mse: 7.0636e-04 - val_loss: 3.0925e-04 - val_mse: 3.0925e-04\n",
            "Epoch 392/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.3482e-04 - mse: 7.3482e-04\n",
            "Epoch 392: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 7.0017e-04 - mse: 7.0017e-04 - val_loss: 3.0518e-04 - val_mse: 3.0518e-04\n",
            "Epoch 393/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.2193e-04 - mse: 7.2193e-04\n",
            "Epoch 393: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.8864e-04 - mse: 6.8864e-04 - val_loss: 3.0704e-04 - val_mse: 3.0704e-04\n",
            "Epoch 394/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 7.3052e-04 - mse: 7.3052e-04\n",
            "Epoch 394: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 7.0048e-04 - mse: 7.0048e-04 - val_loss: 3.0606e-04 - val_mse: 3.0606e-04\n",
            "Epoch 395/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.3852e-04 - mse: 7.3852e-04\n",
            "Epoch 395: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.9351e-04 - mse: 6.9351e-04 - val_loss: 3.0443e-04 - val_mse: 3.0443e-04\n",
            "Epoch 396/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.9991e-04 - mse: 6.9991e-04\n",
            "Epoch 396: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.8938e-04 - mse: 6.8938e-04 - val_loss: 3.0341e-04 - val_mse: 3.0341e-04\n",
            "Epoch 397/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.9961e-04 - mse: 6.9961e-04\n",
            "Epoch 397: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.8775e-04 - mse: 6.8775e-04 - val_loss: 3.0218e-04 - val_mse: 3.0218e-04\n",
            "Epoch 398/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.9689e-04 - mse: 6.9689e-04\n",
            "Epoch 398: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.8702e-04 - mse: 6.8702e-04 - val_loss: 3.0221e-04 - val_mse: 3.0221e-04\n",
            "Epoch 399/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 7.3162e-04 - mse: 7.3162e-04\n",
            "Epoch 399: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.8727e-04 - mse: 6.8727e-04 - val_loss: 3.0128e-04 - val_mse: 3.0128e-04\n",
            "Epoch 400/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.0812e-04 - mse: 7.0812e-04\n",
            "Epoch 400: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.8536e-04 - mse: 6.8536e-04 - val_loss: 3.0030e-04 - val_mse: 3.0030e-04\n",
            "Epoch 401/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.9533e-04 - mse: 6.9533e-04\n",
            "Epoch 401: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.8369e-04 - mse: 6.8369e-04 - val_loss: 2.9936e-04 - val_mse: 2.9936e-04\n",
            "Epoch 402/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 7.0449e-04 - mse: 7.0449e-04\n",
            "Epoch 402: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.8204e-04 - mse: 6.8204e-04 - val_loss: 2.9846e-04 - val_mse: 2.9846e-04\n",
            "Epoch 403/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.6102e-04 - mse: 6.6102e-04\n",
            "Epoch 403: val_loss did not improve from 0.00030\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.6102e-04 - mse: 6.6102e-04 - val_loss: 3.0148e-04 - val_mse: 3.0148e-04\n",
            "Epoch 404/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.1523e-04 - mse: 7.1523e-04\n",
            "Epoch 404: val_loss improved from 0.00030 to 0.00030, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.8227e-04 - mse: 6.8227e-04 - val_loss: 2.9688e-04 - val_mse: 2.9688e-04\n",
            "Epoch 405/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.8603e-04 - mse: 6.8603e-04\n",
            "Epoch 405: val_loss improved from 0.00030 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.7518e-04 - mse: 6.7518e-04 - val_loss: 2.9435e-04 - val_mse: 2.9435e-04\n",
            "Epoch 406/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.8634e-04 - mse: 6.8634e-04\n",
            "Epoch 406: val_loss did not improve from 0.00029\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.7539e-04 - mse: 6.7539e-04 - val_loss: 2.9445e-04 - val_mse: 2.9445e-04\n",
            "Epoch 407/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.8566e-04 - mse: 6.8566e-04\n",
            "Epoch 407: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.7472e-04 - mse: 6.7472e-04 - val_loss: 2.9416e-04 - val_mse: 2.9416e-04\n",
            "Epoch 408/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.7400e-04 - mse: 6.7400e-04\n",
            "Epoch 408: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.7400e-04 - mse: 6.7400e-04 - val_loss: 2.9365e-04 - val_mse: 2.9365e-04\n",
            "Epoch 409/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.0515e-04 - mse: 7.0515e-04\n",
            "Epoch 409: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.7325e-04 - mse: 6.7325e-04 - val_loss: 2.9310e-04 - val_mse: 2.9310e-04\n",
            "Epoch 410/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.8332e-04 - mse: 6.8332e-04\n",
            "Epoch 410: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.7252e-04 - mse: 6.7252e-04 - val_loss: 2.9259e-04 - val_mse: 2.9259e-04\n",
            "Epoch 411/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.8259e-04 - mse: 6.8259e-04\n",
            "Epoch 411: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.7183e-04 - mse: 6.7183e-04 - val_loss: 2.9203e-04 - val_mse: 2.9203e-04\n",
            "Epoch 412/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.8103e-04 - mse: 6.8103e-04\n",
            "Epoch 412: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.7146e-04 - mse: 6.7146e-04 - val_loss: 2.9147e-04 - val_mse: 2.9147e-04\n",
            "Epoch 413/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.8058e-04 - mse: 6.8058e-04\n",
            "Epoch 413: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.7124e-04 - mse: 6.7124e-04 - val_loss: 2.9100e-04 - val_mse: 2.9100e-04\n",
            "Epoch 414/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.7253e-04 - mse: 6.7253e-04\n",
            "Epoch 414: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.7095e-04 - mse: 6.7095e-04 - val_loss: 2.9039e-04 - val_mse: 2.9039e-04\n",
            "Epoch 415/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.7050e-04 - mse: 6.7050e-04\n",
            "Epoch 415: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.6895e-04 - mse: 6.6895e-04 - val_loss: 2.8966e-04 - val_mse: 2.8966e-04\n",
            "Epoch 416/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.6485e-04 - mse: 6.6485e-04\n",
            "Epoch 416: val_loss did not improve from 0.00029\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.6485e-04 - mse: 6.6485e-04 - val_loss: 2.9057e-04 - val_mse: 2.9057e-04\n",
            "Epoch 417/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.7204e-04 - mse: 6.7204e-04\n",
            "Epoch 417: val_loss did not improve from 0.00029\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.6186e-04 - mse: 6.6186e-04 - val_loss: 2.9109e-04 - val_mse: 2.9109e-04\n",
            "Epoch 418/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 7.0028e-04 - mse: 7.0028e-04\n",
            "Epoch 418: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.6868e-04 - mse: 6.6868e-04 - val_loss: 2.8853e-04 - val_mse: 2.8853e-04\n",
            "Epoch 419/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.5243e-04 - mse: 6.5243e-04\n",
            "Epoch 419: val_loss did not improve from 0.00029\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.4512e-04 - mse: 6.4512e-04 - val_loss: 2.9102e-04 - val_mse: 2.9102e-04\n",
            "Epoch 420/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.7766e-04 - mse: 6.7766e-04\n",
            "Epoch 420: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.6809e-04 - mse: 6.6809e-04 - val_loss: 2.8715e-04 - val_mse: 2.8715e-04\n",
            "Epoch 421/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.7008e-04 - mse: 6.7008e-04\n",
            "Epoch 421: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.6016e-04 - mse: 6.6016e-04 - val_loss: 2.8548e-04 - val_mse: 2.8548e-04\n",
            "Epoch 422/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.7356e-04 - mse: 6.7356e-04\n",
            "Epoch 422: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.6331e-04 - mse: 6.6331e-04 - val_loss: 2.8533e-04 - val_mse: 2.8533e-04\n",
            "Epoch 423/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.9053e-04 - mse: 6.9053e-04\n",
            "Epoch 423: val_loss improved from 0.00029 to 0.00029, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.5986e-04 - mse: 6.5986e-04 - val_loss: 2.8503e-04 - val_mse: 2.8503e-04\n",
            "Epoch 424/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.7134e-04 - mse: 6.7134e-04\n",
            "Epoch 424: val_loss improved from 0.00029 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.6120e-04 - mse: 6.6120e-04 - val_loss: 2.8462e-04 - val_mse: 2.8462e-04\n",
            "Epoch 425/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.6054e-04 - mse: 6.6054e-04\n",
            "Epoch 425: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.5907e-04 - mse: 6.5907e-04 - val_loss: 2.8424e-04 - val_mse: 2.8424e-04\n",
            "Epoch 426/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.6177e-04 - mse: 6.6177e-04\n",
            "Epoch 426: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.6027e-04 - mse: 6.6027e-04 - val_loss: 2.8377e-04 - val_mse: 2.8377e-04\n",
            "Epoch 427/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.6781e-04 - mse: 6.6781e-04\n",
            "Epoch 427: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.5788e-04 - mse: 6.5788e-04 - val_loss: 2.8343e-04 - val_mse: 2.8343e-04\n",
            "Epoch 428/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.8500e-04 - mse: 6.8500e-04\n",
            "Epoch 428: val_loss did not improve from 0.00028\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.5465e-04 - mse: 6.5465e-04 - val_loss: 2.8459e-04 - val_mse: 2.8459e-04\n",
            "Epoch 429/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.2627e-04 - mse: 6.2627e-04\n",
            "Epoch 429: val_loss did not improve from 0.00028\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.2541e-04 - mse: 6.2541e-04 - val_loss: 2.8630e-04 - val_mse: 2.8630e-04\n",
            "Epoch 430/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.6862e-04 - mse: 6.6862e-04\n",
            "Epoch 430: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.5936e-04 - mse: 6.5936e-04 - val_loss: 2.8175e-04 - val_mse: 2.8175e-04\n",
            "Epoch 431/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.4951e-04 - mse: 6.4951e-04\n",
            "Epoch 431: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.4951e-04 - mse: 6.4951e-04 - val_loss: 2.7903e-04 - val_mse: 2.7903e-04\n",
            "Epoch 432/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.6165e-04 - mse: 6.6165e-04\n",
            "Epoch 432: val_loss did not improve from 0.00028\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.5219e-04 - mse: 6.5219e-04 - val_loss: 2.7911e-04 - val_mse: 2.7911e-04\n",
            "Epoch 433/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.5976e-04 - mse: 6.5976e-04\n",
            "Epoch 433: val_loss did not improve from 0.00028\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.5041e-04 - mse: 6.5041e-04 - val_loss: 2.7908e-04 - val_mse: 2.7908e-04\n",
            "Epoch 434/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.6181e-04 - mse: 6.6181e-04\n",
            "Epoch 434: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.5226e-04 - mse: 6.5226e-04 - val_loss: 2.7884e-04 - val_mse: 2.7884e-04\n",
            "Epoch 435/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.8029e-04 - mse: 6.8029e-04\n",
            "Epoch 435: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.5046e-04 - mse: 6.5046e-04 - val_loss: 2.7858e-04 - val_mse: 2.7858e-04\n",
            "Epoch 436/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.5960e-04 - mse: 6.5960e-04\n",
            "Epoch 436: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.5014e-04 - mse: 6.5014e-04 - val_loss: 2.7824e-04 - val_mse: 2.7824e-04\n",
            "Epoch 437/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.5828e-04 - mse: 6.5828e-04\n",
            "Epoch 437: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.4890e-04 - mse: 6.4890e-04 - val_loss: 2.7813e-04 - val_mse: 2.7813e-04\n",
            "Epoch 438/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.4634e-04 - mse: 6.4634e-04\n",
            "Epoch 438: val_loss did not improve from 0.00028\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.4634e-04 - mse: 6.4634e-04 - val_loss: 2.7889e-04 - val_mse: 2.7889e-04\n",
            "Epoch 439/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.3037e-04 - mse: 6.3037e-04\n",
            "Epoch 439: val_loss did not improve from 0.00028\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0717e-04 - mse: 6.0717e-04 - val_loss: 2.8156e-04 - val_mse: 2.8156e-04\n",
            "Epoch 440/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 6.8046e-04 - mse: 6.8046e-04\n",
            "Epoch 440: val_loss improved from 0.00028 to 0.00028, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 6.5203e-04 - mse: 6.5203e-04 - val_loss: 2.7602e-04 - val_mse: 2.7602e-04\n",
            "Epoch 441/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.4659e-04 - mse: 6.4659e-04\n",
            "Epoch 441: val_loss improved from 0.00028 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 6.3833e-04 - mse: 6.3833e-04 - val_loss: 2.7321e-04 - val_mse: 2.7321e-04\n",
            "Epoch 442/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.5124e-04 - mse: 6.5124e-04\n",
            "Epoch 442: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 6.4305e-04 - mse: 6.4305e-04 - val_loss: 2.7343e-04 - val_mse: 2.7343e-04\n",
            "Epoch 443/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.4132e-04 - mse: 6.4132e-04\n",
            "Epoch 443: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 6.4132e-04 - mse: 6.4132e-04 - val_loss: 2.7341e-04 - val_mse: 2.7341e-04\n",
            "Epoch 444/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.7044e-04 - mse: 6.7044e-04\n",
            "Epoch 444: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.4148e-04 - mse: 6.4148e-04 - val_loss: 2.7330e-04 - val_mse: 2.7330e-04\n",
            "Epoch 445/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.6978e-04 - mse: 6.6978e-04\n",
            "Epoch 445: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.4085e-04 - mse: 6.4085e-04 - val_loss: 2.7324e-04 - val_mse: 2.7324e-04\n",
            "Epoch 446/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.4226e-04 - mse: 6.4226e-04\n",
            "Epoch 446: val_loss improved from 0.00027 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.4094e-04 - mse: 6.4094e-04 - val_loss: 2.7303e-04 - val_mse: 2.7303e-04\n",
            "Epoch 447/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.4151e-04 - mse: 6.4151e-04\n",
            "Epoch 447: val_loss improved from 0.00027 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 6.4019e-04 - mse: 6.4019e-04 - val_loss: 2.7299e-04 - val_mse: 2.7299e-04\n",
            "Epoch 448/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.7698e-04 - mse: 6.7698e-04\n",
            "Epoch 448: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.3789e-04 - mse: 6.3789e-04 - val_loss: 2.7370e-04 - val_mse: 2.7370e-04\n",
            "Epoch 449/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.1979e-04 - mse: 6.1979e-04\n",
            "Epoch 449: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.0650e-04 - mse: 6.0650e-04 - val_loss: 2.7554e-04 - val_mse: 2.7554e-04\n",
            "Epoch 450/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.5024e-04 - mse: 6.5024e-04\n",
            "Epoch 450: val_loss improved from 0.00027 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.4174e-04 - mse: 6.4174e-04 - val_loss: 2.7172e-04 - val_mse: 2.7172e-04\n",
            "Epoch 451/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.3936e-04 - mse: 6.3936e-04\n",
            "Epoch 451: val_loss improved from 0.00027 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.3175e-04 - mse: 6.3175e-04 - val_loss: 2.6899e-04 - val_mse: 2.6899e-04\n",
            "Epoch 452/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.4233e-04 - mse: 6.4233e-04\n",
            "Epoch 452: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.3413e-04 - mse: 6.3413e-04 - val_loss: 2.6917e-04 - val_mse: 2.6917e-04\n",
            "Epoch 453/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.5207e-04 - mse: 6.5207e-04\n",
            "Epoch 453: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.3352e-04 - mse: 6.3352e-04 - val_loss: 2.6919e-04 - val_mse: 2.6919e-04\n",
            "Epoch 454/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.4201e-04 - mse: 6.4201e-04\n",
            "Epoch 454: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.3379e-04 - mse: 6.3379e-04 - val_loss: 2.6915e-04 - val_mse: 2.6915e-04\n",
            "Epoch 455/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.3319e-04 - mse: 6.3319e-04\n",
            "Epoch 455: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.3319e-04 - mse: 6.3319e-04 - val_loss: 2.6910e-04 - val_mse: 2.6910e-04\n",
            "Epoch 456/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.3478e-04 - mse: 6.3478e-04\n",
            "Epoch 456: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.3351e-04 - mse: 6.3351e-04 - val_loss: 2.6915e-04 - val_mse: 2.6915e-04\n",
            "Epoch 457/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.3909e-04 - mse: 6.3909e-04\n",
            "Epoch 457: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.3066e-04 - mse: 6.3066e-04 - val_loss: 2.7000e-04 - val_mse: 2.7000e-04\n",
            "Epoch 458/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.0960e-04 - mse: 6.0960e-04\n",
            "Epoch 458: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0483e-04 - mse: 6.0483e-04 - val_loss: 2.7457e-04 - val_mse: 2.7457e-04\n",
            "Epoch 459/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.4375e-04 - mse: 6.4375e-04\n",
            "Epoch 459: val_loss improved from 0.00027 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.3500e-04 - mse: 6.3500e-04 - val_loss: 2.6781e-04 - val_mse: 2.6781e-04\n",
            "Epoch 460/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.4458e-04 - mse: 6.4458e-04\n",
            "Epoch 460: val_loss improved from 0.00027 to 0.00027, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.2660e-04 - mse: 6.2660e-04 - val_loss: 2.6603e-04 - val_mse: 2.6603e-04\n",
            "Epoch 461/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.5721e-04 - mse: 6.5721e-04\n",
            "Epoch 461: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.2929e-04 - mse: 6.2929e-04 - val_loss: 2.6607e-04 - val_mse: 2.6607e-04\n",
            "Epoch 462/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.4577e-04 - mse: 6.4577e-04\n",
            "Epoch 462: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.2758e-04 - mse: 6.2758e-04 - val_loss: 2.6608e-04 - val_mse: 2.6608e-04\n",
            "Epoch 463/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.4602e-04 - mse: 6.4602e-04\n",
            "Epoch 463: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.2775e-04 - mse: 6.2775e-04 - val_loss: 2.6639e-04 - val_mse: 2.6639e-04\n",
            "Epoch 464/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.5332e-04 - mse: 6.5332e-04\n",
            "Epoch 464: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.2560e-04 - mse: 6.2560e-04 - val_loss: 2.6719e-04 - val_mse: 2.6719e-04\n",
            "Epoch 465/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.1720e-04 - mse: 6.1720e-04\n",
            "Epoch 465: val_loss did not improve from 0.00027\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.9437e-04 - mse: 5.9437e-04 - val_loss: 2.7077e-04 - val_mse: 2.7077e-04\n",
            "Epoch 466/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.3851e-04 - mse: 6.3851e-04\n",
            "Epoch 466: val_loss improved from 0.00027 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.3025e-04 - mse: 6.3025e-04 - val_loss: 2.6465e-04 - val_mse: 2.6465e-04\n",
            "Epoch 467/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.4535e-04 - mse: 6.4535e-04\n",
            "Epoch 467: val_loss improved from 0.00026 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.1857e-04 - mse: 6.1857e-04 - val_loss: 2.6256e-04 - val_mse: 2.6256e-04\n",
            "Epoch 468/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.5976e-04 - mse: 6.5976e-04\n",
            "Epoch 468: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.2234e-04 - mse: 6.2234e-04 - val_loss: 2.6276e-04 - val_mse: 2.6276e-04\n",
            "Epoch 469/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.5869e-04 - mse: 6.5869e-04\n",
            "Epoch 469: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.2136e-04 - mse: 6.2136e-04 - val_loss: 2.6294e-04 - val_mse: 2.6294e-04\n",
            "Epoch 470/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 6.4898e-04 - mse: 6.4898e-04\n",
            "Epoch 470: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.2231e-04 - mse: 6.2231e-04 - val_loss: 2.6299e-04 - val_mse: 2.6299e-04\n",
            "Epoch 471/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.2125e-04 - mse: 6.2125e-04\n",
            "Epoch 471: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.2125e-04 - mse: 6.2125e-04 - val_loss: 2.6306e-04 - val_mse: 2.6306e-04\n",
            "Epoch 472/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.2183e-04 - mse: 6.2183e-04\n",
            "Epoch 472: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 6.2062e-04 - mse: 6.2062e-04 - val_loss: 2.6384e-04 - val_mse: 2.6384e-04\n",
            "Epoch 473/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.4383e-04 - mse: 6.4383e-04\n",
            "Epoch 473: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.0752e-04 - mse: 6.0752e-04 - val_loss: 2.6907e-04 - val_mse: 2.6907e-04\n",
            "Epoch 474/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.4954e-04 - mse: 6.4954e-04\n",
            "Epoch 474: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.2195e-04 - mse: 6.2195e-04 - val_loss: 2.6408e-04 - val_mse: 2.6408e-04\n",
            "Epoch 475/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.1219e-04 - mse: 6.1219e-04\n",
            "Epoch 475: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.8944e-04 - mse: 5.8944e-04 - val_loss: 2.6720e-04 - val_mse: 2.6720e-04\n",
            "Epoch 476/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.2296e-04 - mse: 6.2296e-04\n",
            "Epoch 476: val_loss improved from 0.00026 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.2296e-04 - mse: 6.2296e-04 - val_loss: 2.6052e-04 - val_mse: 2.6052e-04\n",
            "Epoch 477/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.4851e-04 - mse: 6.4851e-04\n",
            "Epoch 477: val_loss improved from 0.00026 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.1222e-04 - mse: 6.1222e-04 - val_loss: 2.5872e-04 - val_mse: 2.5872e-04\n",
            "Epoch 478/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 6.1666e-04 - mse: 6.1666e-04\n",
            "Epoch 478: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.1552e-04 - mse: 6.1552e-04 - val_loss: 2.5895e-04 - val_mse: 2.5895e-04\n",
            "Epoch 479/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.3091e-04 - mse: 6.3091e-04\n",
            "Epoch 479: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.1368e-04 - mse: 6.1368e-04 - val_loss: 2.5922e-04 - val_mse: 2.5922e-04\n",
            "Epoch 480/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.1468e-04 - mse: 6.1468e-04\n",
            "Epoch 480: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.1468e-04 - mse: 6.1468e-04 - val_loss: 2.5948e-04 - val_mse: 2.5948e-04\n",
            "Epoch 481/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.4925e-04 - mse: 6.4925e-04\n",
            "Epoch 481: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.1268e-04 - mse: 6.1268e-04 - val_loss: 2.6064e-04 - val_mse: 2.6064e-04\n",
            "Epoch 482/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.2591e-04 - mse: 6.2591e-04\n",
            "Epoch 482: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.9148e-04 - mse: 5.9148e-04 - val_loss: 2.6657e-04 - val_mse: 2.6657e-04\n",
            "Epoch 483/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.4436e-04 - mse: 6.4436e-04\n",
            "Epoch 483: val_loss improved from 0.00026 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.1721e-04 - mse: 6.1721e-04 - val_loss: 2.5840e-04 - val_mse: 2.5840e-04\n",
            "Epoch 484/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.4553e-04 - mse: 6.4553e-04\n",
            "Epoch 484: val_loss improved from 0.00026 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0943e-04 - mse: 6.0943e-04 - val_loss: 2.5730e-04 - val_mse: 2.5730e-04\n",
            "Epoch 485/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.2925e-04 - mse: 6.2925e-04\n",
            "Epoch 485: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 6.1197e-04 - mse: 6.1197e-04 - val_loss: 2.5840e-04 - val_mse: 2.5840e-04\n",
            "Epoch 486/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.2856e-04 - mse: 6.2856e-04\n",
            "Epoch 486: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.0267e-04 - mse: 6.0267e-04 - val_loss: 2.6223e-04 - val_mse: 2.6223e-04\n",
            "Epoch 487/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.0145e-04 - mse: 6.0145e-04\n",
            "Epoch 487: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0145e-04 - mse: 6.0145e-04 - val_loss: 2.6399e-04 - val_mse: 2.6399e-04\n",
            "Epoch 488/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.2926e-04 - mse: 6.2926e-04\n",
            "Epoch 488: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0317e-04 - mse: 6.0317e-04 - val_loss: 2.6254e-04 - val_mse: 2.6254e-04\n",
            "Epoch 489/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.9237e-04 - mse: 5.9237e-04\n",
            "Epoch 489: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.9139e-04 - mse: 5.9139e-04 - val_loss: 2.6374e-04 - val_mse: 2.6374e-04\n",
            "Epoch 490/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.1846e-04 - mse: 6.1846e-04\n",
            "Epoch 490: val_loss improved from 0.00026 to 0.00026, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 6.1128e-04 - mse: 6.1128e-04 - val_loss: 2.5560e-04 - val_mse: 2.5560e-04\n",
            "Epoch 491/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 6.2921e-04 - mse: 6.2921e-04\n",
            "Epoch 491: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0346e-04 - mse: 6.0346e-04 - val_loss: 2.5597e-04 - val_mse: 2.5597e-04\n",
            "Epoch 492/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.0462e-04 - mse: 6.0462e-04\n",
            "Epoch 492: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.9718e-04 - mse: 5.9718e-04 - val_loss: 2.6136e-04 - val_mse: 2.6136e-04\n",
            "Epoch 493/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.1228e-04 - mse: 6.1228e-04\n",
            "Epoch 493: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.8793e-04 - mse: 5.8793e-04 - val_loss: 2.6289e-04 - val_mse: 2.6289e-04\n",
            "Epoch 494/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.2505e-04 - mse: 6.2505e-04\n",
            "Epoch 494: val_loss improved from 0.00026 to 0.00025, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0809e-04 - mse: 6.0809e-04 - val_loss: 2.5370e-04 - val_mse: 2.5370e-04\n",
            "Epoch 495/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 6.0730e-04 - mse: 6.0730e-04\n",
            "Epoch 495: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.0003e-04 - mse: 6.0003e-04 - val_loss: 2.5430e-04 - val_mse: 2.5430e-04\n",
            "Epoch 496/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 6.1552e-04 - mse: 6.1552e-04\n",
            "Epoch 496: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.8951e-04 - mse: 5.8951e-04 - val_loss: 2.6146e-04 - val_mse: 2.6146e-04\n",
            "Epoch 497/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.1139e-04 - mse: 6.1139e-04\n",
            "Epoch 497: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0443e-04 - mse: 6.0443e-04 - val_loss: 2.5416e-04 - val_mse: 2.5416e-04\n",
            "Epoch 498/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.9327e-04 - mse: 5.9327e-04\n",
            "Epoch 498: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.8611e-04 - mse: 5.8611e-04 - val_loss: 2.6118e-04 - val_mse: 2.6118e-04\n",
            "Epoch 499/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 6.1022e-04 - mse: 6.1022e-04\n",
            "Epoch 499: val_loss improved from 0.00025 to 0.00025, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 6.0333e-04 - mse: 6.0333e-04 - val_loss: 2.5317e-04 - val_mse: 2.5317e-04\n",
            "Epoch 500/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.1019e-04 - mse: 6.1019e-04\n",
            "Epoch 500: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.8568e-04 - mse: 5.8568e-04 - val_loss: 2.6005e-04 - val_mse: 2.6005e-04\n",
            "Epoch 501/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.0585e-04 - mse: 6.0585e-04\n",
            "Epoch 501: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.9891e-04 - mse: 5.9891e-04 - val_loss: 2.5514e-04 - val_mse: 2.5514e-04\n",
            "Epoch 502/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.5613e-04 - mse: 5.5613e-04\n",
            "Epoch 502: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5073e-04 - mse: 5.5073e-04 - val_loss: 2.6367e-04 - val_mse: 2.6367e-04\n",
            "Epoch 503/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.0617e-04 - mse: 6.0617e-04\n",
            "Epoch 503: val_loss improved from 0.00025 to 0.00025, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 6.0617e-04 - mse: 6.0617e-04 - val_loss: 2.4914e-04 - val_mse: 2.4914e-04\n",
            "Epoch 504/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.1950e-04 - mse: 6.1950e-04\n",
            "Epoch 504: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.8598e-04 - mse: 5.8598e-04 - val_loss: 2.5083e-04 - val_mse: 2.5083e-04\n",
            "Epoch 505/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.5577e-04 - mse: 5.5577e-04\n",
            "Epoch 505: val_loss did not improve from 0.00025\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5242e-04 - mse: 5.5242e-04 - val_loss: 2.6180e-04 - val_mse: 2.6180e-04\n",
            "Epoch 506/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.9857e-04 - mse: 5.9857e-04\n",
            "Epoch 506: val_loss improved from 0.00025 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.9857e-04 - mse: 5.9857e-04 - val_loss: 2.4356e-04 - val_mse: 2.4356e-04\n",
            "Epoch 507/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.8700e-04 - mse: 5.8700e-04\n",
            "Epoch 507: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.8169e-04 - mse: 5.8169e-04 - val_loss: 2.4733e-04 - val_mse: 2.4733e-04\n",
            "Epoch 508/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.6537e-04 - mse: 5.6537e-04\n",
            "Epoch 508: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6466e-04 - mse: 5.6466e-04 - val_loss: 2.5767e-04 - val_mse: 2.5767e-04\n",
            "Epoch 509/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.9664e-04 - mse: 5.9664e-04\n",
            "Epoch 509: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.9568e-04 - mse: 5.9568e-04 - val_loss: 2.4461e-04 - val_mse: 2.4461e-04\n",
            "Epoch 510/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 6.0012e-04 - mse: 6.0012e-04\n",
            "Epoch 510: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.8543e-04 - mse: 5.8543e-04 - val_loss: 2.4553e-04 - val_mse: 2.4553e-04\n",
            "Epoch 511/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.0842e-04 - mse: 6.0842e-04\n",
            "Epoch 511: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.8441e-04 - mse: 5.8441e-04 - val_loss: 2.5018e-04 - val_mse: 2.5018e-04\n",
            "Epoch 512/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.5471e-04 - mse: 5.5471e-04\n",
            "Epoch 512: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5124e-04 - mse: 5.5124e-04 - val_loss: 2.5961e-04 - val_mse: 2.5961e-04\n",
            "Epoch 513/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 6.0231e-04 - mse: 6.0231e-04\n",
            "Epoch 513: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.9614e-04 - mse: 5.9614e-04 - val_loss: 2.4376e-04 - val_mse: 2.4376e-04\n",
            "Epoch 514/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.7981e-04 - mse: 5.7981e-04\n",
            "Epoch 514: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.7981e-04 - mse: 5.7981e-04 - val_loss: 2.4511e-04 - val_mse: 2.4511e-04\n",
            "Epoch 515/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.6803e-04 - mse: 5.6803e-04\n",
            "Epoch 515: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.6189e-04 - mse: 5.6189e-04 - val_loss: 2.5508e-04 - val_mse: 2.5508e-04\n",
            "Epoch 516/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.8986e-04 - mse: 5.8986e-04\n",
            "Epoch 516: val_loss improved from 0.00024 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.8986e-04 - mse: 5.8986e-04 - val_loss: 2.4304e-04 - val_mse: 2.4304e-04\n",
            "Epoch 517/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.9315e-04 - mse: 5.9315e-04\n",
            "Epoch 517: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.7872e-04 - mse: 5.7872e-04 - val_loss: 2.4603e-04 - val_mse: 2.4603e-04\n",
            "Epoch 518/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.7574e-04 - mse: 5.7574e-04\n",
            "Epoch 518: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4624e-04 - mse: 5.4624e-04 - val_loss: 2.5812e-04 - val_mse: 2.5812e-04\n",
            "Epoch 519/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.8925e-04 - mse: 5.8925e-04\n",
            "Epoch 519: val_loss improved from 0.00024 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.8925e-04 - mse: 5.8925e-04 - val_loss: 2.4131e-04 - val_mse: 2.4131e-04\n",
            "Epoch 520/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.8037e-04 - mse: 5.8037e-04\n",
            "Epoch 520: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.7549e-04 - mse: 5.7549e-04 - val_loss: 2.4410e-04 - val_mse: 2.4410e-04\n",
            "Epoch 521/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.7343e-04 - mse: 5.7343e-04\n",
            "Epoch 521: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.4412e-04 - mse: 5.4412e-04 - val_loss: 2.5669e-04 - val_mse: 2.5669e-04\n",
            "Epoch 522/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.1981e-04 - mse: 6.1981e-04\n",
            "Epoch 522: val_loss improved from 0.00024 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.8632e-04 - mse: 5.8632e-04 - val_loss: 2.3960e-04 - val_mse: 2.3960e-04\n",
            "Epoch 523/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.7318e-04 - mse: 5.7318e-04\n",
            "Epoch 523: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.7245e-04 - mse: 5.7245e-04 - val_loss: 2.4259e-04 - val_mse: 2.4259e-04\n",
            "Epoch 524/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.4786e-04 - mse: 5.4786e-04\n",
            "Epoch 524: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 5.4231e-04 - mse: 5.4231e-04 - val_loss: 2.5548e-04 - val_mse: 2.5548e-04\n",
            "Epoch 525/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.8911e-04 - mse: 5.8911e-04\n",
            "Epoch 525: val_loss improved from 0.00024 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 5.8394e-04 - mse: 5.8394e-04 - val_loss: 2.3809e-04 - val_mse: 2.3809e-04\n",
            "Epoch 526/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.9163e-04 - mse: 5.9163e-04\n",
            "Epoch 526: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6946e-04 - mse: 5.6946e-04 - val_loss: 2.4132e-04 - val_mse: 2.4132e-04\n",
            "Epoch 527/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.6517e-04 - mse: 5.6517e-04\n",
            "Epoch 527: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4053e-04 - mse: 5.4053e-04 - val_loss: 2.5448e-04 - val_mse: 2.5448e-04\n",
            "Epoch 528/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 6.0508e-04 - mse: 6.0508e-04\n",
            "Epoch 528: val_loss improved from 0.00024 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.8177e-04 - mse: 5.8177e-04 - val_loss: 2.3684e-04 - val_mse: 2.3684e-04\n",
            "Epoch 529/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.7335e-04 - mse: 5.7335e-04\n",
            "Epoch 529: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.6729e-04 - mse: 5.6729e-04 - val_loss: 2.4020e-04 - val_mse: 2.4020e-04\n",
            "Epoch 530/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.4084e-04 - mse: 5.4084e-04\n",
            "Epoch 530: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.3888e-04 - mse: 5.3888e-04 - val_loss: 2.5361e-04 - val_mse: 2.5361e-04\n",
            "Epoch 531/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.8419e-04 - mse: 5.8419e-04\n",
            "Epoch 531: val_loss improved from 0.00024 to 0.00024, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.7938e-04 - mse: 5.7938e-04 - val_loss: 2.3596e-04 - val_mse: 2.3596e-04\n",
            "Epoch 532/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.6498e-04 - mse: 5.6498e-04\n",
            "Epoch 532: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6434e-04 - mse: 5.6434e-04 - val_loss: 2.4042e-04 - val_mse: 2.4042e-04\n",
            "Epoch 533/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.3858e-04 - mse: 5.3858e-04\n",
            "Epoch 533: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.2919e-04 - mse: 5.2919e-04 - val_loss: 2.5438e-04 - val_mse: 2.5438e-04\n",
            "Epoch 534/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.1063e-04 - mse: 6.1063e-04\n",
            "Epoch 534: val_loss improved from 0.00024 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.7814e-04 - mse: 5.7814e-04 - val_loss: 2.3499e-04 - val_mse: 2.3499e-04\n",
            "Epoch 535/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.7340e-04 - mse: 5.7340e-04\n",
            "Epoch 535: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.6055e-04 - mse: 5.6055e-04 - val_loss: 2.3940e-04 - val_mse: 2.3940e-04\n",
            "Epoch 536/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.3617e-04 - mse: 5.3617e-04\n",
            "Epoch 536: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2677e-04 - mse: 5.2677e-04 - val_loss: 2.5334e-04 - val_mse: 2.5334e-04\n",
            "Epoch 537/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.9024e-04 - mse: 5.9024e-04\n",
            "Epoch 537: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.7647e-04 - mse: 5.7647e-04 - val_loss: 2.3293e-04 - val_mse: 2.3293e-04\n",
            "Epoch 538/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.6181e-04 - mse: 5.6181e-04\n",
            "Epoch 538: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5760e-04 - mse: 5.5760e-04 - val_loss: 2.3773e-04 - val_mse: 2.3773e-04\n",
            "Epoch 539/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.2936e-04 - mse: 5.2936e-04\n",
            "Epoch 539: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2444e-04 - mse: 5.2444e-04 - val_loss: 2.5280e-04 - val_mse: 2.5280e-04\n",
            "Epoch 540/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.7844e-04 - mse: 5.7844e-04\n",
            "Epoch 540: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.7410e-04 - mse: 5.7410e-04 - val_loss: 2.3222e-04 - val_mse: 2.3222e-04\n",
            "Epoch 541/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.7598e-04 - mse: 5.7598e-04\n",
            "Epoch 541: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5521e-04 - mse: 5.5521e-04 - val_loss: 2.3709e-04 - val_mse: 2.3709e-04\n",
            "Epoch 542/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.3142e-04 - mse: 5.3142e-04\n",
            "Epoch 542: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.2229e-04 - mse: 5.2229e-04 - val_loss: 2.5183e-04 - val_mse: 2.5183e-04\n",
            "Epoch 543/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 6.0244e-04 - mse: 6.0244e-04\n",
            "Epoch 543: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.7100e-04 - mse: 5.7100e-04 - val_loss: 2.3054e-04 - val_mse: 2.3054e-04\n",
            "Epoch 544/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.7584e-04 - mse: 5.7584e-04\n",
            "Epoch 544: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5524e-04 - mse: 5.5524e-04 - val_loss: 2.3519e-04 - val_mse: 2.3519e-04\n",
            "Epoch 545/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.2736e-04 - mse: 5.2736e-04\n",
            "Epoch 545: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.1855e-04 - mse: 5.1855e-04 - val_loss: 2.5189e-04 - val_mse: 2.5189e-04\n",
            "Epoch 546/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.8297e-04 - mse: 5.8297e-04\n",
            "Epoch 546: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.6991e-04 - mse: 5.6991e-04 - val_loss: 2.2965e-04 - val_mse: 2.2965e-04\n",
            "Epoch 547/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.7336e-04 - mse: 5.7336e-04\n",
            "Epoch 547: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.5298e-04 - mse: 5.5298e-04 - val_loss: 2.3446e-04 - val_mse: 2.3446e-04\n",
            "Epoch 548/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.3482e-04 - mse: 5.3482e-04\n",
            "Epoch 548: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1765e-04 - mse: 5.1765e-04 - val_loss: 2.5108e-04 - val_mse: 2.5108e-04\n",
            "Epoch 549/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.7349e-04 - mse: 5.7349e-04\n",
            "Epoch 549: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.6772e-04 - mse: 5.6772e-04 - val_loss: 2.2872e-04 - val_mse: 2.2872e-04\n",
            "Epoch 550/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.5165e-04 - mse: 5.5165e-04\n",
            "Epoch 550: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5119e-04 - mse: 5.5119e-04 - val_loss: 2.3364e-04 - val_mse: 2.3364e-04\n",
            "Epoch 551/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.3952e-04 - mse: 5.3952e-04\n",
            "Epoch 551: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1623e-04 - mse: 5.1623e-04 - val_loss: 2.5059e-04 - val_mse: 2.5059e-04\n",
            "Epoch 552/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.7041e-04 - mse: 5.7041e-04\n",
            "Epoch 552: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6623e-04 - mse: 5.6623e-04 - val_loss: 2.2801e-04 - val_mse: 2.2801e-04\n",
            "Epoch 553/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.6154e-04 - mse: 5.6154e-04\n",
            "Epoch 553: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4986e-04 - mse: 5.4986e-04 - val_loss: 2.3294e-04 - val_mse: 2.3294e-04\n",
            "Epoch 554/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.3194e-04 - mse: 5.3194e-04\n",
            "Epoch 554: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1493e-04 - mse: 5.1493e-04 - val_loss: 2.5012e-04 - val_mse: 2.5012e-04\n",
            "Epoch 555/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.8566e-04 - mse: 5.8566e-04\n",
            "Epoch 555: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6438e-04 - mse: 5.6438e-04 - val_loss: 2.2736e-04 - val_mse: 2.2736e-04\n",
            "Epoch 556/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.6801e-04 - mse: 5.6801e-04\n",
            "Epoch 556: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4812e-04 - mse: 5.4812e-04 - val_loss: 2.3243e-04 - val_mse: 2.3243e-04\n",
            "Epoch 557/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.1843e-04 - mse: 5.1843e-04\n",
            "Epoch 557: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1373e-04 - mse: 5.1373e-04 - val_loss: 2.4975e-04 - val_mse: 2.4975e-04\n",
            "Epoch 558/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.9336e-04 - mse: 5.9336e-04\n",
            "Epoch 558: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.6293e-04 - mse: 5.6293e-04 - val_loss: 2.2690e-04 - val_mse: 2.2690e-04\n",
            "Epoch 559/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.7576e-04 - mse: 5.7576e-04\n",
            "Epoch 559: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.4705e-04 - mse: 5.4705e-04 - val_loss: 2.3198e-04 - val_mse: 2.3198e-04\n",
            "Epoch 560/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.1282e-04 - mse: 5.1282e-04\n",
            "Epoch 560: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1247e-04 - mse: 5.1247e-04 - val_loss: 2.4945e-04 - val_mse: 2.4945e-04\n",
            "Epoch 561/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.9087e-04 - mse: 5.9087e-04\n",
            "Epoch 561: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6069e-04 - mse: 5.6069e-04 - val_loss: 2.2632e-04 - val_mse: 2.2632e-04\n",
            "Epoch 562/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.6357e-04 - mse: 5.6357e-04\n",
            "Epoch 562: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4405e-04 - mse: 5.4405e-04 - val_loss: 2.3176e-04 - val_mse: 2.3176e-04\n",
            "Epoch 563/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.1246e-04 - mse: 5.1246e-04\n",
            "Epoch 563: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1210e-04 - mse: 5.1210e-04 - val_loss: 2.4910e-04 - val_mse: 2.4910e-04\n",
            "Epoch 564/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.6061e-04 - mse: 5.6061e-04\n",
            "Epoch 564: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.6061e-04 - mse: 5.6061e-04 - val_loss: 2.2583e-04 - val_mse: 2.2583e-04\n",
            "Epoch 565/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.6391e-04 - mse: 5.6391e-04\n",
            "Epoch 565: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.4432e-04 - mse: 5.4432e-04 - val_loss: 2.3112e-04 - val_mse: 2.3112e-04\n",
            "Epoch 566/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.1033e-04 - mse: 5.1033e-04\n",
            "Epoch 566: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.1001e-04 - mse: 5.1001e-04 - val_loss: 2.4885e-04 - val_mse: 2.4885e-04\n",
            "Epoch 567/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.6147e-04 - mse: 5.6147e-04\n",
            "Epoch 567: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5826e-04 - mse: 5.5826e-04 - val_loss: 2.2545e-04 - val_mse: 2.2545e-04\n",
            "Epoch 568/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.4163e-04 - mse: 5.4163e-04\n",
            "Epoch 568: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4163e-04 - mse: 5.4163e-04 - val_loss: 2.3096e-04 - val_mse: 2.3096e-04\n",
            "Epoch 569/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.0861e-04 - mse: 5.0861e-04\n",
            "Epoch 569: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0829e-04 - mse: 5.0829e-04 - val_loss: 2.4888e-04 - val_mse: 2.4888e-04\n",
            "Epoch 570/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.8221e-04 - mse: 5.8221e-04\n",
            "Epoch 570: val_loss improved from 0.00023 to 0.00023, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5776e-04 - mse: 5.5776e-04 - val_loss: 2.2503e-04 - val_mse: 2.2503e-04\n",
            "Epoch 571/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.5294e-04 - mse: 5.5294e-04\n",
            "Epoch 571: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4170e-04 - mse: 5.4170e-04 - val_loss: 2.3049e-04 - val_mse: 2.3049e-04\n",
            "Epoch 572/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.3288e-04 - mse: 5.3288e-04\n",
            "Epoch 572: val_loss did not improve from 0.00023\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.0738e-04 - mse: 5.0738e-04 - val_loss: 2.4840e-04 - val_mse: 2.4840e-04\n",
            "Epoch 573/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5587e-04 - mse: 5.5587e-04\n",
            "Epoch 573: val_loss improved from 0.00023 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5587e-04 - mse: 5.5587e-04 - val_loss: 2.2483e-04 - val_mse: 2.2483e-04\n",
            "Epoch 574/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.4008e-04 - mse: 5.4008e-04\n",
            "Epoch 574: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.3966e-04 - mse: 5.3966e-04 - val_loss: 2.3049e-04 - val_mse: 2.3049e-04\n",
            "Epoch 575/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0535e-04 - mse: 5.0535e-04\n",
            "Epoch 575: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0535e-04 - mse: 5.0535e-04 - val_loss: 2.4844e-04 - val_mse: 2.4844e-04\n",
            "Epoch 576/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.7525e-04 - mse: 5.7525e-04\n",
            "Epoch 576: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5472e-04 - mse: 5.5472e-04 - val_loss: 2.2437e-04 - val_mse: 2.2437e-04\n",
            "Epoch 577/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.6609e-04 - mse: 5.6609e-04\n",
            "Epoch 577: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3820e-04 - mse: 5.3820e-04 - val_loss: 2.3009e-04 - val_mse: 2.3009e-04\n",
            "Epoch 578/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.1166e-04 - mse: 5.1166e-04\n",
            "Epoch 578: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.0332e-04 - mse: 5.0332e-04 - val_loss: 2.4832e-04 - val_mse: 2.4832e-04\n",
            "Epoch 579/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.8312e-04 - mse: 5.8312e-04\n",
            "Epoch 579: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.5360e-04 - mse: 5.5360e-04 - val_loss: 2.2398e-04 - val_mse: 2.2398e-04\n",
            "Epoch 580/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.6473e-04 - mse: 5.6473e-04\n",
            "Epoch 580: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3695e-04 - mse: 5.3695e-04 - val_loss: 2.2956e-04 - val_mse: 2.2956e-04\n",
            "Epoch 581/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.1379e-04 - mse: 5.1379e-04\n",
            "Epoch 581: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.0491e-04 - mse: 5.0491e-04 - val_loss: 2.4737e-04 - val_mse: 2.4737e-04\n",
            "Epoch 582/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.5459e-04 - mse: 5.5459e-04\n",
            "Epoch 582: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5149e-04 - mse: 5.5149e-04 - val_loss: 2.2419e-04 - val_mse: 2.2419e-04\n",
            "Epoch 583/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.6354e-04 - mse: 5.6354e-04\n",
            "Epoch 583: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.3577e-04 - mse: 5.3577e-04 - val_loss: 2.3017e-04 - val_mse: 2.3017e-04\n",
            "Epoch 584/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0304e-04 - mse: 5.0304e-04\n",
            "Epoch 584: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0304e-04 - mse: 5.0304e-04 - val_loss: 2.4769e-04 - val_mse: 2.4769e-04\n",
            "Epoch 585/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.5099e-04 - mse: 5.5099e-04\n",
            "Epoch 585: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.5044e-04 - mse: 5.5044e-04 - val_loss: 2.2445e-04 - val_mse: 2.2445e-04\n",
            "Epoch 586/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.4098e-04 - mse: 5.4098e-04\n",
            "Epoch 586: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2986e-04 - mse: 5.2986e-04 - val_loss: 2.3360e-04 - val_mse: 2.3360e-04\n",
            "Epoch 587/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.4407e-04 - mse: 5.4407e-04\n",
            "Epoch 587: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1718e-04 - mse: 5.1718e-04 - val_loss: 2.4212e-04 - val_mse: 2.4212e-04\n",
            "Epoch 588/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.5323e-04 - mse: 5.5323e-04\n",
            "Epoch 588: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4803e-04 - mse: 5.4803e-04 - val_loss: 2.2423e-04 - val_mse: 2.2423e-04\n",
            "Epoch 589/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.3439e-04 - mse: 5.3439e-04\n",
            "Epoch 589: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.3439e-04 - mse: 5.3439e-04 - val_loss: 2.3062e-04 - val_mse: 2.3062e-04\n",
            "Epoch 590/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.0398e-04 - mse: 5.0398e-04\n",
            "Epoch 590: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0149e-04 - mse: 5.0149e-04 - val_loss: 2.4835e-04 - val_mse: 2.4835e-04\n",
            "Epoch 591/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.5437e-04 - mse: 5.5437e-04\n",
            "Epoch 591: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4904e-04 - mse: 5.4904e-04 - val_loss: 2.2549e-04 - val_mse: 2.2549e-04\n",
            "Epoch 592/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.3322e-04 - mse: 5.3322e-04\n",
            "Epoch 592: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1519e-04 - mse: 5.1519e-04 - val_loss: 2.4074e-04 - val_mse: 2.4074e-04\n",
            "Epoch 593/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.7329e-04 - mse: 5.7329e-04\n",
            "Epoch 593: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4455e-04 - mse: 5.4455e-04 - val_loss: 2.2652e-04 - val_mse: 2.2652e-04\n",
            "Epoch 594/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.3102e-04 - mse: 5.3102e-04\n",
            "Epoch 594: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0801e-04 - mse: 5.0801e-04 - val_loss: 2.4417e-04 - val_mse: 2.4417e-04\n",
            "Epoch 595/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.5754e-04 - mse: 5.5754e-04\n",
            "Epoch 595: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4579e-04 - mse: 5.4579e-04 - val_loss: 2.2378e-04 - val_mse: 2.2378e-04\n",
            "Epoch 596/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.4384e-04 - mse: 5.4384e-04\n",
            "Epoch 596: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.3259e-04 - mse: 5.3259e-04 - val_loss: 2.3008e-04 - val_mse: 2.3008e-04\n",
            "Epoch 597/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.1685e-04 - mse: 5.1685e-04\n",
            "Epoch 597: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0002e-04 - mse: 5.0002e-04 - val_loss: 2.4799e-04 - val_mse: 2.4799e-04\n",
            "Epoch 598/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.7008e-04 - mse: 5.7008e-04\n",
            "Epoch 598: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.4587e-04 - mse: 5.4587e-04 - val_loss: 2.2593e-04 - val_mse: 2.2593e-04\n",
            "Epoch 599/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.0585e-04 - mse: 5.0585e-04\n",
            "Epoch 599: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0532e-04 - mse: 5.0532e-04 - val_loss: 2.4344e-04 - val_mse: 2.4344e-04\n",
            "Epoch 600/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.4254e-04 - mse: 5.4254e-04\n",
            "Epoch 600: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4254e-04 - mse: 5.4254e-04 - val_loss: 2.2326e-04 - val_mse: 2.2326e-04\n",
            "Epoch 601/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.3505e-04 - mse: 5.3505e-04\n",
            "Epoch 601: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3000e-04 - mse: 5.3000e-04 - val_loss: 2.2959e-04 - val_mse: 2.2959e-04\n",
            "Epoch 602/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.0654e-04 - mse: 5.0654e-04\n",
            "Epoch 602: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.9755e-04 - mse: 4.9755e-04 - val_loss: 2.4754e-04 - val_mse: 2.4754e-04\n",
            "Epoch 603/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.5521e-04 - mse: 5.5521e-04\n",
            "Epoch 603: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4317e-04 - mse: 5.4317e-04 - val_loss: 2.2542e-04 - val_mse: 2.2542e-04\n",
            "Epoch 604/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.0619e-04 - mse: 5.0619e-04\n",
            "Epoch 604: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.0311e-04 - mse: 5.0311e-04 - val_loss: 2.4285e-04 - val_mse: 2.4285e-04\n",
            "Epoch 605/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.4379e-04 - mse: 5.4379e-04\n",
            "Epoch 605: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 5.4001e-04 - mse: 5.4001e-04 - val_loss: 2.2259e-04 - val_mse: 2.2259e-04\n",
            "Epoch 606/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.3126e-04 - mse: 5.3126e-04\n",
            "Epoch 606: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 5.2751e-04 - mse: 5.2751e-04 - val_loss: 2.2902e-04 - val_mse: 2.2902e-04\n",
            "Epoch 607/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.9464e-04 - mse: 4.9464e-04\n",
            "Epoch 607: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 4.9366e-04 - mse: 4.9366e-04 - val_loss: 2.4770e-04 - val_mse: 2.4770e-04\n",
            "Epoch 608/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.4035e-04 - mse: 5.4035e-04\n",
            "Epoch 608: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.4035e-04 - mse: 5.4035e-04 - val_loss: 2.2475e-04 - val_mse: 2.2475e-04\n",
            "Epoch 609/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0117e-04 - mse: 5.0117e-04\n",
            "Epoch 609: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0117e-04 - mse: 5.0117e-04 - val_loss: 2.4212e-04 - val_mse: 2.4212e-04\n",
            "Epoch 610/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.6508e-04 - mse: 5.6508e-04\n",
            "Epoch 610: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3697e-04 - mse: 5.3697e-04 - val_loss: 2.2350e-04 - val_mse: 2.2350e-04\n",
            "Epoch 611/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0779e-04 - mse: 5.0779e-04\n",
            "Epoch 611: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0779e-04 - mse: 5.0779e-04 - val_loss: 2.3919e-04 - val_mse: 2.3919e-04\n",
            "Epoch 612/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.3740e-04 - mse: 5.3740e-04\n",
            "Epoch 612: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.3421e-04 - mse: 5.3421e-04 - val_loss: 2.2662e-04 - val_mse: 2.2662e-04\n",
            "Epoch 613/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9316e-04 - mse: 4.9316e-04\n",
            "Epoch 613: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.9316e-04 - mse: 4.9316e-04 - val_loss: 2.4600e-04 - val_mse: 2.4600e-04\n",
            "Epoch 614/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.4164e-04 - mse: 5.4164e-04\n",
            "Epoch 614: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.3645e-04 - mse: 5.3645e-04 - val_loss: 2.2517e-04 - val_mse: 2.2517e-04\n",
            "Epoch 615/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9316e-04 - mse: 4.9316e-04\n",
            "Epoch 615: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.9316e-04 - mse: 4.9316e-04 - val_loss: 2.4444e-04 - val_mse: 2.4444e-04\n",
            "Epoch 616/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.3755e-04 - mse: 5.3755e-04\n",
            "Epoch 616: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3443e-04 - mse: 5.3443e-04 - val_loss: 2.2432e-04 - val_mse: 2.2432e-04\n",
            "Epoch 617/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.9287e-04 - mse: 4.9287e-04\n",
            "Epoch 617: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.9233e-04 - mse: 4.9233e-04 - val_loss: 2.4374e-04 - val_mse: 2.4374e-04\n",
            "Epoch 618/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.3582e-04 - mse: 5.3582e-04\n",
            "Epoch 618: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.3280e-04 - mse: 5.3280e-04 - val_loss: 2.2378e-04 - val_mse: 2.2378e-04\n",
            "Epoch 619/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.9168e-04 - mse: 4.9168e-04\n",
            "Epoch 619: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.9114e-04 - mse: 4.9114e-04 - val_loss: 2.4336e-04 - val_mse: 2.4336e-04\n",
            "Epoch 620/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.5048e-04 - mse: 5.5048e-04\n",
            "Epoch 620: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3132e-04 - mse: 5.3132e-04 - val_loss: 2.2341e-04 - val_mse: 2.2341e-04\n",
            "Epoch 621/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.1460e-04 - mse: 5.1460e-04\n",
            "Epoch 621: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.9002e-04 - mse: 4.9002e-04 - val_loss: 2.4315e-04 - val_mse: 2.4315e-04\n",
            "Epoch 622/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.3394e-04 - mse: 5.3394e-04\n",
            "Epoch 622: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.3021e-04 - mse: 5.3021e-04 - val_loss: 2.2304e-04 - val_mse: 2.2304e-04\n",
            "Epoch 623/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.9187e-04 - mse: 4.9187e-04\n",
            "Epoch 623: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.8899e-04 - mse: 4.8899e-04 - val_loss: 2.4297e-04 - val_mse: 2.4297e-04\n",
            "Epoch 624/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.4047e-04 - mse: 5.4047e-04\n",
            "Epoch 624: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.2932e-04 - mse: 5.2932e-04 - val_loss: 2.2263e-04 - val_mse: 2.2263e-04\n",
            "Epoch 625/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.0344e-04 - mse: 5.0344e-04\n",
            "Epoch 625: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8720e-04 - mse: 4.8720e-04 - val_loss: 2.4284e-04 - val_mse: 2.4284e-04\n",
            "Epoch 626/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.3145e-04 - mse: 5.3145e-04\n",
            "Epoch 626: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.2780e-04 - mse: 5.2780e-04 - val_loss: 2.2239e-04 - val_mse: 2.2239e-04\n",
            "Epoch 627/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.1019e-04 - mse: 5.1019e-04\n",
            "Epoch 627: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8600e-04 - mse: 4.8600e-04 - val_loss: 2.4273e-04 - val_mse: 2.4273e-04\n",
            "Epoch 628/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.4551e-04 - mse: 5.4551e-04\n",
            "Epoch 628: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.2675e-04 - mse: 5.2675e-04 - val_loss: 2.2210e-04 - val_mse: 2.2210e-04\n",
            "Epoch 629/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.8779e-04 - mse: 4.8779e-04\n",
            "Epoch 629: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.8497e-04 - mse: 4.8497e-04 - val_loss: 2.4255e-04 - val_mse: 2.4255e-04\n",
            "Epoch 630/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.5283e-04 - mse: 5.5283e-04\n",
            "Epoch 630: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2585e-04 - mse: 5.2585e-04 - val_loss: 2.2176e-04 - val_mse: 2.2176e-04\n",
            "Epoch 631/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.8504e-04 - mse: 4.8504e-04\n",
            "Epoch 631: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8392e-04 - mse: 4.8392e-04 - val_loss: 2.4237e-04 - val_mse: 2.4237e-04\n",
            "Epoch 632/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.4349e-04 - mse: 5.4349e-04\n",
            "Epoch 632: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.2490e-04 - mse: 5.2490e-04 - val_loss: 2.2145e-04 - val_mse: 2.2145e-04\n",
            "Epoch 633/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.9883e-04 - mse: 4.9883e-04\n",
            "Epoch 633: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.8289e-04 - mse: 4.8289e-04 - val_loss: 2.4239e-04 - val_mse: 2.4239e-04\n",
            "Epoch 634/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.2809e-04 - mse: 5.2809e-04\n",
            "Epoch 634: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2543e-04 - mse: 5.2543e-04 - val_loss: 2.2081e-04 - val_mse: 2.2081e-04\n",
            "Epoch 635/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.7932e-04 - mse: 4.7932e-04\n",
            "Epoch 635: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7882e-04 - mse: 4.7882e-04 - val_loss: 2.4252e-04 - val_mse: 2.4252e-04\n",
            "Epoch 636/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.2301e-04 - mse: 5.2301e-04\n",
            "Epoch 636: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2065e-04 - mse: 5.2065e-04 - val_loss: 2.2135e-04 - val_mse: 2.2135e-04\n",
            "Epoch 637/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8145e-04 - mse: 4.8145e-04\n",
            "Epoch 637: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8145e-04 - mse: 4.8145e-04 - val_loss: 2.4227e-04 - val_mse: 2.4227e-04\n",
            "Epoch 638/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.4243e-04 - mse: 5.4243e-04\n",
            "Epoch 638: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.2392e-04 - mse: 5.2392e-04 - val_loss: 2.2045e-04 - val_mse: 2.2045e-04\n",
            "Epoch 639/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.7998e-04 - mse: 4.7998e-04\n",
            "Epoch 639: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7732e-04 - mse: 4.7732e-04 - val_loss: 2.4234e-04 - val_mse: 2.4234e-04\n",
            "Epoch 640/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.2174e-04 - mse: 5.2174e-04\n",
            "Epoch 640: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1717e-04 - mse: 5.1717e-04 - val_loss: 2.2129e-04 - val_mse: 2.2129e-04\n",
            "Epoch 641/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7994e-04 - mse: 4.7994e-04\n",
            "Epoch 641: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7994e-04 - mse: 4.7994e-04 - val_loss: 2.4226e-04 - val_mse: 2.4226e-04\n",
            "Epoch 642/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.2298e-04 - mse: 5.2298e-04\n",
            "Epoch 642: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.2245e-04 - mse: 5.2245e-04 - val_loss: 2.2020e-04 - val_mse: 2.2020e-04\n",
            "Epoch 643/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 4.9683e-04 - mse: 4.9683e-04\n",
            "Epoch 643: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7577e-04 - mse: 4.7577e-04 - val_loss: 2.4240e-04 - val_mse: 2.4240e-04\n",
            "Epoch 644/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.2026e-04 - mse: 5.2026e-04\n",
            "Epoch 644: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.1573e-04 - mse: 5.1573e-04 - val_loss: 2.2108e-04 - val_mse: 2.2108e-04\n",
            "Epoch 645/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.9392e-04 - mse: 4.9392e-04\n",
            "Epoch 645: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7828e-04 - mse: 4.7828e-04 - val_loss: 2.4236e-04 - val_mse: 2.4236e-04\n",
            "Epoch 646/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.3192e-04 - mse: 5.3192e-04\n",
            "Epoch 646: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.2127e-04 - mse: 5.2127e-04 - val_loss: 2.1983e-04 - val_mse: 2.1983e-04\n",
            "Epoch 647/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.8168e-04 - mse: 4.8168e-04\n",
            "Epoch 647: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7350e-04 - mse: 4.7350e-04 - val_loss: 2.4249e-04 - val_mse: 2.4249e-04\n",
            "Epoch 648/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.3186e-04 - mse: 5.3186e-04\n",
            "Epoch 648: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.1423e-04 - mse: 5.1423e-04 - val_loss: 2.2074e-04 - val_mse: 2.2074e-04\n",
            "Epoch 649/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.7670e-04 - mse: 4.7670e-04\n",
            "Epoch 649: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7406e-04 - mse: 4.7406e-04 - val_loss: 2.4252e-04 - val_mse: 2.4252e-04\n",
            "Epoch 650/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.3965e-04 - mse: 5.3965e-04\n",
            "Epoch 650: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1403e-04 - mse: 5.1403e-04 - val_loss: 2.2071e-04 - val_mse: 2.2071e-04\n",
            "Epoch 651/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.8904e-04 - mse: 4.8904e-04\n",
            "Epoch 651: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7375e-04 - mse: 4.7375e-04 - val_loss: 2.4262e-04 - val_mse: 2.4262e-04\n",
            "Epoch 652/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1328e-04 - mse: 5.1328e-04\n",
            "Epoch 652: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1328e-04 - mse: 5.1328e-04 - val_loss: 2.2075e-04 - val_mse: 2.2075e-04\n",
            "Epoch 653/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.7742e-04 - mse: 4.7742e-04\n",
            "Epoch 653: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7314e-04 - mse: 4.7314e-04 - val_loss: 2.4284e-04 - val_mse: 2.4284e-04\n",
            "Epoch 654/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.3848e-04 - mse: 5.3848e-04\n",
            "Epoch 654: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.1298e-04 - mse: 5.1298e-04 - val_loss: 2.2071e-04 - val_mse: 2.2071e-04\n",
            "Epoch 655/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.7277e-04 - mse: 4.7277e-04\n",
            "Epoch 655: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7224e-04 - mse: 4.7224e-04 - val_loss: 2.4307e-04 - val_mse: 2.4307e-04\n",
            "Epoch 656/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.2995e-04 - mse: 5.2995e-04\n",
            "Epoch 656: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1246e-04 - mse: 5.1246e-04 - val_loss: 2.2073e-04 - val_mse: 2.2073e-04\n",
            "Epoch 657/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.7577e-04 - mse: 4.7577e-04\n",
            "Epoch 657: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7151e-04 - mse: 4.7151e-04 - val_loss: 2.4323e-04 - val_mse: 2.4323e-04\n",
            "Epoch 658/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.1408e-04 - mse: 5.1408e-04\n",
            "Epoch 658: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1201e-04 - mse: 5.1201e-04 - val_loss: 2.2064e-04 - val_mse: 2.2064e-04\n",
            "Epoch 659/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.7091e-04 - mse: 4.7091e-04\n",
            "Epoch 659: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7038e-04 - mse: 4.7038e-04 - val_loss: 2.4338e-04 - val_mse: 2.4338e-04\n",
            "Epoch 660/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.3658e-04 - mse: 5.3658e-04\n",
            "Epoch 660: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.1125e-04 - mse: 5.1125e-04 - val_loss: 2.2059e-04 - val_mse: 2.2059e-04\n",
            "Epoch 661/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.7034e-04 - mse: 4.7034e-04\n",
            "Epoch 661: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6981e-04 - mse: 4.6981e-04 - val_loss: 2.4335e-04 - val_mse: 2.4335e-04\n",
            "Epoch 662/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1065e-04 - mse: 5.1065e-04\n",
            "Epoch 662: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.1065e-04 - mse: 5.1065e-04 - val_loss: 2.2052e-04 - val_mse: 2.2052e-04\n",
            "Epoch 663/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.9172e-04 - mse: 4.9172e-04\n",
            "Epoch 663: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6911e-04 - mse: 4.6911e-04 - val_loss: 2.4342e-04 - val_mse: 2.4342e-04\n",
            "Epoch 664/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.3534e-04 - mse: 5.3534e-04\n",
            "Epoch 664: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 5.1010e-04 - mse: 5.1010e-04 - val_loss: 2.2048e-04 - val_mse: 2.2048e-04\n",
            "Epoch 665/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.7095e-04 - mse: 4.7095e-04\n",
            "Epoch 665: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6830e-04 - mse: 4.6830e-04 - val_loss: 2.4349e-04 - val_mse: 2.4349e-04\n",
            "Epoch 666/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0951e-04 - mse: 5.0951e-04\n",
            "Epoch 666: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0951e-04 - mse: 5.0951e-04 - val_loss: 2.2037e-04 - val_mse: 2.2037e-04\n",
            "Epoch 667/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.7155e-04 - mse: 4.7155e-04\n",
            "Epoch 667: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6735e-04 - mse: 4.6735e-04 - val_loss: 2.4356e-04 - val_mse: 2.4356e-04\n",
            "Epoch 668/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.2600e-04 - mse: 5.2600e-04\n",
            "Epoch 668: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0869e-04 - mse: 5.0869e-04 - val_loss: 2.2020e-04 - val_mse: 2.2020e-04\n",
            "Epoch 669/1000\n",
            "52/61 [========================>.....] - ETA: 0s - loss: 4.8775e-04 - mse: 4.8775e-04\n",
            "Epoch 669: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6561e-04 - mse: 4.6561e-04 - val_loss: 2.4366e-04 - val_mse: 2.4366e-04\n",
            "Epoch 670/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.0796e-04 - mse: 5.0796e-04\n",
            "Epoch 670: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0747e-04 - mse: 5.0747e-04 - val_loss: 2.2030e-04 - val_mse: 2.2030e-04\n",
            "Epoch 671/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.8780e-04 - mse: 4.8780e-04\n",
            "Epoch 671: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6548e-04 - mse: 4.6548e-04 - val_loss: 2.4353e-04 - val_mse: 2.4353e-04\n",
            "Epoch 672/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.1092e-04 - mse: 5.1092e-04\n",
            "Epoch 672: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0657e-04 - mse: 5.0657e-04 - val_loss: 2.2000e-04 - val_mse: 2.2000e-04\n",
            "Epoch 673/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.7819e-04 - mse: 4.7819e-04\n",
            "Epoch 673: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6351e-04 - mse: 4.6351e-04 - val_loss: 2.4371e-04 - val_mse: 2.4371e-04\n",
            "Epoch 674/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.2300e-04 - mse: 5.2300e-04\n",
            "Epoch 674: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0588e-04 - mse: 5.0588e-04 - val_loss: 2.1991e-04 - val_mse: 2.1991e-04\n",
            "Epoch 675/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.7755e-04 - mse: 4.7755e-04\n",
            "Epoch 675: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.6290e-04 - mse: 4.6290e-04 - val_loss: 2.4359e-04 - val_mse: 2.4359e-04\n",
            "Epoch 676/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.0898e-04 - mse: 5.0898e-04\n",
            "Epoch 676: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0466e-04 - mse: 5.0466e-04 - val_loss: 2.1977e-04 - val_mse: 2.1977e-04\n",
            "Epoch 677/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.6962e-04 - mse: 4.6962e-04\n",
            "Epoch 677: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6182e-04 - mse: 4.6182e-04 - val_loss: 2.4356e-04 - val_mse: 2.4356e-04\n",
            "Epoch 678/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.1351e-04 - mse: 5.1351e-04\n",
            "Epoch 678: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 5.0377e-04 - mse: 5.0377e-04 - val_loss: 2.1960e-04 - val_mse: 2.1960e-04\n",
            "Epoch 679/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.6154e-04 - mse: 4.6154e-04\n",
            "Epoch 679: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6086e-04 - mse: 4.6086e-04 - val_loss: 2.4351e-04 - val_mse: 2.4351e-04\n",
            "Epoch 680/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 5.0349e-04 - mse: 5.0349e-04\n",
            "Epoch 680: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0299e-04 - mse: 5.0299e-04 - val_loss: 2.1941e-04 - val_mse: 2.1941e-04\n",
            "Epoch 681/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.6062e-04 - mse: 4.6062e-04\n",
            "Epoch 681: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.5995e-04 - mse: 4.5995e-04 - val_loss: 2.4343e-04 - val_mse: 2.4343e-04\n",
            "Epoch 682/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0194e-04 - mse: 5.0194e-04\n",
            "Epoch 682: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0194e-04 - mse: 5.0194e-04 - val_loss: 2.1955e-04 - val_mse: 2.1955e-04\n",
            "Epoch 683/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.7909e-04 - mse: 4.7909e-04\n",
            "Epoch 683: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5747e-04 - mse: 4.5747e-04 - val_loss: 2.4419e-04 - val_mse: 2.4419e-04\n",
            "Epoch 684/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 5.0523e-04 - mse: 5.0523e-04\n",
            "Epoch 684: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 5.0095e-04 - mse: 5.0095e-04 - val_loss: 2.1938e-04 - val_mse: 2.1938e-04\n",
            "Epoch 685/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.6312e-04 - mse: 4.6312e-04\n",
            "Epoch 685: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5565e-04 - mse: 4.5565e-04 - val_loss: 2.4428e-04 - val_mse: 2.4428e-04\n",
            "Epoch 686/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.0945e-04 - mse: 5.0945e-04\n",
            "Epoch 686: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.9988e-04 - mse: 4.9988e-04 - val_loss: 2.1904e-04 - val_mse: 2.1904e-04\n",
            "Epoch 687/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.5445e-04 - mse: 4.5445e-04\n",
            "Epoch 687: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5405e-04 - mse: 4.5405e-04 - val_loss: 2.4439e-04 - val_mse: 2.4439e-04\n",
            "Epoch 688/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.2058e-04 - mse: 5.2058e-04\n",
            "Epoch 688: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0349e-04 - mse: 5.0349e-04 - val_loss: 2.1751e-04 - val_mse: 2.1751e-04\n",
            "Epoch 689/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.5906e-04 - mse: 4.5906e-04\n",
            "Epoch 689: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 4.5182e-04 - mse: 4.5182e-04 - val_loss: 2.4364e-04 - val_mse: 2.4364e-04\n",
            "Epoch 690/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 5.0472e-04 - mse: 5.0472e-04\n",
            "Epoch 690: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 5.0154e-04 - mse: 5.0154e-04 - val_loss: 2.1710e-04 - val_mse: 2.1710e-04\n",
            "Epoch 691/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.5673e-04 - mse: 4.5673e-04\n",
            "Epoch 691: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 4.4961e-04 - mse: 4.4961e-04 - val_loss: 2.4364e-04 - val_mse: 2.4364e-04\n",
            "Epoch 692/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 5.0211e-04 - mse: 5.0211e-04\n",
            "Epoch 692: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 5.0016e-04 - mse: 5.0016e-04 - val_loss: 2.1646e-04 - val_mse: 2.1646e-04\n",
            "Epoch 693/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.5582e-04 - mse: 4.5582e-04\n",
            "Epoch 693: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.4874e-04 - mse: 4.4874e-04 - val_loss: 2.4281e-04 - val_mse: 2.4281e-04\n",
            "Epoch 694/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.0823e-04 - mse: 5.0823e-04\n",
            "Epoch 694: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.9869e-04 - mse: 4.9869e-04 - val_loss: 2.1617e-04 - val_mse: 2.1617e-04\n",
            "Epoch 695/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.5094e-04 - mse: 4.5094e-04\n",
            "Epoch 695: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4863e-04 - mse: 4.4863e-04 - val_loss: 2.4248e-04 - val_mse: 2.4248e-04\n",
            "Epoch 696/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.1479e-04 - mse: 5.1479e-04\n",
            "Epoch 696: val_loss improved from 0.00022 to 0.00022, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.9815e-04 - mse: 4.9815e-04 - val_loss: 2.1557e-04 - val_mse: 2.1557e-04\n",
            "Epoch 697/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5047e-04 - mse: 4.5047e-04\n",
            "Epoch 697: val_loss did not improve from 0.00022\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5047e-04 - mse: 4.5047e-04 - val_loss: 2.4093e-04 - val_mse: 2.4093e-04\n",
            "Epoch 698/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.9711e-04 - mse: 4.9711e-04\n",
            "Epoch 698: val_loss improved from 0.00022 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.9657e-04 - mse: 4.9657e-04 - val_loss: 2.1498e-04 - val_mse: 2.1498e-04\n",
            "Epoch 699/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.4935e-04 - mse: 4.4935e-04\n",
            "Epoch 699: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.4876e-04 - mse: 4.4876e-04 - val_loss: 2.4049e-04 - val_mse: 2.4049e-04\n",
            "Epoch 700/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.9445e-04 - mse: 4.9445e-04\n",
            "Epoch 700: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.9392e-04 - mse: 4.9392e-04 - val_loss: 2.1455e-04 - val_mse: 2.1455e-04\n",
            "Epoch 701/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.5112e-04 - mse: 4.5112e-04\n",
            "Epoch 701: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4718e-04 - mse: 4.4718e-04 - val_loss: 2.3987e-04 - val_mse: 2.3987e-04\n",
            "Epoch 702/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 5.0042e-04 - mse: 5.0042e-04\n",
            "Epoch 702: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.9138e-04 - mse: 4.9138e-04 - val_loss: 2.1411e-04 - val_mse: 2.1411e-04\n",
            "Epoch 703/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.4579e-04 - mse: 4.4579e-04\n",
            "Epoch 703: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.4561e-04 - mse: 4.4561e-04 - val_loss: 2.3944e-04 - val_mse: 2.3944e-04\n",
            "Epoch 704/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.9879e-04 - mse: 4.9879e-04\n",
            "Epoch 704: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.8987e-04 - mse: 4.8987e-04 - val_loss: 2.1360e-04 - val_mse: 2.1360e-04\n",
            "Epoch 705/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.4673e-04 - mse: 4.4673e-04\n",
            "Epoch 705: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4444e-04 - mse: 4.4444e-04 - val_loss: 2.3902e-04 - val_mse: 2.3902e-04\n",
            "Epoch 706/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.9006e-04 - mse: 4.9006e-04\n",
            "Epoch 706: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.8876e-04 - mse: 4.8876e-04 - val_loss: 2.1310e-04 - val_mse: 2.1310e-04\n",
            "Epoch 707/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.6362e-04 - mse: 4.6362e-04\n",
            "Epoch 707: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4325e-04 - mse: 4.4325e-04 - val_loss: 2.3864e-04 - val_mse: 2.3864e-04\n",
            "Epoch 708/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.8790e-04 - mse: 4.8790e-04\n",
            "Epoch 708: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.8741e-04 - mse: 4.8741e-04 - val_loss: 2.1268e-04 - val_mse: 2.1268e-04\n",
            "Epoch 709/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.5541e-04 - mse: 4.5541e-04\n",
            "Epoch 709: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.4220e-04 - mse: 4.4220e-04 - val_loss: 2.3842e-04 - val_mse: 2.3842e-04\n",
            "Epoch 710/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.9568e-04 - mse: 4.9568e-04\n",
            "Epoch 710: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.8702e-04 - mse: 4.8702e-04 - val_loss: 2.1209e-04 - val_mse: 2.1209e-04\n",
            "Epoch 711/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 4.6026e-04 - mse: 4.6026e-04\n",
            "Epoch 711: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4068e-04 - mse: 4.4068e-04 - val_loss: 2.3812e-04 - val_mse: 2.3812e-04\n",
            "Epoch 712/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.8877e-04 - mse: 4.8877e-04\n",
            "Epoch 712: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8490e-04 - mse: 4.8490e-04 - val_loss: 2.1199e-04 - val_mse: 2.1199e-04\n",
            "Epoch 713/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.4071e-04 - mse: 4.4071e-04\n",
            "Epoch 713: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4013e-04 - mse: 4.4013e-04 - val_loss: 2.3801e-04 - val_mse: 2.3801e-04\n",
            "Epoch 714/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 5.0145e-04 - mse: 5.0145e-04\n",
            "Epoch 714: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.8595e-04 - mse: 4.8595e-04 - val_loss: 2.1125e-04 - val_mse: 2.1125e-04\n",
            "Epoch 715/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3850e-04 - mse: 4.3850e-04\n",
            "Epoch 715: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3850e-04 - mse: 4.3850e-04 - val_loss: 2.3787e-04 - val_mse: 2.3787e-04\n",
            "Epoch 716/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 5.0761e-04 - mse: 5.0761e-04\n",
            "Epoch 716: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8484e-04 - mse: 4.8484e-04 - val_loss: 2.1091e-04 - val_mse: 2.1091e-04\n",
            "Epoch 717/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3735e-04 - mse: 4.3735e-04\n",
            "Epoch 717: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3735e-04 - mse: 4.3735e-04 - val_loss: 2.3766e-04 - val_mse: 2.3766e-04\n",
            "Epoch 718/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.0498e-04 - mse: 5.0498e-04\n",
            "Epoch 718: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.8387e-04 - mse: 4.8387e-04 - val_loss: 2.1059e-04 - val_mse: 2.1059e-04\n",
            "Epoch 719/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.3871e-04 - mse: 4.3871e-04\n",
            "Epoch 719: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3658e-04 - mse: 4.3658e-04 - val_loss: 2.3737e-04 - val_mse: 2.3737e-04\n",
            "Epoch 720/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8270e-04 - mse: 4.8270e-04\n",
            "Epoch 720: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8270e-04 - mse: 4.8270e-04 - val_loss: 2.1026e-04 - val_mse: 2.1026e-04\n",
            "Epoch 721/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.3826e-04 - mse: 4.3826e-04\n",
            "Epoch 721: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3614e-04 - mse: 4.3614e-04 - val_loss: 2.3696e-04 - val_mse: 2.3696e-04\n",
            "Epoch 722/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 5.0249e-04 - mse: 5.0249e-04\n",
            "Epoch 722: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8153e-04 - mse: 4.8153e-04 - val_loss: 2.0987e-04 - val_mse: 2.0987e-04\n",
            "Epoch 723/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3783e-04 - mse: 4.3783e-04\n",
            "Epoch 723: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3413e-04 - mse: 4.3413e-04 - val_loss: 2.3699e-04 - val_mse: 2.3699e-04\n",
            "Epoch 724/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.9567e-04 - mse: 4.9567e-04\n",
            "Epoch 724: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.8063e-04 - mse: 4.8063e-04 - val_loss: 2.0963e-04 - val_mse: 2.0963e-04\n",
            "Epoch 725/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.5262e-04 - mse: 4.5262e-04\n",
            "Epoch 725: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3323e-04 - mse: 4.3323e-04 - val_loss: 2.3691e-04 - val_mse: 2.3691e-04\n",
            "Epoch 726/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.8809e-04 - mse: 4.8809e-04\n",
            "Epoch 726: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7995e-04 - mse: 4.7995e-04 - val_loss: 2.0938e-04 - val_mse: 2.0938e-04\n",
            "Epoch 727/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.4568e-04 - mse: 4.4568e-04\n",
            "Epoch 727: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3312e-04 - mse: 4.3312e-04 - val_loss: 2.3642e-04 - val_mse: 2.3642e-04\n",
            "Epoch 728/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.7835e-04 - mse: 4.7835e-04\n",
            "Epoch 728: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7769e-04 - mse: 4.7769e-04 - val_loss: 2.0934e-04 - val_mse: 2.0934e-04\n",
            "Epoch 729/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.4493e-04 - mse: 4.4493e-04\n",
            "Epoch 729: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3242e-04 - mse: 4.3242e-04 - val_loss: 2.3628e-04 - val_mse: 2.3628e-04\n",
            "Epoch 730/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.9176e-04 - mse: 4.9176e-04\n",
            "Epoch 730: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7705e-04 - mse: 4.7705e-04 - val_loss: 2.0901e-04 - val_mse: 2.0901e-04\n",
            "Epoch 731/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.3320e-04 - mse: 4.3320e-04\n",
            "Epoch 731: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3115e-04 - mse: 4.3115e-04 - val_loss: 2.3626e-04 - val_mse: 2.3626e-04\n",
            "Epoch 732/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 4.9670e-04 - mse: 4.9670e-04\n",
            "Epoch 732: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7613e-04 - mse: 4.7613e-04 - val_loss: 2.0868e-04 - val_mse: 2.0868e-04\n",
            "Epoch 733/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3481e-04 - mse: 4.3481e-04\n",
            "Epoch 733: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3115e-04 - mse: 4.3115e-04 - val_loss: 2.3569e-04 - val_mse: 2.3569e-04\n",
            "Epoch 734/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.7587e-04 - mse: 4.7587e-04\n",
            "Epoch 734: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7544e-04 - mse: 4.7544e-04 - val_loss: 2.0873e-04 - val_mse: 2.0873e-04\n",
            "Epoch 735/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.2999e-04 - mse: 4.2999e-04\n",
            "Epoch 735: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3028e-04 - mse: 4.3028e-04 - val_loss: 2.3612e-04 - val_mse: 2.3612e-04\n",
            "Epoch 736/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.7580e-04 - mse: 4.7580e-04\n",
            "Epoch 736: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.7524e-04 - mse: 4.7524e-04 - val_loss: 2.0856e-04 - val_mse: 2.0856e-04\n",
            "Epoch 737/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.4118e-04 - mse: 4.4118e-04\n",
            "Epoch 737: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2891e-04 - mse: 4.2891e-04 - val_loss: 2.3633e-04 - val_mse: 2.3633e-04\n",
            "Epoch 738/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.7494e-04 - mse: 4.7494e-04\n",
            "Epoch 738: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7449e-04 - mse: 4.7449e-04 - val_loss: 2.0829e-04 - val_mse: 2.0829e-04\n",
            "Epoch 739/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.4757e-04 - mse: 4.4757e-04\n",
            "Epoch 739: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 4.2861e-04 - mse: 4.2861e-04 - val_loss: 2.3593e-04 - val_mse: 2.3593e-04\n",
            "Epoch 740/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.8830e-04 - mse: 4.8830e-04\n",
            "Epoch 740: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7384e-04 - mse: 4.7384e-04 - val_loss: 2.0815e-04 - val_mse: 2.0815e-04\n",
            "Epoch 741/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3238e-04 - mse: 4.3238e-04\n",
            "Epoch 741: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2874e-04 - mse: 4.2874e-04 - val_loss: 2.3561e-04 - val_mse: 2.3561e-04\n",
            "Epoch 742/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.7516e-04 - mse: 4.7516e-04\n",
            "Epoch 742: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7304e-04 - mse: 4.7304e-04 - val_loss: 2.0806e-04 - val_mse: 2.0806e-04\n",
            "Epoch 743/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2542e-04 - mse: 4.2542e-04\n",
            "Epoch 743: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2542e-04 - mse: 4.2542e-04 - val_loss: 2.3672e-04 - val_mse: 2.3672e-04\n",
            "Epoch 744/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.7608e-04 - mse: 4.7608e-04\n",
            "Epoch 744: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7253e-04 - mse: 4.7253e-04 - val_loss: 2.0821e-04 - val_mse: 2.0821e-04\n",
            "Epoch 745/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3044e-04 - mse: 4.3044e-04\n",
            "Epoch 745: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2682e-04 - mse: 4.2682e-04 - val_loss: 2.3601e-04 - val_mse: 2.3601e-04\n",
            "Epoch 746/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.7319e-04 - mse: 4.7319e-04\n",
            "Epoch 746: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.7110e-04 - mse: 4.7110e-04 - val_loss: 2.0811e-04 - val_mse: 2.0811e-04\n",
            "Epoch 747/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.2830e-04 - mse: 4.2830e-04\n",
            "Epoch 747: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2623e-04 - mse: 4.2623e-04 - val_loss: 2.3592e-04 - val_mse: 2.3592e-04\n",
            "Epoch 748/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.7098e-04 - mse: 4.7098e-04\n",
            "Epoch 748: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.7054e-04 - mse: 4.7054e-04 - val_loss: 2.0780e-04 - val_mse: 2.0780e-04\n",
            "Epoch 749/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2367e-04 - mse: 4.2367e-04\n",
            "Epoch 749: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2367e-04 - mse: 4.2367e-04 - val_loss: 2.3643e-04 - val_mse: 2.3643e-04\n",
            "Epoch 750/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6987e-04 - mse: 4.6987e-04\n",
            "Epoch 750: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6987e-04 - mse: 4.6987e-04 - val_loss: 2.0786e-04 - val_mse: 2.0786e-04\n",
            "Epoch 751/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.2338e-04 - mse: 4.2338e-04\n",
            "Epoch 751: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2277e-04 - mse: 4.2277e-04 - val_loss: 2.3672e-04 - val_mse: 2.3672e-04\n",
            "Epoch 752/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.6985e-04 - mse: 4.6985e-04\n",
            "Epoch 752: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.6939e-04 - mse: 4.6939e-04 - val_loss: 2.0784e-04 - val_mse: 2.0784e-04\n",
            "Epoch 753/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2207e-04 - mse: 4.2207e-04\n",
            "Epoch 753: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.2207e-04 - mse: 4.2207e-04 - val_loss: 2.3687e-04 - val_mse: 2.3687e-04\n",
            "Epoch 754/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.7110e-04 - mse: 4.7110e-04\n",
            "Epoch 754: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6901e-04 - mse: 4.6901e-04 - val_loss: 2.0774e-04 - val_mse: 2.0774e-04\n",
            "Epoch 755/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.2223e-04 - mse: 4.2223e-04\n",
            "Epoch 755: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2161e-04 - mse: 4.2161e-04 - val_loss: 2.3688e-04 - val_mse: 2.3688e-04\n",
            "Epoch 756/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.8988e-04 - mse: 4.8988e-04\n",
            "Epoch 756: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6878e-04 - mse: 4.6878e-04 - val_loss: 2.0754e-04 - val_mse: 2.0754e-04\n",
            "Epoch 757/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2625e-04 - mse: 4.2625e-04\n",
            "Epoch 757: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.2043e-04 - mse: 4.2043e-04 - val_loss: 2.3705e-04 - val_mse: 2.3705e-04\n",
            "Epoch 758/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.6829e-04 - mse: 4.6829e-04\n",
            "Epoch 758: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 19ms/step - loss: 4.6782e-04 - mse: 4.6782e-04 - val_loss: 2.0760e-04 - val_mse: 2.0760e-04\n",
            "Epoch 759/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.1909e-04 - mse: 4.1909e-04\n",
            "Epoch 759: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 4.1956e-04 - mse: 4.1956e-04 - val_loss: 2.3729e-04 - val_mse: 2.3729e-04\n",
            "Epoch 760/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.6779e-04 - mse: 4.6779e-04\n",
            "Epoch 760: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 4.6731e-04 - mse: 4.6731e-04 - val_loss: 2.0760e-04 - val_mse: 2.0760e-04\n",
            "Epoch 761/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.1948e-04 - mse: 4.1948e-04\n",
            "Epoch 761: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 4.1885e-04 - mse: 4.1885e-04 - val_loss: 2.3748e-04 - val_mse: 2.3748e-04\n",
            "Epoch 762/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6691e-04 - mse: 4.6691e-04\n",
            "Epoch 762: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 4.6691e-04 - mse: 4.6691e-04 - val_loss: 2.0757e-04 - val_mse: 2.0757e-04\n",
            "Epoch 763/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.2017e-04 - mse: 4.2017e-04\n",
            "Epoch 763: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.1816e-04 - mse: 4.1816e-04 - val_loss: 2.3767e-04 - val_mse: 2.3767e-04\n",
            "Epoch 764/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.7417e-04 - mse: 4.7417e-04\n",
            "Epoch 764: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 4.6666e-04 - mse: 4.6666e-04 - val_loss: 2.0766e-04 - val_mse: 2.0766e-04\n",
            "Epoch 765/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2432e-04 - mse: 4.2432e-04\n",
            "Epoch 765: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.1852e-04 - mse: 4.1852e-04 - val_loss: 2.3753e-04 - val_mse: 2.3753e-04\n",
            "Epoch 766/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.7973e-04 - mse: 4.7973e-04\n",
            "Epoch 766: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6575e-04 - mse: 4.6575e-04 - val_loss: 2.0753e-04 - val_mse: 2.0753e-04\n",
            "Epoch 767/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2365e-04 - mse: 4.2365e-04\n",
            "Epoch 767: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1785e-04 - mse: 4.1785e-04 - val_loss: 2.3727e-04 - val_mse: 2.3727e-04\n",
            "Epoch 768/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.8462e-04 - mse: 4.8462e-04\n",
            "Epoch 768: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6394e-04 - mse: 4.6394e-04 - val_loss: 2.0741e-04 - val_mse: 2.0741e-04\n",
            "Epoch 769/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.1741e-04 - mse: 4.1741e-04\n",
            "Epoch 769: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1541e-04 - mse: 4.1541e-04 - val_loss: 2.3784e-04 - val_mse: 2.3784e-04\n",
            "Epoch 770/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.7786e-04 - mse: 4.7786e-04\n",
            "Epoch 770: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.6399e-04 - mse: 4.6399e-04 - val_loss: 2.0711e-04 - val_mse: 2.0711e-04\n",
            "Epoch 771/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.2157e-04 - mse: 4.2157e-04\n",
            "Epoch 771: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1799e-04 - mse: 4.1799e-04 - val_loss: 2.3590e-04 - val_mse: 2.3590e-04\n",
            "Epoch 772/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.6301e-04 - mse: 4.6301e-04\n",
            "Epoch 772: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6272e-04 - mse: 4.6272e-04 - val_loss: 2.0733e-04 - val_mse: 2.0733e-04\n",
            "Epoch 773/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2400e-04 - mse: 4.2400e-04\n",
            "Epoch 773: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1806e-04 - mse: 4.1806e-04 - val_loss: 2.3650e-04 - val_mse: 2.3650e-04\n",
            "Epoch 774/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.7643e-04 - mse: 4.7643e-04\n",
            "Epoch 774: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6268e-04 - mse: 4.6268e-04 - val_loss: 2.0746e-04 - val_mse: 2.0746e-04\n",
            "Epoch 775/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2353e-04 - mse: 4.2353e-04\n",
            "Epoch 775: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.1759e-04 - mse: 4.1759e-04 - val_loss: 2.3680e-04 - val_mse: 2.3680e-04\n",
            "Epoch 776/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.6970e-04 - mse: 4.6970e-04\n",
            "Epoch 776: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6235e-04 - mse: 4.6235e-04 - val_loss: 2.0752e-04 - val_mse: 2.0752e-04\n",
            "Epoch 777/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.1683e-04 - mse: 4.1683e-04\n",
            "Epoch 777: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1705e-04 - mse: 4.1705e-04 - val_loss: 2.3690e-04 - val_mse: 2.3690e-04\n",
            "Epoch 778/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.8240e-04 - mse: 4.8240e-04\n",
            "Epoch 778: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.6190e-04 - mse: 4.6190e-04 - val_loss: 2.0735e-04 - val_mse: 2.0735e-04\n",
            "Epoch 779/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2179e-04 - mse: 4.2179e-04\n",
            "Epoch 779: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1588e-04 - mse: 4.1588e-04 - val_loss: 2.3694e-04 - val_mse: 2.3694e-04\n",
            "Epoch 780/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6105e-04 - mse: 4.6105e-04\n",
            "Epoch 780: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.6105e-04 - mse: 4.6105e-04 - val_loss: 2.0732e-04 - val_mse: 2.0732e-04\n",
            "Epoch 781/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.1473e-04 - mse: 4.1473e-04\n",
            "Epoch 781: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1497e-04 - mse: 4.1497e-04 - val_loss: 2.3715e-04 - val_mse: 2.3715e-04\n",
            "Epoch 782/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6099e-04 - mse: 4.6099e-04\n",
            "Epoch 782: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.6099e-04 - mse: 4.6099e-04 - val_loss: 2.0696e-04 - val_mse: 2.0696e-04\n",
            "Epoch 783/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.1436e-04 - mse: 4.1436e-04\n",
            "Epoch 783: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1362e-04 - mse: 4.1362e-04 - val_loss: 2.3706e-04 - val_mse: 2.3706e-04\n",
            "Epoch 784/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5937e-04 - mse: 4.5937e-04\n",
            "Epoch 784: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5937e-04 - mse: 4.5937e-04 - val_loss: 2.0722e-04 - val_mse: 2.0722e-04\n",
            "Epoch 785/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.1526e-04 - mse: 4.1526e-04\n",
            "Epoch 785: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1299e-04 - mse: 4.1299e-04 - val_loss: 2.3738e-04 - val_mse: 2.3738e-04\n",
            "Epoch 786/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.6308e-04 - mse: 4.6308e-04\n",
            "Epoch 786: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.5960e-04 - mse: 4.5960e-04 - val_loss: 2.0718e-04 - val_mse: 2.0718e-04\n",
            "Epoch 787/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.1236e-04 - mse: 4.1236e-04\n",
            "Epoch 787: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1161e-04 - mse: 4.1161e-04 - val_loss: 2.3791e-04 - val_mse: 2.3791e-04\n",
            "Epoch 788/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.6003e-04 - mse: 4.6003e-04\n",
            "Epoch 788: val_loss improved from 0.00021 to 0.00021, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5945e-04 - mse: 4.5945e-04 - val_loss: 2.0685e-04 - val_mse: 2.0685e-04\n",
            "Epoch 789/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.1216e-04 - mse: 4.1216e-04\n",
            "Epoch 789: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1239e-04 - mse: 4.1239e-04 - val_loss: 2.3717e-04 - val_mse: 2.3717e-04\n",
            "Epoch 790/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.7953e-04 - mse: 4.7953e-04\n",
            "Epoch 790: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5920e-04 - mse: 4.5920e-04 - val_loss: 2.0737e-04 - val_mse: 2.0737e-04\n",
            "Epoch 791/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.1380e-04 - mse: 4.1380e-04\n",
            "Epoch 791: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1150e-04 - mse: 4.1150e-04 - val_loss: 2.3834e-04 - val_mse: 2.3834e-04\n",
            "Epoch 792/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.7955e-04 - mse: 4.7955e-04\n",
            "Epoch 792: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5918e-04 - mse: 4.5918e-04 - val_loss: 2.0711e-04 - val_mse: 2.0711e-04\n",
            "Epoch 793/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.2336e-04 - mse: 4.2336e-04\n",
            "Epoch 793: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1186e-04 - mse: 4.1186e-04 - val_loss: 2.3778e-04 - val_mse: 2.3778e-04\n",
            "Epoch 794/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.6609e-04 - mse: 4.6609e-04\n",
            "Epoch 794: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5868e-04 - mse: 4.5868e-04 - val_loss: 2.0762e-04 - val_mse: 2.0762e-04\n",
            "Epoch 795/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.1657e-04 - mse: 4.1657e-04\n",
            "Epoch 795: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1075e-04 - mse: 4.1075e-04 - val_loss: 2.3901e-04 - val_mse: 2.3901e-04\n",
            "Epoch 796/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.6121e-04 - mse: 4.6121e-04\n",
            "Epoch 796: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5884e-04 - mse: 4.5884e-04 - val_loss: 2.0737e-04 - val_mse: 2.0737e-04\n",
            "Epoch 797/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.1210e-04 - mse: 4.1210e-04\n",
            "Epoch 797: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.1131e-04 - mse: 4.1131e-04 - val_loss: 2.3816e-04 - val_mse: 2.3816e-04\n",
            "Epoch 798/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.6058e-04 - mse: 4.6058e-04\n",
            "Epoch 798: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5821e-04 - mse: 4.5821e-04 - val_loss: 2.0778e-04 - val_mse: 2.0778e-04\n",
            "Epoch 799/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.2142e-04 - mse: 4.2142e-04\n",
            "Epoch 799: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0997e-04 - mse: 4.0997e-04 - val_loss: 2.3932e-04 - val_mse: 2.3932e-04\n",
            "Epoch 800/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.6564e-04 - mse: 4.6564e-04\n",
            "Epoch 800: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5809e-04 - mse: 4.5809e-04 - val_loss: 2.0753e-04 - val_mse: 2.0753e-04\n",
            "Epoch 801/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.1599e-04 - mse: 4.1599e-04\n",
            "Epoch 801: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1011e-04 - mse: 4.1011e-04 - val_loss: 2.3862e-04 - val_mse: 2.3862e-04\n",
            "Epoch 802/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5740e-04 - mse: 4.5740e-04\n",
            "Epoch 802: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5740e-04 - mse: 4.5740e-04 - val_loss: 2.0749e-04 - val_mse: 2.0749e-04\n",
            "Epoch 803/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.1608e-04 - mse: 4.1608e-04\n",
            "Epoch 803: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1015e-04 - mse: 4.1015e-04 - val_loss: 2.3865e-04 - val_mse: 2.3865e-04\n",
            "Epoch 804/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5728e-04 - mse: 4.5728e-04\n",
            "Epoch 804: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5728e-04 - mse: 4.5728e-04 - val_loss: 2.0738e-04 - val_mse: 2.0738e-04\n",
            "Epoch 805/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.1171e-04 - mse: 4.1171e-04\n",
            "Epoch 805: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0922e-04 - mse: 4.0922e-04 - val_loss: 2.3867e-04 - val_mse: 2.3867e-04\n",
            "Epoch 806/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.6419e-04 - mse: 4.6419e-04\n",
            "Epoch 806: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5661e-04 - mse: 4.5661e-04 - val_loss: 2.0750e-04 - val_mse: 2.0750e-04\n",
            "Epoch 807/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.0853e-04 - mse: 4.0853e-04\n",
            "Epoch 807: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0857e-04 - mse: 4.0857e-04 - val_loss: 2.3873e-04 - val_mse: 2.3873e-04\n",
            "Epoch 808/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.6891e-04 - mse: 4.6891e-04\n",
            "Epoch 808: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5522e-04 - mse: 4.5522e-04 - val_loss: 2.0771e-04 - val_mse: 2.0771e-04\n",
            "Epoch 809/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.1409e-04 - mse: 4.1409e-04\n",
            "Epoch 809: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.0817e-04 - mse: 4.0817e-04 - val_loss: 2.3919e-04 - val_mse: 2.3919e-04\n",
            "Epoch 810/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.5548e-04 - mse: 4.5548e-04\n",
            "Epoch 810: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5476e-04 - mse: 4.5476e-04 - val_loss: 2.0889e-04 - val_mse: 2.0889e-04\n",
            "Epoch 811/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0921e-04 - mse: 4.0921e-04\n",
            "Epoch 811: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0554e-04 - mse: 4.0554e-04 - val_loss: 2.4128e-04 - val_mse: 2.4128e-04\n",
            "Epoch 812/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 4.7416e-04 - mse: 4.7416e-04\n",
            "Epoch 812: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.5419e-04 - mse: 4.5419e-04 - val_loss: 2.0918e-04 - val_mse: 2.0918e-04\n",
            "Epoch 813/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.1292e-04 - mse: 4.1292e-04\n",
            "Epoch 813: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0185e-04 - mse: 4.0185e-04 - val_loss: 2.4291e-04 - val_mse: 2.4291e-04\n",
            "Epoch 814/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.7345e-04 - mse: 4.7345e-04\n",
            "Epoch 814: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5336e-04 - mse: 4.5336e-04 - val_loss: 2.0801e-04 - val_mse: 2.0801e-04\n",
            "Epoch 815/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.1844e-04 - mse: 4.1844e-04\n",
            "Epoch 815: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0146e-04 - mse: 4.0146e-04 - val_loss: 2.4070e-04 - val_mse: 2.4070e-04\n",
            "Epoch 816/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.5091e-04 - mse: 4.5091e-04\n",
            "Epoch 816: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.5014e-04 - mse: 4.5014e-04 - val_loss: 2.0910e-04 - val_mse: 2.0910e-04\n",
            "Epoch 817/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0036e-04 - mse: 4.0036e-04\n",
            "Epoch 817: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9678e-04 - mse: 3.9678e-04 - val_loss: 2.4280e-04 - val_mse: 2.4280e-04\n",
            "Epoch 818/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.5299e-04 - mse: 4.5299e-04\n",
            "Epoch 818: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4936e-04 - mse: 4.4936e-04 - val_loss: 2.0826e-04 - val_mse: 2.0826e-04\n",
            "Epoch 819/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.9429e-04 - mse: 3.9429e-04\n",
            "Epoch 819: val_loss did not improve from 0.00021\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9344e-04 - mse: 3.9344e-04 - val_loss: 2.4293e-04 - val_mse: 2.4293e-04\n",
            "Epoch 820/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.6219e-04 - mse: 4.6219e-04\n",
            "Epoch 820: val_loss improved from 0.00021 to 0.00020, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4898e-04 - mse: 4.4898e-04 - val_loss: 2.0473e-04 - val_mse: 2.0473e-04\n",
            "Epoch 821/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0232e-04 - mse: 4.0232e-04\n",
            "Epoch 821: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9876e-04 - mse: 3.9876e-04 - val_loss: 2.3741e-04 - val_mse: 2.3741e-04\n",
            "Epoch 822/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.4775e-04 - mse: 4.4775e-04\n",
            "Epoch 822: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4731e-04 - mse: 4.4731e-04 - val_loss: 2.0615e-04 - val_mse: 2.0615e-04\n",
            "Epoch 823/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.0325e-04 - mse: 4.0325e-04\n",
            "Epoch 823: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9789e-04 - mse: 3.9789e-04 - val_loss: 2.3906e-04 - val_mse: 2.3906e-04\n",
            "Epoch 824/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.5068e-04 - mse: 4.5068e-04\n",
            "Epoch 824: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4720e-04 - mse: 4.4720e-04 - val_loss: 2.0780e-04 - val_mse: 2.0780e-04\n",
            "Epoch 825/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.0375e-04 - mse: 4.0375e-04\n",
            "Epoch 825: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9330e-04 - mse: 3.9330e-04 - val_loss: 2.4265e-04 - val_mse: 2.4265e-04\n",
            "Epoch 826/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.5390e-04 - mse: 4.5390e-04\n",
            "Epoch 826: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4676e-04 - mse: 4.4676e-04 - val_loss: 2.0768e-04 - val_mse: 2.0768e-04\n",
            "Epoch 827/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9073e-04 - mse: 3.9073e-04\n",
            "Epoch 827: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9136e-04 - mse: 3.9136e-04 - val_loss: 2.4276e-04 - val_mse: 2.4276e-04\n",
            "Epoch 828/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.5403e-04 - mse: 4.5403e-04\n",
            "Epoch 828: val_loss improved from 0.00020 to 0.00020, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4709e-04 - mse: 4.4709e-04 - val_loss: 2.0387e-04 - val_mse: 2.0387e-04\n",
            "Epoch 829/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.9767e-04 - mse: 3.9767e-04\n",
            "Epoch 829: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9682e-04 - mse: 3.9682e-04 - val_loss: 2.3692e-04 - val_mse: 2.3692e-04\n",
            "Epoch 830/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.5841e-04 - mse: 4.5841e-04\n",
            "Epoch 830: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4552e-04 - mse: 4.4552e-04 - val_loss: 2.0547e-04 - val_mse: 2.0547e-04\n",
            "Epoch 831/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9529e-04 - mse: 3.9529e-04\n",
            "Epoch 831: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9575e-04 - mse: 3.9575e-04 - val_loss: 2.3893e-04 - val_mse: 2.3893e-04\n",
            "Epoch 832/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.5875e-04 - mse: 4.5875e-04\n",
            "Epoch 832: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4580e-04 - mse: 4.4580e-04 - val_loss: 2.0581e-04 - val_mse: 2.0581e-04\n",
            "Epoch 833/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9449e-04 - mse: 3.9449e-04\n",
            "Epoch 833: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9449e-04 - mse: 3.9449e-04 - val_loss: 2.3970e-04 - val_mse: 2.3970e-04\n",
            "Epoch 834/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4468e-04 - mse: 4.4468e-04\n",
            "Epoch 834: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 4.4468e-04 - mse: 4.4468e-04 - val_loss: 2.0745e-04 - val_mse: 2.0745e-04\n",
            "Epoch 835/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8929e-04 - mse: 3.8929e-04\n",
            "Epoch 835: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 3.8996e-04 - mse: 3.8996e-04 - val_loss: 2.4319e-04 - val_mse: 2.4319e-04\n",
            "Epoch 836/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.4633e-04 - mse: 4.4633e-04\n",
            "Epoch 836: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4391e-04 - mse: 4.4391e-04 - val_loss: 2.0769e-04 - val_mse: 2.0769e-04\n",
            "Epoch 837/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 3.9805e-04 - mse: 3.9805e-04\n",
            "Epoch 837: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8790e-04 - mse: 3.8790e-04 - val_loss: 2.4342e-04 - val_mse: 2.4342e-04\n",
            "Epoch 838/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.4502e-04 - mse: 4.4502e-04\n",
            "Epoch 838: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4432e-04 - mse: 4.4432e-04 - val_loss: 2.0422e-04 - val_mse: 2.0422e-04\n",
            "Epoch 839/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9154e-04 - mse: 3.9154e-04\n",
            "Epoch 839: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9209e-04 - mse: 3.9209e-04 - val_loss: 2.3856e-04 - val_mse: 2.3856e-04\n",
            "Epoch 840/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.5501e-04 - mse: 4.5501e-04\n",
            "Epoch 840: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4231e-04 - mse: 4.4231e-04 - val_loss: 2.0669e-04 - val_mse: 2.0669e-04\n",
            "Epoch 841/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8803e-04 - mse: 3.8803e-04\n",
            "Epoch 841: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8803e-04 - mse: 3.8803e-04 - val_loss: 2.4293e-04 - val_mse: 2.4293e-04\n",
            "Epoch 842/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4417e-04 - mse: 4.4417e-04\n",
            "Epoch 842: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4417e-04 - mse: 4.4417e-04 - val_loss: 2.0411e-04 - val_mse: 2.0411e-04\n",
            "Epoch 843/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.9273e-04 - mse: 3.9273e-04\n",
            "Epoch 843: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9048e-04 - mse: 3.9048e-04 - val_loss: 2.3934e-04 - val_mse: 2.3934e-04\n",
            "Epoch 844/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4173e-04 - mse: 4.4173e-04\n",
            "Epoch 844: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.4173e-04 - mse: 4.4173e-04 - val_loss: 2.0641e-04 - val_mse: 2.0641e-04\n",
            "Epoch 845/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 3.9105e-04 - mse: 3.9105e-04\n",
            "Epoch 845: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8618e-04 - mse: 3.8618e-04 - val_loss: 2.4310e-04 - val_mse: 2.4310e-04\n",
            "Epoch 846/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.5545e-04 - mse: 4.5545e-04\n",
            "Epoch 846: val_loss improved from 0.00020 to 0.00020, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.4273e-04 - mse: 4.4273e-04 - val_loss: 2.0347e-04 - val_mse: 2.0347e-04\n",
            "Epoch 847/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.9237e-04 - mse: 3.9237e-04\n",
            "Epoch 847: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8892e-04 - mse: 3.8892e-04 - val_loss: 2.3836e-04 - val_mse: 2.3836e-04\n",
            "Epoch 848/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.4305e-04 - mse: 4.4305e-04\n",
            "Epoch 848: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3970e-04 - mse: 4.3970e-04 - val_loss: 2.0604e-04 - val_mse: 2.0604e-04\n",
            "Epoch 849/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.0245e-04 - mse: 4.0245e-04\n",
            "Epoch 849: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8672e-04 - mse: 3.8672e-04 - val_loss: 2.4119e-04 - val_mse: 2.4119e-04\n",
            "Epoch 850/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.5382e-04 - mse: 4.5382e-04\n",
            "Epoch 850: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.3533e-04 - mse: 4.3533e-04 - val_loss: 2.0996e-04 - val_mse: 2.0996e-04\n",
            "Epoch 851/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.8381e-04 - mse: 3.8381e-04\n",
            "Epoch 851: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8163e-04 - mse: 3.8163e-04 - val_loss: 2.4478e-04 - val_mse: 2.4478e-04\n",
            "Epoch 852/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.4034e-04 - mse: 4.4034e-04\n",
            "Epoch 852: val_loss improved from 0.00020 to 0.00020, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.3962e-04 - mse: 4.3962e-04 - val_loss: 2.0257e-04 - val_mse: 2.0257e-04\n",
            "Epoch 853/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 3.9619e-04 - mse: 3.9619e-04\n",
            "Epoch 853: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8627e-04 - mse: 3.8627e-04 - val_loss: 2.3778e-04 - val_mse: 2.3778e-04\n",
            "Epoch 854/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3781e-04 - mse: 4.3781e-04\n",
            "Epoch 854: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3455e-04 - mse: 4.3455e-04 - val_loss: 2.0687e-04 - val_mse: 2.0687e-04\n",
            "Epoch 855/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 3.9218e-04 - mse: 3.9218e-04\n",
            "Epoch 855: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8235e-04 - mse: 3.8235e-04 - val_loss: 2.4271e-04 - val_mse: 2.4271e-04\n",
            "Epoch 856/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3723e-04 - mse: 4.3723e-04\n",
            "Epoch 856: val_loss improved from 0.00020 to 0.00020, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3723e-04 - mse: 4.3723e-04 - val_loss: 2.0251e-04 - val_mse: 2.0251e-04\n",
            "Epoch 857/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.8772e-04 - mse: 3.8772e-04\n",
            "Epoch 857: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8553e-04 - mse: 3.8553e-04 - val_loss: 2.3786e-04 - val_mse: 2.3786e-04\n",
            "Epoch 858/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.3420e-04 - mse: 4.3420e-04\n",
            "Epoch 858: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3347e-04 - mse: 4.3347e-04 - val_loss: 2.0820e-04 - val_mse: 2.0820e-04\n",
            "Epoch 859/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.8229e-04 - mse: 3.8229e-04\n",
            "Epoch 859: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.7887e-04 - mse: 3.7887e-04 - val_loss: 2.4543e-04 - val_mse: 2.4543e-04\n",
            "Epoch 860/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.4864e-04 - mse: 4.4864e-04\n",
            "Epoch 860: val_loss improved from 0.00020 to 0.00020, saving model to best_model.ckpt\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.3643e-04 - mse: 4.3643e-04 - val_loss: 2.0226e-04 - val_mse: 2.0226e-04\n",
            "Epoch 861/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8262e-04 - mse: 3.8262e-04\n",
            "Epoch 861: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8341e-04 - mse: 3.8341e-04 - val_loss: 2.3781e-04 - val_mse: 2.3781e-04\n",
            "Epoch 862/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3164e-04 - mse: 4.3164e-04\n",
            "Epoch 862: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3164e-04 - mse: 4.3164e-04 - val_loss: 2.0773e-04 - val_mse: 2.0773e-04\n",
            "Epoch 863/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.7836e-04 - mse: 3.7836e-04\n",
            "Epoch 863: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.7743e-04 - mse: 3.7743e-04 - val_loss: 2.4525e-04 - val_mse: 2.4525e-04\n",
            "Epoch 864/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.3686e-04 - mse: 4.3686e-04\n",
            "Epoch 864: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3494e-04 - mse: 4.3494e-04 - val_loss: 2.0240e-04 - val_mse: 2.0240e-04\n",
            "Epoch 865/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 3.9539e-04 - mse: 3.9539e-04\n",
            "Epoch 865: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8032e-04 - mse: 3.8032e-04 - val_loss: 2.3897e-04 - val_mse: 2.3897e-04\n",
            "Epoch 866/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 4.4884e-04 - mse: 4.4884e-04\n",
            "Epoch 866: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.3034e-04 - mse: 4.3034e-04 - val_loss: 2.0630e-04 - val_mse: 2.0630e-04\n",
            "Epoch 867/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7963e-04 - mse: 3.7963e-04\n",
            "Epoch 867: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.7963e-04 - mse: 3.7963e-04 - val_loss: 2.4091e-04 - val_mse: 2.4091e-04\n",
            "Epoch 868/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.3517e-04 - mse: 4.3517e-04\n",
            "Epoch 868: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2890e-04 - mse: 4.2890e-04 - val_loss: 2.0723e-04 - val_mse: 2.0723e-04\n",
            "Epoch 869/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 3.8271e-04 - mse: 3.8271e-04\n",
            "Epoch 869: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.7826e-04 - mse: 3.7826e-04 - val_loss: 2.4147e-04 - val_mse: 2.4147e-04\n",
            "Epoch 870/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.2378e-04 - mse: 4.2378e-04\n",
            "Epoch 870: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2279e-04 - mse: 4.2279e-04 - val_loss: 2.1396e-04 - val_mse: 2.1396e-04\n",
            "Epoch 871/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.2450e-04 - mse: 4.2450e-04\n",
            "Epoch 871: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0796e-04 - mse: 4.0796e-04 - val_loss: 2.2097e-04 - val_mse: 2.2097e-04\n",
            "Epoch 872/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.3537e-04 - mse: 4.3537e-04\n",
            "Epoch 872: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1792e-04 - mse: 4.1792e-04 - val_loss: 2.2077e-04 - val_mse: 2.2077e-04\n",
            "Epoch 873/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.1907e-04 - mse: 4.1907e-04\n",
            "Epoch 873: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1808e-04 - mse: 4.1808e-04 - val_loss: 2.2312e-04 - val_mse: 2.2312e-04\n",
            "Epoch 874/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.2795e-04 - mse: 4.2795e-04\n",
            "Epoch 874: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2428e-04 - mse: 4.2428e-04 - val_loss: 2.2177e-04 - val_mse: 2.2177e-04\n",
            "Epoch 875/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.2217e-04 - mse: 4.2217e-04\n",
            "Epoch 875: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2123e-04 - mse: 4.2123e-04 - val_loss: 2.2598e-04 - val_mse: 2.2598e-04\n",
            "Epoch 876/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.3014e-04 - mse: 4.3014e-04\n",
            "Epoch 876: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2684e-04 - mse: 4.2684e-04 - val_loss: 2.2519e-04 - val_mse: 2.2519e-04\n",
            "Epoch 877/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.2703e-04 - mse: 4.2703e-04\n",
            "Epoch 877: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2577e-04 - mse: 4.2577e-04 - val_loss: 2.2750e-04 - val_mse: 2.2750e-04\n",
            "Epoch 878/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.2944e-04 - mse: 4.2944e-04\n",
            "Epoch 878: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2793e-04 - mse: 4.2793e-04 - val_loss: 2.2889e-04 - val_mse: 2.2889e-04\n",
            "Epoch 879/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.3197e-04 - mse: 4.3197e-04\n",
            "Epoch 879: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3024e-04 - mse: 4.3024e-04 - val_loss: 2.2963e-04 - val_mse: 2.2963e-04\n",
            "Epoch 880/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2929e-04 - mse: 4.2929e-04\n",
            "Epoch 880: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2929e-04 - mse: 4.2929e-04 - val_loss: 2.3133e-04 - val_mse: 2.3133e-04\n",
            "Epoch 881/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3595e-04 - mse: 4.3595e-04\n",
            "Epoch 881: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3169e-04 - mse: 4.3169e-04 - val_loss: 2.3141e-04 - val_mse: 2.3141e-04\n",
            "Epoch 882/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.4861e-04 - mse: 4.4861e-04\n",
            "Epoch 882: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2978e-04 - mse: 4.2978e-04 - val_loss: 2.3332e-04 - val_mse: 2.3332e-04\n",
            "Epoch 883/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.3131e-04 - mse: 4.3131e-04\n",
            "Epoch 883: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3002e-04 - mse: 4.3002e-04 - val_loss: 2.3449e-04 - val_mse: 2.3449e-04\n",
            "Epoch 884/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3365e-04 - mse: 4.3365e-04\n",
            "Epoch 884: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3365e-04 - mse: 4.3365e-04 - val_loss: 2.3355e-04 - val_mse: 2.3355e-04\n",
            "Epoch 885/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.3239e-04 - mse: 4.3239e-04\n",
            "Epoch 885: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2789e-04 - mse: 4.2789e-04 - val_loss: 2.3671e-04 - val_mse: 2.3671e-04\n",
            "Epoch 886/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.4040e-04 - mse: 4.4040e-04\n",
            "Epoch 886: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3188e-04 - mse: 4.3188e-04 - val_loss: 2.3582e-04 - val_mse: 2.3582e-04\n",
            "Epoch 887/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.3194e-04 - mse: 4.3194e-04\n",
            "Epoch 887: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2922e-04 - mse: 4.2922e-04 - val_loss: 2.3726e-04 - val_mse: 2.3726e-04\n",
            "Epoch 888/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.3311e-04 - mse: 4.3311e-04\n",
            "Epoch 888: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3172e-04 - mse: 4.3172e-04 - val_loss: 2.3616e-04 - val_mse: 2.3616e-04\n",
            "Epoch 889/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.2925e-04 - mse: 4.2925e-04\n",
            "Epoch 889: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2781e-04 - mse: 4.2781e-04 - val_loss: 2.3807e-04 - val_mse: 2.3807e-04\n",
            "Epoch 890/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.3284e-04 - mse: 4.3284e-04\n",
            "Epoch 890: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.3005e-04 - mse: 4.3005e-04 - val_loss: 2.3665e-04 - val_mse: 2.3665e-04\n",
            "Epoch 891/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.2452e-04 - mse: 4.2452e-04\n",
            "Epoch 891: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.1976e-04 - mse: 4.1976e-04 - val_loss: 2.4322e-04 - val_mse: 2.4322e-04\n",
            "Epoch 892/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.3710e-04 - mse: 4.3710e-04\n",
            "Epoch 892: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3214e-04 - mse: 4.3214e-04 - val_loss: 2.3529e-04 - val_mse: 2.3529e-04\n",
            "Epoch 893/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2603e-04 - mse: 4.2603e-04\n",
            "Epoch 893: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1780e-04 - mse: 4.1780e-04 - val_loss: 2.4314e-04 - val_mse: 2.4314e-04\n",
            "Epoch 894/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.3336e-04 - mse: 4.3336e-04\n",
            "Epoch 894: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.3042e-04 - mse: 4.3042e-04 - val_loss: 2.3476e-04 - val_mse: 2.3476e-04\n",
            "Epoch 895/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.2878e-04 - mse: 4.2878e-04\n",
            "Epoch 895: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1571e-04 - mse: 4.1571e-04 - val_loss: 2.4304e-04 - val_mse: 2.4304e-04\n",
            "Epoch 896/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.4742e-04 - mse: 4.4742e-04\n",
            "Epoch 896: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2834e-04 - mse: 4.2834e-04 - val_loss: 2.3465e-04 - val_mse: 2.3465e-04\n",
            "Epoch 897/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.1793e-04 - mse: 4.1793e-04\n",
            "Epoch 897: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.1313e-04 - mse: 4.1313e-04 - val_loss: 2.4307e-04 - val_mse: 2.4307e-04\n",
            "Epoch 898/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.4466e-04 - mse: 4.4466e-04\n",
            "Epoch 898: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.2578e-04 - mse: 4.2578e-04 - val_loss: 2.3423e-04 - val_mse: 2.3423e-04\n",
            "Epoch 899/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.1345e-04 - mse: 4.1345e-04\n",
            "Epoch 899: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1197e-04 - mse: 4.1197e-04 - val_loss: 2.4187e-04 - val_mse: 2.4187e-04\n",
            "Epoch 900/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.3316e-04 - mse: 4.3316e-04\n",
            "Epoch 900: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.2461e-04 - mse: 4.2461e-04 - val_loss: 2.3317e-04 - val_mse: 2.3317e-04\n",
            "Epoch 901/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.1661e-04 - mse: 4.1661e-04\n",
            "Epoch 901: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0882e-04 - mse: 4.0882e-04 - val_loss: 2.4166e-04 - val_mse: 2.4166e-04\n",
            "Epoch 902/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.2700e-04 - mse: 4.2700e-04\n",
            "Epoch 902: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.2213e-04 - mse: 4.2213e-04 - val_loss: 2.3254e-04 - val_mse: 2.3254e-04\n",
            "Epoch 903/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.0927e-04 - mse: 4.0927e-04\n",
            "Epoch 903: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0706e-04 - mse: 4.0706e-04 - val_loss: 2.4073e-04 - val_mse: 2.4073e-04\n",
            "Epoch 904/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.2515e-04 - mse: 4.2515e-04\n",
            "Epoch 904: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.2035e-04 - mse: 4.2035e-04 - val_loss: 2.3176e-04 - val_mse: 2.3176e-04\n",
            "Epoch 905/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.1041e-04 - mse: 4.1041e-04\n",
            "Epoch 905: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 4.0585e-04 - mse: 4.0585e-04 - val_loss: 2.3977e-04 - val_mse: 2.3977e-04\n",
            "Epoch 906/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.2701e-04 - mse: 4.2701e-04\n",
            "Epoch 906: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 4.1426e-04 - mse: 4.1426e-04 - val_loss: 2.3282e-04 - val_mse: 2.3282e-04\n",
            "Epoch 907/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.0377e-04 - mse: 4.0377e-04\n",
            "Epoch 907: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 4.0183e-04 - mse: 4.0183e-04 - val_loss: 2.3963e-04 - val_mse: 2.3963e-04\n",
            "Epoch 908/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.2267e-04 - mse: 4.2267e-04\n",
            "Epoch 908: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 4.1481e-04 - mse: 4.1481e-04 - val_loss: 2.3046e-04 - val_mse: 2.3046e-04\n",
            "Epoch 909/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.0213e-04 - mse: 4.0213e-04\n",
            "Epoch 909: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0073e-04 - mse: 4.0073e-04 - val_loss: 2.3905e-04 - val_mse: 2.3905e-04\n",
            "Epoch 910/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1159e-04 - mse: 4.1159e-04\n",
            "Epoch 910: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.1159e-04 - mse: 4.1159e-04 - val_loss: 2.3148e-04 - val_mse: 2.3148e-04\n",
            "Epoch 911/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0533e-04 - mse: 4.0533e-04\n",
            "Epoch 911: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.0093e-04 - mse: 4.0093e-04 - val_loss: 2.3799e-04 - val_mse: 2.3799e-04\n",
            "Epoch 912/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.0607e-04 - mse: 4.0607e-04\n",
            "Epoch 912: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0464e-04 - mse: 4.0464e-04 - val_loss: 2.3527e-04 - val_mse: 2.3527e-04\n",
            "Epoch 913/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.0445e-04 - mse: 4.0445e-04\n",
            "Epoch 913: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.0270e-04 - mse: 4.0270e-04 - val_loss: 2.3463e-04 - val_mse: 2.3463e-04\n",
            "Epoch 914/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.0402e-04 - mse: 4.0402e-04\n",
            "Epoch 914: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.0263e-04 - mse: 4.0263e-04 - val_loss: 2.3366e-04 - val_mse: 2.3366e-04\n",
            "Epoch 915/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0231e-04 - mse: 4.0231e-04\n",
            "Epoch 915: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.0231e-04 - mse: 4.0231e-04 - val_loss: 2.3306e-04 - val_mse: 2.3306e-04\n",
            "Epoch 916/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.2327e-04 - mse: 4.2327e-04\n",
            "Epoch 916: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 4.0622e-04 - mse: 4.0622e-04 - val_loss: 2.2903e-04 - val_mse: 2.2903e-04\n",
            "Epoch 917/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0205e-04 - mse: 4.0205e-04\n",
            "Epoch 917: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9780e-04 - mse: 3.9780e-04 - val_loss: 2.3442e-04 - val_mse: 2.3442e-04\n",
            "Epoch 918/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.1225e-04 - mse: 4.1225e-04\n",
            "Epoch 918: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0056e-04 - mse: 4.0056e-04 - val_loss: 2.3288e-04 - val_mse: 2.3288e-04\n",
            "Epoch 919/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0438e-04 - mse: 4.0438e-04\n",
            "Epoch 919: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0438e-04 - mse: 4.0438e-04 - val_loss: 2.2851e-04 - val_mse: 2.2851e-04\n",
            "Epoch 920/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9737e-04 - mse: 3.9737e-04\n",
            "Epoch 920: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9607e-04 - mse: 3.9607e-04 - val_loss: 2.3428e-04 - val_mse: 2.3428e-04\n",
            "Epoch 921/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 4.0102e-04 - mse: 4.0102e-04\n",
            "Epoch 921: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9958e-04 - mse: 3.9958e-04 - val_loss: 2.3240e-04 - val_mse: 2.3240e-04\n",
            "Epoch 922/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0311e-04 - mse: 4.0311e-04\n",
            "Epoch 922: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 4.0311e-04 - mse: 4.0311e-04 - val_loss: 2.2842e-04 - val_mse: 2.2842e-04\n",
            "Epoch 923/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9561e-04 - mse: 3.9561e-04\n",
            "Epoch 923: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9561e-04 - mse: 3.9561e-04 - val_loss: 2.3367e-04 - val_mse: 2.3367e-04\n",
            "Epoch 924/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0663e-04 - mse: 4.0663e-04\n",
            "Epoch 924: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0249e-04 - mse: 4.0249e-04 - val_loss: 2.2856e-04 - val_mse: 2.2856e-04\n",
            "Epoch 925/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.9866e-04 - mse: 3.9866e-04\n",
            "Epoch 925: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9451e-04 - mse: 3.9451e-04 - val_loss: 2.3485e-04 - val_mse: 2.3485e-04\n",
            "Epoch 926/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.0736e-04 - mse: 4.0736e-04\n",
            "Epoch 926: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0347e-04 - mse: 4.0347e-04 - val_loss: 2.2770e-04 - val_mse: 2.2770e-04\n",
            "Epoch 927/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9365e-04 - mse: 3.9365e-04\n",
            "Epoch 927: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9257e-04 - mse: 3.9257e-04 - val_loss: 2.3446e-04 - val_mse: 2.3446e-04\n",
            "Epoch 928/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.0779e-04 - mse: 4.0779e-04\n",
            "Epoch 928: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0108e-04 - mse: 4.0108e-04 - val_loss: 2.2801e-04 - val_mse: 2.2801e-04\n",
            "Epoch 929/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9420e-04 - mse: 3.9420e-04\n",
            "Epoch 929: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9311e-04 - mse: 3.9311e-04 - val_loss: 2.3411e-04 - val_mse: 2.3411e-04\n",
            "Epoch 930/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0270e-04 - mse: 4.0270e-04\n",
            "Epoch 930: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0270e-04 - mse: 4.0270e-04 - val_loss: 2.2685e-04 - val_mse: 2.2685e-04\n",
            "Epoch 931/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9025e-04 - mse: 3.9025e-04\n",
            "Epoch 931: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9025e-04 - mse: 3.9025e-04 - val_loss: 2.3505e-04 - val_mse: 2.3505e-04\n",
            "Epoch 932/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.0383e-04 - mse: 4.0383e-04\n",
            "Epoch 932: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0011e-04 - mse: 4.0011e-04 - val_loss: 2.2767e-04 - val_mse: 2.2767e-04\n",
            "Epoch 933/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9168e-04 - mse: 3.9168e-04\n",
            "Epoch 933: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9168e-04 - mse: 3.9168e-04 - val_loss: 2.3391e-04 - val_mse: 2.3391e-04\n",
            "Epoch 934/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0568e-04 - mse: 4.0568e-04\n",
            "Epoch 934: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0166e-04 - mse: 4.0166e-04 - val_loss: 2.2638e-04 - val_mse: 2.2638e-04\n",
            "Epoch 935/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8974e-04 - mse: 3.8974e-04\n",
            "Epoch 935: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8974e-04 - mse: 3.8974e-04 - val_loss: 2.3454e-04 - val_mse: 2.3454e-04\n",
            "Epoch 936/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 4.0337e-04 - mse: 4.0337e-04\n",
            "Epoch 936: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 3.9975e-04 - mse: 3.9975e-04 - val_loss: 2.2716e-04 - val_mse: 2.2716e-04\n",
            "Epoch 937/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9035e-04 - mse: 3.9035e-04\n",
            "Epoch 937: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9035e-04 - mse: 3.9035e-04 - val_loss: 2.3436e-04 - val_mse: 2.3436e-04\n",
            "Epoch 938/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0097e-04 - mse: 4.0097e-04\n",
            "Epoch 938: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0097e-04 - mse: 4.0097e-04 - val_loss: 2.2616e-04 - val_mse: 2.2616e-04\n",
            "Epoch 939/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.9133e-04 - mse: 3.9133e-04\n",
            "Epoch 939: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8777e-04 - mse: 3.8777e-04 - val_loss: 2.3519e-04 - val_mse: 2.3519e-04\n",
            "Epoch 940/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.1497e-04 - mse: 4.1497e-04\n",
            "Epoch 940: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 3.9878e-04 - mse: 3.9878e-04 - val_loss: 2.2716e-04 - val_mse: 2.2716e-04\n",
            "Epoch 941/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.9143e-04 - mse: 3.9143e-04\n",
            "Epoch 941: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9012e-04 - mse: 3.9012e-04 - val_loss: 2.3384e-04 - val_mse: 2.3384e-04\n",
            "Epoch 942/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.1730e-04 - mse: 4.1730e-04\n",
            "Epoch 942: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 4.0095e-04 - mse: 4.0095e-04 - val_loss: 2.2601e-04 - val_mse: 2.2601e-04\n",
            "Epoch 943/1000\n",
            "54/61 [=========================>....] - ETA: 0s - loss: 4.0374e-04 - mse: 4.0374e-04\n",
            "Epoch 943: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8816e-04 - mse: 3.8816e-04 - val_loss: 2.3513e-04 - val_mse: 2.3513e-04\n",
            "Epoch 944/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0020e-04 - mse: 4.0020e-04\n",
            "Epoch 944: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 4.0020e-04 - mse: 4.0020e-04 - val_loss: 2.2638e-04 - val_mse: 2.2638e-04\n",
            "Epoch 945/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8720e-04 - mse: 3.8720e-04\n",
            "Epoch 945: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8720e-04 - mse: 3.8720e-04 - val_loss: 2.3535e-04 - val_mse: 2.3535e-04\n",
            "Epoch 946/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 4.0329e-04 - mse: 4.0329e-04\n",
            "Epoch 946: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9932e-04 - mse: 3.9932e-04 - val_loss: 2.2635e-04 - val_mse: 2.2635e-04\n",
            "Epoch 947/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8701e-04 - mse: 3.8701e-04\n",
            "Epoch 947: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8701e-04 - mse: 3.8701e-04 - val_loss: 2.3492e-04 - val_mse: 2.3492e-04\n",
            "Epoch 948/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 4.0010e-04 - mse: 4.0010e-04\n",
            "Epoch 948: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9881e-04 - mse: 3.9881e-04 - val_loss: 2.2641e-04 - val_mse: 2.2641e-04\n",
            "Epoch 949/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.8751e-04 - mse: 3.8751e-04\n",
            "Epoch 949: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8619e-04 - mse: 3.8619e-04 - val_loss: 2.3531e-04 - val_mse: 2.3531e-04\n",
            "Epoch 950/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9782e-04 - mse: 3.9782e-04\n",
            "Epoch 950: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9782e-04 - mse: 3.9782e-04 - val_loss: 2.2630e-04 - val_mse: 2.2630e-04\n",
            "Epoch 951/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8478e-04 - mse: 3.8478e-04\n",
            "Epoch 951: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8478e-04 - mse: 3.8478e-04 - val_loss: 2.3548e-04 - val_mse: 2.3548e-04\n",
            "Epoch 952/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.0730e-04 - mse: 4.0730e-04\n",
            "Epoch 952: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9628e-04 - mse: 3.9628e-04 - val_loss: 2.2670e-04 - val_mse: 2.2670e-04\n",
            "Epoch 953/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 3.9525e-04 - mse: 3.9525e-04\n",
            "Epoch 953: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8482e-04 - mse: 3.8482e-04 - val_loss: 2.3510e-04 - val_mse: 2.3510e-04\n",
            "Epoch 954/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.9729e-04 - mse: 3.9729e-04\n",
            "Epoch 954: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9600e-04 - mse: 3.9600e-04 - val_loss: 2.2670e-04 - val_mse: 2.2670e-04\n",
            "Epoch 955/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.8739e-04 - mse: 3.8739e-04\n",
            "Epoch 955: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8387e-04 - mse: 3.8387e-04 - val_loss: 2.3566e-04 - val_mse: 2.3566e-04\n",
            "Epoch 956/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.0183e-04 - mse: 4.0183e-04\n",
            "Epoch 956: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9551e-04 - mse: 3.9551e-04 - val_loss: 2.2647e-04 - val_mse: 2.2647e-04\n",
            "Epoch 957/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8394e-04 - mse: 3.8394e-04\n",
            "Epoch 957: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8394e-04 - mse: 3.8394e-04 - val_loss: 2.3512e-04 - val_mse: 2.3512e-04\n",
            "Epoch 958/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.9925e-04 - mse: 3.9925e-04\n",
            "Epoch 958: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9537e-04 - mse: 3.9537e-04 - val_loss: 2.2653e-04 - val_mse: 2.2653e-04\n",
            "Epoch 959/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.8727e-04 - mse: 3.8727e-04\n",
            "Epoch 959: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8335e-04 - mse: 3.8335e-04 - val_loss: 2.3572e-04 - val_mse: 2.3572e-04\n",
            "Epoch 960/1000\n",
            "53/61 [=========================>....] - ETA: 0s - loss: 4.1285e-04 - mse: 4.1285e-04\n",
            "Epoch 960: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9583e-04 - mse: 3.9583e-04 - val_loss: 2.2636e-04 - val_mse: 2.2636e-04\n",
            "Epoch 961/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.8683e-04 - mse: 3.8683e-04\n",
            "Epoch 961: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8330e-04 - mse: 3.8330e-04 - val_loss: 2.3555e-04 - val_mse: 2.3555e-04\n",
            "Epoch 962/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9537e-04 - mse: 3.9537e-04\n",
            "Epoch 962: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9537e-04 - mse: 3.9537e-04 - val_loss: 2.2621e-04 - val_mse: 2.2621e-04\n",
            "Epoch 963/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.8636e-04 - mse: 3.8636e-04\n",
            "Epoch 963: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8244e-04 - mse: 3.8244e-04 - val_loss: 2.3575e-04 - val_mse: 2.3575e-04\n",
            "Epoch 964/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.9597e-04 - mse: 3.9597e-04\n",
            "Epoch 964: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9467e-04 - mse: 3.9467e-04 - val_loss: 2.2638e-04 - val_mse: 2.2638e-04\n",
            "Epoch 965/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.8320e-04 - mse: 3.8320e-04\n",
            "Epoch 965: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.8187e-04 - mse: 3.8187e-04 - val_loss: 2.3587e-04 - val_mse: 2.3587e-04\n",
            "Epoch 966/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 4.0040e-04 - mse: 4.0040e-04\n",
            "Epoch 966: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9412e-04 - mse: 3.9412e-04 - val_loss: 2.2626e-04 - val_mse: 2.2626e-04\n",
            "Epoch 967/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8194e-04 - mse: 3.8194e-04\n",
            "Epoch 967: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8118e-04 - mse: 3.8118e-04 - val_loss: 2.3601e-04 - val_mse: 2.3601e-04\n",
            "Epoch 968/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9357e-04 - mse: 3.9357e-04\n",
            "Epoch 968: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9357e-04 - mse: 3.9357e-04 - val_loss: 2.2636e-04 - val_mse: 2.2636e-04\n",
            "Epoch 969/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.8205e-04 - mse: 3.8205e-04\n",
            "Epoch 969: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8071e-04 - mse: 3.8071e-04 - val_loss: 2.3589e-04 - val_mse: 2.3589e-04\n",
            "Epoch 970/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.0388e-04 - mse: 4.0388e-04\n",
            "Epoch 970: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9304e-04 - mse: 3.9304e-04 - val_loss: 2.2611e-04 - val_mse: 2.2611e-04\n",
            "Epoch 971/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.8143e-04 - mse: 3.8143e-04\n",
            "Epoch 971: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 3.8008e-04 - mse: 3.8008e-04 - val_loss: 2.3594e-04 - val_mse: 2.3594e-04\n",
            "Epoch 972/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9347e-04 - mse: 3.9347e-04\n",
            "Epoch 972: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 3.9253e-04 - mse: 3.9253e-04 - val_loss: 2.2613e-04 - val_mse: 2.2613e-04\n",
            "Epoch 973/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8039e-04 - mse: 3.8039e-04\n",
            "Epoch 973: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 3.7965e-04 - mse: 3.7965e-04 - val_loss: 2.3579e-04 - val_mse: 2.3579e-04\n",
            "Epoch 974/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9178e-04 - mse: 3.9178e-04\n",
            "Epoch 974: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 3.9178e-04 - mse: 3.9178e-04 - val_loss: 2.2610e-04 - val_mse: 2.2610e-04\n",
            "Epoch 975/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7899e-04 - mse: 3.7899e-04\n",
            "Epoch 975: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.7899e-04 - mse: 3.7899e-04 - val_loss: 2.3586e-04 - val_mse: 2.3586e-04\n",
            "Epoch 976/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.9468e-04 - mse: 3.9468e-04\n",
            "Epoch 976: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.9120e-04 - mse: 3.9120e-04 - val_loss: 2.2598e-04 - val_mse: 2.2598e-04\n",
            "Epoch 977/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.8215e-04 - mse: 3.8215e-04\n",
            "Epoch 977: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.7862e-04 - mse: 3.7862e-04 - val_loss: 2.3565e-04 - val_mse: 2.3565e-04\n",
            "Epoch 978/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 4.0139e-04 - mse: 4.0139e-04\n",
            "Epoch 978: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.9070e-04 - mse: 3.9070e-04 - val_loss: 2.2596e-04 - val_mse: 2.2596e-04\n",
            "Epoch 979/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7846e-04 - mse: 3.7846e-04\n",
            "Epoch 979: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.7846e-04 - mse: 3.7846e-04 - val_loss: 2.3550e-04 - val_mse: 2.3550e-04\n",
            "Epoch 980/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.9436e-04 - mse: 3.9436e-04\n",
            "Epoch 980: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 3.9051e-04 - mse: 3.9051e-04 - val_loss: 2.2561e-04 - val_mse: 2.2561e-04\n",
            "Epoch 981/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.8167e-04 - mse: 3.8167e-04\n",
            "Epoch 981: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.7777e-04 - mse: 3.7777e-04 - val_loss: 2.3533e-04 - val_mse: 2.3533e-04\n",
            "Epoch 982/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.9034e-04 - mse: 3.9034e-04\n",
            "Epoch 982: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 3.8946e-04 - mse: 3.8946e-04 - val_loss: 2.2596e-04 - val_mse: 2.2596e-04\n",
            "Epoch 983/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.7846e-04 - mse: 3.7846e-04\n",
            "Epoch 983: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 3.7709e-04 - mse: 3.7709e-04 - val_loss: 2.3562e-04 - val_mse: 2.3562e-04\n",
            "Epoch 984/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.9017e-04 - mse: 3.9017e-04\n",
            "Epoch 984: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 3.8884e-04 - mse: 3.8884e-04 - val_loss: 2.2598e-04 - val_mse: 2.2598e-04\n",
            "Epoch 985/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.8066e-04 - mse: 3.8066e-04\n",
            "Epoch 985: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 3.7675e-04 - mse: 3.7675e-04 - val_loss: 2.3562e-04 - val_mse: 2.3562e-04\n",
            "Epoch 986/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.9275e-04 - mse: 3.9275e-04\n",
            "Epoch 986: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 3.8891e-04 - mse: 3.8891e-04 - val_loss: 2.2540e-04 - val_mse: 2.2540e-04\n",
            "Epoch 987/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.7753e-04 - mse: 3.7753e-04\n",
            "Epoch 987: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.7616e-04 - mse: 3.7616e-04 - val_loss: 2.3503e-04 - val_mse: 2.3503e-04\n",
            "Epoch 988/1000\n",
            "58/61 [===========================>..] - ETA: 0s - loss: 3.8919e-04 - mse: 3.8919e-04\n",
            "Epoch 988: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8568e-04 - mse: 3.8568e-04 - val_loss: 2.2637e-04 - val_mse: 2.2637e-04\n",
            "Epoch 989/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8017e-04 - mse: 3.8017e-04\n",
            "Epoch 989: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 3.8017e-04 - mse: 3.8017e-04 - val_loss: 2.2895e-04 - val_mse: 2.2895e-04\n",
            "Epoch 990/1000\n",
            "55/61 [==========================>...] - ETA: 0s - loss: 3.9228e-04 - mse: 3.9228e-04\n",
            "Epoch 990: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8209e-04 - mse: 3.8209e-04 - val_loss: 2.2801e-04 - val_mse: 2.2801e-04\n",
            "Epoch 991/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8282e-04 - mse: 3.8282e-04\n",
            "Epoch 991: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8207e-04 - mse: 3.8207e-04 - val_loss: 2.2765e-04 - val_mse: 2.2765e-04\n",
            "Epoch 992/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8170e-04 - mse: 3.8170e-04\n",
            "Epoch 992: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8092e-04 - mse: 3.8092e-04 - val_loss: 2.2853e-04 - val_mse: 2.2853e-04\n",
            "Epoch 993/1000\n",
            "60/61 [============================>.] - ETA: 0s - loss: 3.8302e-04 - mse: 3.8302e-04\n",
            "Epoch 993: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8166e-04 - mse: 3.8166e-04 - val_loss: 2.2808e-04 - val_mse: 2.2808e-04\n",
            "Epoch 994/1000\n",
            "56/61 [==========================>...] - ETA: 0s - loss: 3.8634e-04 - mse: 3.8634e-04\n",
            "Epoch 994: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 3.8053e-04 - mse: 3.8053e-04 - val_loss: 2.2908e-04 - val_mse: 2.2908e-04\n",
            "Epoch 995/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8138e-04 - mse: 3.8138e-04\n",
            "Epoch 995: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8138e-04 - mse: 3.8138e-04 - val_loss: 2.2857e-04 - val_mse: 2.2857e-04\n",
            "Epoch 996/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8141e-04 - mse: 3.8141e-04\n",
            "Epoch 996: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8141e-04 - mse: 3.8141e-04 - val_loss: 2.2879e-04 - val_mse: 2.2879e-04\n",
            "Epoch 997/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8127e-04 - mse: 3.8127e-04\n",
            "Epoch 997: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8042e-04 - mse: 3.8042e-04 - val_loss: 2.2989e-04 - val_mse: 2.2989e-04\n",
            "Epoch 998/1000\n",
            "57/61 [===========================>..] - ETA: 0s - loss: 3.8244e-04 - mse: 3.8244e-04\n",
            "Epoch 998: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 3.8156e-04 - mse: 3.8156e-04 - val_loss: 2.2914e-04 - val_mse: 2.2914e-04\n",
            "Epoch 999/1000\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8138e-04 - mse: 3.8138e-04\n",
            "Epoch 999: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 3.8138e-04 - mse: 3.8138e-04 - val_loss: 2.2947e-04 - val_mse: 2.2947e-04\n",
            "Epoch 1000/1000\n",
            "59/61 [============================>.] - ETA: 0s - loss: 3.8575e-04 - mse: 3.8575e-04\n",
            "Epoch 1000: val_loss did not improve from 0.00020\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 3.8184e-04 - mse: 3.8184e-04 - val_loss: 2.2945e-04 - val_mse: 2.2945e-04\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_data,\n",
        "                    validation_data=(test_data),\n",
        "                    epochs=1000,\n",
        "                    callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOTcpYM1LE39"
      },
      "source": [
        "저장한 ModelCheckpoint 를 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoPIDRSo32f-",
        "outputId": "5cd100f2-a407-4641-fedc-d941f6c5052a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f76401ab950>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model.load_weights(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTZ-EMWzLE39"
      },
      "source": [
        "`test_data`를 활용하여 예측을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxAxSLNL0l8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5ddbad-43a6-4341-f085-6cf6b642bf68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 1s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMPXjSQ10l8d",
        "outputId": "759a18c4-5037-4987-8177-9bd8a718ff6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(206, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaWpXnQf6zMt"
      },
      "source": [
        "## 예측 데이터 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO2Pk2OaLE3-"
      },
      "source": [
        "아래 시각화 코드중 y_test 데이터에 **[WINDOW_SIZE:]**으로 슬라이싱을 한 이유는\n",
        "\n",
        "예측 데이터에서 WINDOW_SIZE일치의 데이터로 WINDOW_SIZE+1일치를 예측해야하기 때문에 test_data로 예측 시 *앞의 WINDOW_SIZE일은 예측하지 않습니다.*\n",
        "\n",
        "따라서, index WINDOW_SIZE와 비교하면 더욱 정확합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "2F4QQ1O_0l8m",
        "outputId": "ac47b160-b476-4fb4-b406-c4398b589445"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "temp = np.hstack([X_test[WINDOW_SIZE:], pred])\n",
        "temp2 = np.hstack([X_test[WINDOW_SIZE:], y_test.values.reshape(-1, 1)[WINDOW_SIZE:]])\n",
        "\n",
        "preds = scaler.inverse_transform(temp)\n",
        "preds2 = scaler.inverse_transform(temp2)\n",
        "preds = preds[:, -1]\n",
        "label = preds2[:, -1]\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.plot(X_test[WINDOW_SIZE:].index, label, label='actual')\n",
        "plt.plot(X_test[WINDOW_SIZE:].index, preds, label='prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OeNg384psw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}