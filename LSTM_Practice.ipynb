{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cswcjt/Deep_Learning/blob/main/LSTM_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tvy3hvk6kPT8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wicGYpqGkPUA"
      },
      "source": [
        "-  Data : [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "- sequence length 3이며 batch size 4인 데이터로 나눔\n",
        "- [[1,2,3],\n",
        "   [4,5,6],\n",
        "   [7,8,9],\n",
        "   [10,11,12]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "focZ4SGckPUC"
      },
      "outputs": [],
      "source": [
        "inputs = torch.Tensor([1,2,3,4,5,6,7,8,9,10,11,12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W9UsBtEwkPUC"
      },
      "outputs": [],
      "source": [
        "input_size = 1\n",
        "seq_length = 3\n",
        "hidden_size = 2\n",
        "num_layers = 2\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfKfI5VJkPUD"
      },
      "source": [
        "### nn.LSTM Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXxkzwiZkPUD"
      },
      "source": [
        "Input : input과 hidden_0 이라는 2개의 input을 받음\n",
        "- input : neural network로 들어가는 sequence input [seq_length, batch size, input size]\n",
        "\n",
        "- hidden_0 : network의 초기 hidden state [num layers*num directions, batch size, input size] \n",
        "    - num directions : Bidirectional RNN일 경우 2, 나머지 1\n",
        "    - hidden_0은 따로 초기화 하지 않으면 Pytorch에 의해 자동으로 모두 0으로 초기화 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYDotE-0kPUD"
      },
      "source": [
        "Output : out과 hidden이라는 2개의 출력을 냄\n",
        "- out : 마지막 RNN layer로부터 매 timesteps마다의 output\n",
        "- h_n : 모든 RNN layer로부터 마지막 timestep의 hidden value\n",
        "    - (num_layers* num_directions, batch, hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "py9uiQZMkPUE"
      },
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "# batch_first = True이면 (seq, batch, feature) -> (batch, seq, feature)로 바뀜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yy8s3orykPUF",
        "outputId": "2e180a62-5a39-4a84-f3b9-55eec7490f42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs before : torch.Size([12])\n",
            "----------------------------------------\n",
            "inputs after : torch.Size([4, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "# Input\n",
        "print('inputs before :',inputs.shape)\n",
        "inputs = inputs.view(batch_size, seq_length, input_size)\n",
        "print('-'*40)\n",
        "print('inputs after :',inputs.shape) # [batch size, seq length, input size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BBQnefI4kPUG",
        "outputId": "04f38d8d-afc3-4ea2-e366-e50464624c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out: torch.Size([4, 3, 2])\n",
            "hidden: torch.Size([2, 4, 2])\n",
            "cell: torch.Size([2, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "# Output -> pytorch가 자동으로 하는 동작을 눈으로 확인해보자\n",
        "\n",
        "# hidden_init = hidden initialize\n",
        "# cell_init = cell initialize\n",
        "hidden_init = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "cell_init = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "\n",
        "out, (hidden, cell) = lstm(inputs, (hidden_init,cell_init))\n",
        "print('out:',out.shape) # [batch size, seq length, num_directions*hidden size]\n",
        "print('hidden:',hidden.shape) # [num directions * num layers, batch size, hidden size]\n",
        "print('cell:',cell.shape) # [num directions * num layers, batch size, hidden size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1sGZu_PkPUG"
      },
      "source": [
        "### LSTM application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihsIxxtRkPUH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# MNIST 데이터셋 \n",
        "train_data = datasets.MNIST(\n",
        "    root=\"../data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"../data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# Data loader\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qLno_NJIkPUH"
      },
      "outputs": [],
      "source": [
        "# LSTM - many to one\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = 28\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 2\n",
        "        # batch_first = True이면 (seq, batch, feature) -> (batch, seq, feature)로 바뀜\n",
        "        self.LSTM = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
        "        # fc = fully connected\n",
        "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # hidden state와 cell state 초기화\n",
        "        hidden_init = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        cell_init = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        out, hidden = self.LSTM(x, (hidden_init, cell_init))  \n",
        "        # out: [mini-batch, seq_length, hidden_size]\n",
        "        # out[:, -1, :] -> 모든 mini-batch, 마지막 seq_length, 모든 hidden_size \n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTM(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XlXuy7y2kPUI"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "CELoss = nn.CrossEntropyLoss()\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "P-zSYmfOkPUI",
        "outputId": "1857f605-a394-451a-8808-e887b1827436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iteration : 469\n",
            "Epoch [1/3], Loss: 0.2551\n",
            "Epoch [2/3], Loss: 0.2318\n",
            "Epoch [3/3], Loss: 0.0082\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# MINIST의 shape = 28*28 --> sequence_length, input_size을 정하는 기준\n",
        "# for i, (images, labels) in enumerate(train_loader):\n",
        "#     print(type(images))\n",
        "#     print(images.shape)\n",
        "#     break\n",
        "\n",
        "total_epochs = 3\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "\n",
        "print('number of iteration :', len(train_loader))\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(total_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = CELoss(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        adam_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        adam_optimizer.step()\n",
        "        \n",
        "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, total_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lMxMREy4kPUI",
        "outputId": "56acefc3-f335-4c54-b6a9-2b5e57c91db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 96.72 %\n"
          ]
        }
      ],
      "source": [
        "# 학습이 끝난 후 모델 성능 테스트\n",
        "# torch.no_grad() --> test에서는 back propagation 작업을 하지 않으므로 gradient를 계산하지 않도록 함 - 메모리의 효율성을 위해\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Z_PcjuS5kPUJ"
      },
      "outputs": [],
      "source": [
        "# 학습한 모델을 model_RNN.ckpt라는 이름으로 저장\n",
        "torch.save(model.state_dict(), 'model_LSTM.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jji2qnakPUJ"
      },
      "source": [
        "### Bidirectional RNN Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XHX_TiwxkPUJ"
      },
      "outputs": [],
      "source": [
        "# Bidirectional LSTM - many to one\n",
        "class Bi_LSTM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Bi_LSTM, self).__init__()\n",
        "        self.input_size = 28\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 2\n",
        "        self.LSTM = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=True)\n",
        "        # batch_first = True이면 (seq, batch, feature) -> (batch, seq, feature)로 바뀜\n",
        "        self.fc = nn.Linear(self.hidden_size*2, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # hidden state와 cell state 초기화\n",
        "        hidden_init = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "        cell_init = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        out, _ = self.LSTM(x, (hidden_init, cell_init))  # out: [mini-batch, seq_length, hidden_size]\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Bi_LSTM(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iU8hfUCvkPUJ"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "CELoss = nn.CrossEntropyLoss()\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UfybHLplkPUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98de402-b819-453b-fc3f-f203d51350c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of iteration : 469\n",
            "Epoch [1/3], Loss: 0.2164\n",
            "Epoch [2/3], Loss: 0.1474\n",
            "Epoch [3/3], Loss: 0.1364\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "total_epochs = 3\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "\n",
        "print('number of iteration :', len(train_loader))\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(total_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = CELoss(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        adam_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        adam_optimizer.step()\n",
        "        \n",
        "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, total_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1Cz_ImhmkPUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cad13b7-da15-4b28-d6d2-8689fea9af7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 97.65 %\n"
          ]
        }
      ],
      "source": [
        "# 학습이 끝난 후 모델 성능 테스트\n",
        "# torch.no_grad() --> test에서는 back propagation 작업을 하지 않으므로 gradient를 계산하지 않도록 함 - 메모리의 효율성을 위해\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}